{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models - Inital Attempt\n",
    "These were our initial models that were copied over into Models_Final.ipynb. These inital models yielded much lower success rates - verification was around %75, and our prediction model around %51. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88iolf-xuK28"
   },
   "outputs": [],
   "source": [
    "!pip install global-land-mask\n",
    "!pip install fastdtw\n",
    "!pip install haversine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y7xpUGtaJ7uI",
    "outputId": "9c33fb21-84e8-4c40-edf7-a96f7b3349b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jkLDKly9JG6o"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from global_land_mask import globe\n",
    "from scipy.spatial.distance import euclidean\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import requests\n",
    "from io import StringIO\n",
    "from geopy.distance import geodesic\n",
    "from haversine import haversine, Unit\n",
    "\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import statistics\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GwtMH4mH1nwP"
   },
   "outputs": [],
   "source": [
    "#location spoofer\n",
    "def calculate_bounding_box(center_lat, center_lon, radius_miles):\n",
    "    return (\n",
    "        geodesic().destination((center_lat, center_lon), 0, radius_miles).latitude,\n",
    "        geodesic().destination((center_lat, center_lon), 90, radius_miles).longitude,\n",
    "        geodesic().destination((center_lat, center_lon), 180, radius_miles).latitude,\n",
    "        geodesic().destination((center_lat, center_lon), 270, radius_miles).longitude\n",
    "    )\n",
    "def get_random_point_within_bounding_box(bounding_box):\n",
    "    random_lat = random.uniform(bounding_box[0], bounding_box[2])\n",
    "    random_lon = random.uniform(bounding_box[1], bounding_box[3])\n",
    "    return random_lat, random_lon\n",
    "\n",
    "def get_points_within_radius(center_lat, center_lon, smaller_radius_miles, larger_radius_miles):\n",
    "    smaller_bounding_box = calculate_bounding_box(center_lat, center_lon, smaller_radius_miles)\n",
    "    larger_bounding_box = calculate_bounding_box(center_lat, center_lon, larger_radius_miles)\n",
    "\n",
    "    random_point_within_larger_box = get_random_point_within_bounding_box(larger_bounding_box)\n",
    "\n",
    "    while (\n",
    "        smaller_bounding_box[0] <= random_point_within_larger_box[0] <= smaller_bounding_box[2] and\n",
    "        smaller_bounding_box[1] <= random_point_within_larger_box[1] <= smaller_bounding_box[3]\n",
    "    ):\n",
    "        random_point_within_larger_box = get_random_point_within_bounding_box(larger_bounding_box)\n",
    "\n",
    "    return random_point_within_larger_box\n",
    "\n",
    "def spoof_location(center_lat, center_lon, smaller_radius_miles, larger_radius_miles):\n",
    "    for i in range(10):\n",
    "      lat_spoof, lon_spoof = get_points_within_radius(center_lat, center_lon, smaller_radius_miles, larger_radius_miles)\n",
    "      if globe.is_land(lat_spoof, lon_spoof):\n",
    "        return (lat_spoof, lon_spoof)\n",
    "    return get_points_within_radius(center_lat, center_lon, smaller_radius_miles, larger_radius_miles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rO_wGpgj17Vk"
   },
   "outputs": [],
   "source": [
    "# center_lat = 42.7284\n",
    "# center_lon = -73.6918\n",
    "# smaller_radius_miles = 50\n",
    "# larger_radius_miles = 400\n",
    "\n",
    "# for ix in range(5) :\n",
    "#   result = spoof_location(center_lat, center_lon, smaller_radius_miles, larger_radius_miles)\n",
    "#   print(f\"Count {ix+1} ----> Latitude : {result[0]:.5f} ----> Longitude : {result[1]:.5f}\")\n",
    "\n",
    "# import folium\n",
    "\n",
    "# latitude = 25.47\n",
    "# longitude = -112.58\n",
    "\n",
    "\n",
    "# lon2 = -67.19\n",
    "# lat2 = 48.94\n",
    "\n",
    "# lon3 = -82.39\n",
    "# lat3 = 27.28\n",
    "\n",
    "# map_center1 = [latitude, longitude]\n",
    "# map_center2 = [lat2, lon2]\n",
    "# map_center3 = [lat3, lon3]\n",
    "# my_map = folium.Map(location=map_center1, zoom_start=10)\n",
    "# my_map = folium.Map(location=map_center2, zoom_start=10)\n",
    "# my_map = folium.Map(location=map_center3, zoom_start=10)\n",
    "# folium.Marker(location=map_center1, popup=\"Given Location\").add_to(my_map)\n",
    "# folium.Marker(location=map_center2, popup=\"Given Location\").add_to(my_map)\n",
    "# folium.Marker(location=map_center3, popup=\"Given Location\").add_to(my_map)\n",
    "\n",
    "# my_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "MruJJwErOXJP"
   },
   "outputs": [],
   "source": [
    "#dataset and model classes for verification model\n",
    "\n",
    "class TreeRingDataset(Dataset):\n",
    "\n",
    "  _cache = None\n",
    "  _train = None\n",
    "  _test = None\n",
    "  _validate = None\n",
    "\n",
    "  def __init__(self, set_type=\"train\"):\n",
    "\n",
    "    if TreeRingDataset._cache is None:\n",
    "      res = requests.get(\"https://paleo-data.s3.amazonaws.com/data.csv\")\n",
    "      TreeRingDataset._cache = pd.read_csv(StringIO(res.text), sep=\",\")\n",
    "\n",
    "    res = requests.get(\"https://paleo-data.s3.amazonaws.com/main_weather.csv\")\n",
    "    self.W_df = pd.read_csv(StringIO(res.text), sep=\",\")\n",
    "    self.W_df.drop_duplicates(subset = ['year', 'lat', 'lon'],\n",
    "                                          inplace = True,\n",
    "                                          ignore_index = True)\n",
    "\n",
    "    self.df = TreeRingDataset._cache.copy()\n",
    "\n",
    "    self.df.drop(self.df.columns[list(range(1, 1940))], axis=1, inplace=True)\n",
    "    self.df.dropna(subset=self.df.columns[1:52], how='any', inplace=True)\n",
    "\n",
    "    if type(TreeRingDataset._train) == type(None):\n",
    "      print(\"Performing a Split\")\n",
    "      TreeRingDataset._train, TreeRingDataset._test, TreeRingDataset._validate = self.__split()\n",
    "\n",
    "    self.df = TreeRingDataset._train if set_type == \"train\" else (TreeRingDataset._test if set_type == \"test\" else TreeRingDataset._validate)\n",
    "\n",
    "  def __split(self):\n",
    "    train, test = train_test_split(self.df, train_size=.70)\n",
    "    validate, test = train_test_split(test, train_size=.5)\n",
    "    return (train, test, validate)\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.df.shape[0]\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    x = torch.tensor(self.df.iloc[index, 1:52].ffill().bfill())\n",
    "    latn, lats, lone, lonw = self.df.iloc[index, 87:91]\n",
    "    # lat = (latn+lats)/2\n",
    "    # lon = (lone+lonw)/2\n",
    "    lat = latn\n",
    "    lon = lone\n",
    "\n",
    "    #print(lat, lon)\n",
    "\n",
    "    if random.random() > 0.5:\n",
    "      other_locations = self.W_df[['lat','lon']][(self.W_df['lat'] != lat) & (self.W_df['lon'] != lon)]\n",
    "      unique_locations = other_locations.drop_duplicates()\n",
    "      spoof_location = unique_locations.sample(n=1)\n",
    "      label = 0\n",
    "      lat = spoof_location['lat'].iloc[0]\n",
    "      lon = spoof_location['lon'].iloc[0]\n",
    "\n",
    "    else:\n",
    "      label = 1\n",
    "\n",
    "    #print(lat, lon)\n",
    "\n",
    "\n",
    "    weather_data = self.W_df[(self.W_df['lat'] == lat) & (self.W_df['lon'] == lon)]\n",
    "\n",
    "    temp = torch.tensor(weather_data['temperature_avg'].values)\n",
    "    precip = torch.tensor(weather_data['precipitation_avg'].values)\n",
    "    x = torch.cat((x, temp, precip))\n",
    "    #print(x.shape)\n",
    "\n",
    "    return x, label\n",
    "\n",
    "class VerificationModel(nn.Module):\n",
    "  def __init__(self, num_rings):\n",
    "      super(VerificationModel, self).__init__()\n",
    "      self.fc = nn.Sequential(\n",
    "          nn.Linear(num_rings * 3, 512),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, 512),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, 512),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 1),\n",
    "          nn.Sigmoid()\n",
    "      )\n",
    "\n",
    "  def forward(self, x):\n",
    "      out = self.fc(x)\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "0xcmAW4KPC6F"
   },
   "outputs": [],
   "source": [
    "#dataset and model classes for location  model\n",
    "\n",
    "class TreeRingDataset(Dataset):\n",
    "\n",
    "  _cache = None\n",
    "  _train = None\n",
    "  _test = None\n",
    "  _validate = None\n",
    "\n",
    "  def __init__(self, set_type=\"train\"):\n",
    "\n",
    "    if TreeRingDataset._cache is None:\n",
    "      res = requests.get(\"https://paleo-data.s3.amazonaws.com/data.csv\")\n",
    "      TreeRingDataset._cache = pd.read_csv(StringIO(res.text), sep=\",\")\n",
    "\n",
    "    self.df = TreeRingDataset._cache.copy()\n",
    "\n",
    "    self.df.drop(self.df.columns[list(range(1, 1940))], axis=1, inplace=True)\n",
    "    self.df.dropna(subset=self.df.columns[1:52], how='any', inplace=True)\n",
    "\n",
    "    if type(TreeRingDataset._train) == type(None):\n",
    "      print(\"Performing a Split\")\n",
    "      TreeRingDataset._train, TreeRingDataset._test, TreeRingDataset._validate = self.__split()\n",
    "\n",
    "    self.df = TreeRingDataset._train if set_type == \"train\" else (TreeRingDataset._test if set_type == \"test\" else TreeRingDataset._validate)\n",
    "\n",
    "\n",
    "  def __split(self):\n",
    "    train, test = train_test_split(self.df, train_size=.70)\n",
    "    validate, test = train_test_split(test, train_size=.5)\n",
    "    return (train, test, validate)\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.df.shape[0]\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    x = torch.tensor(self.df.iloc[index, 1:52])\n",
    "    latn, lats, lone, lonw = self.df.iloc[index, 87:91]\n",
    "    label = torch.tensor([(latn+lats)/2, (lone+lonw)/2])\n",
    "    return x, label\n",
    "\n",
    "class LocationModel(nn.Module):\n",
    "  def __init__(self, num_rings):\n",
    "      super(LocationModel, self).__init__()\n",
    "      self.fc = nn.Sequential(\n",
    "          nn.Linear(num_rings, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, 2),\n",
    "      )\n",
    "\n",
    "  def forward(self, x):\n",
    "      out = self.fc(x)\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYN-G9azJ7zl"
   },
   "outputs": [],
   "source": [
    "#Dataset and Dataloader calls\n",
    "train_dataset = TreeRingDataset(set_type= \"train\")\n",
    "print(train_dataset.__getitem__(5))\n",
    "val_dataset = TreeRingDataset(set_type= \"validate\")\n",
    "test_dataset = TreeRingDataset(set_type= \"test\")\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_data_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32OJmPtqbfhT",
    "outputId": "fdc892a2-629b-44f2-aa56-9223b2e3e807"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10.728399999999993, -3.6918000000000006)\n"
     ]
    }
   ],
   "source": [
    "#calculates climate comparison\n",
    "def climate_comparison(ts1, ts2):\n",
    "  distance, path = fastdtw(ts1, ts2, dist=euclidean)\n",
    "  return(distance)\n",
    "\n",
    "#input is a tuple with lat and lon and a tuple with start and end year,\n",
    "#outputs a vector of temperatures over that time period\n",
    "def get_temp(lat_lon, years):\n",
    "  return 0\n",
    "\n",
    "#input is a tuple with lat and lon and a tuple with start and end year,\n",
    "#outputs a vector of precipitation values over that time period\n",
    "def get_precip(lat_lon):\n",
    "  return 0\n",
    "\n",
    "\n",
    "def clip_coords(lat_lon):\n",
    "  lat, lon = lat_lon\n",
    "  return (math.fmod(lat, 90), math.fmod(lon, 180))\n",
    "\n",
    "def climate_loss(output, target):\n",
    "  output = clip_coords(output)\n",
    "  ts_temp_output = get_temp(output)\n",
    "  ts_precip_output = get_precip(output)\n",
    "  ts_temp_target = get_temp(target)\n",
    "  ts_precip_target = get_precip(target)\n",
    "\n",
    "  temp_dtw = climate_comparison(ts_temp_output, ts_temp_target)\n",
    "  precip_dtw = climate_comparison(ts_precip_output, ts_precip_target)\n",
    "  return (output[0] - target[0])**2 + (target[1] - target[1])**2 + (temp_dtw * precip_dtw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpJyrDwFbXzC"
   },
   "outputs": [],
   "source": [
    "#location model training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "#model = VerificationModel(51).to(device)\n",
    "model = LocationModel(51).to(device)\n",
    "\n",
    "loss_function = climate_loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Define the number of epochs for training\n",
    "num_epochs = 50\n",
    "\n",
    "# Get the total number of training samples\n",
    "training_size = len(train_data_loader.dataset)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch\", epoch+1, \"\\n---------------------\")\n",
    "\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    training_batch = 0\n",
    "\n",
    "    # Iterate over training batches\n",
    "    for inputs, labels in train_data_loader:\n",
    "        # Move inputs and labels to the specified device\n",
    "        inputs = inputs.float()\n",
    "        # print(inputs[0])\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass to get outputs\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate the loss\n",
    "        outputs = outputs.view(-1)\n",
    "\n",
    "        labels = labels.float()\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "\n",
    "        # Backward pass to calculate gradients\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # Update running loss and batch count\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        training_batch += len(inputs)\n",
    "        print(f\"training loss: {loss:>7f}  Batch: [{training_batch:>5d}/{training_size:>5d}]\")\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_data_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    validation_loss = 0.0\n",
    "    total_distances = 0\n",
    "    validation_size = len(validation_data_loader.dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        validation_batch = 0\n",
    "        for inputs, labels in validation_data_loader:\n",
    "            # Move inputs and labels to the specified device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            # outputs = outputs.view(-1)\n",
    "\n",
    "            labels = labels.float()\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            # Update validation loss and count\n",
    "            validation_loss += loss.item() * inputs.size(0)\n",
    "            # Calculate accuracy\n",
    "            total_distances += haversine((outputs[0], outputs[1]),(labels[0], labels[1]))\n",
    "\n",
    "            print(f\"validation loss: {loss:>7f}  Batch: [{validation_batch:>5d}/{validation_size:>5d}]\")\n",
    "\n",
    "    # Calculate average validation loss and accuracy for the epoch\n",
    "    epoch_validation_loss = validation_loss / len(validation_data_loader.dataset)\n",
    "    validation_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {epoch_validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.2f}%\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1m-MqJ9-jMHm",
    "outputId": "d9219d3f-45e9-4f3c-92d8-d3283674e810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Epoch 1 \n",
      "---------------------\n",
      "training loss: 0.672027  Batch: [   32/ 4992]\n",
      "training loss: 0.692549  Batch: [   64/ 4992]\n",
      "training loss: 0.702754  Batch: [   96/ 4992]\n",
      "training loss: 0.755907  Batch: [  128/ 4992]\n",
      "training loss: 0.745147  Batch: [  160/ 4992]\n",
      "training loss: 0.671949  Batch: [  192/ 4992]\n",
      "training loss: 0.696577  Batch: [  224/ 4992]\n",
      "training loss: 0.702525  Batch: [  256/ 4992]\n",
      "training loss: 0.699351  Batch: [  288/ 4992]\n",
      "training loss: 0.695840  Batch: [  320/ 4992]\n",
      "training loss: 0.688111  Batch: [  352/ 4992]\n",
      "training loss: 0.703228  Batch: [  384/ 4992]\n",
      "training loss: 0.699311  Batch: [  416/ 4992]\n",
      "training loss: 0.697787  Batch: [  448/ 4992]\n",
      "training loss: 0.718982  Batch: [  480/ 4992]\n",
      "training loss: 0.668359  Batch: [  512/ 4992]\n",
      "training loss: 0.687824  Batch: [  544/ 4992]\n",
      "training loss: 0.696861  Batch: [  576/ 4992]\n",
      "training loss: 0.687268  Batch: [  608/ 4992]\n",
      "training loss: 0.685816  Batch: [  640/ 4992]\n",
      "training loss: 0.690914  Batch: [  672/ 4992]\n",
      "training loss: 0.721792  Batch: [  704/ 4992]\n",
      "training loss: 0.664062  Batch: [  736/ 4992]\n",
      "training loss: 0.679515  Batch: [  768/ 4992]\n",
      "training loss: 0.682000  Batch: [  800/ 4992]\n",
      "training loss: 0.687770  Batch: [  832/ 4992]\n",
      "training loss: 0.661412  Batch: [  864/ 4992]\n",
      "training loss: 0.682459  Batch: [  896/ 4992]\n",
      "training loss: 0.734504  Batch: [  928/ 4992]\n",
      "training loss: 0.687123  Batch: [  960/ 4992]\n",
      "training loss: 0.736878  Batch: [  992/ 4992]\n",
      "training loss: 0.709159  Batch: [ 1024/ 4992]\n",
      "training loss: 0.703491  Batch: [ 1056/ 4992]\n",
      "training loss: 0.680205  Batch: [ 1088/ 4992]\n",
      "training loss: 0.685830  Batch: [ 1120/ 4992]\n",
      "training loss: 0.696137  Batch: [ 1152/ 4992]\n",
      "training loss: 0.690710  Batch: [ 1184/ 4992]\n",
      "training loss: 0.674161  Batch: [ 1216/ 4992]\n",
      "training loss: 0.727490  Batch: [ 1248/ 4992]\n",
      "training loss: 0.681162  Batch: [ 1280/ 4992]\n",
      "training loss: 0.660524  Batch: [ 1312/ 4992]\n",
      "training loss: 0.717077  Batch: [ 1344/ 4992]\n",
      "training loss: 0.732560  Batch: [ 1376/ 4992]\n",
      "training loss: 0.663965  Batch: [ 1408/ 4992]\n",
      "training loss: 0.699116  Batch: [ 1440/ 4992]\n",
      "training loss: 0.672708  Batch: [ 1472/ 4992]\n",
      "training loss: 0.675196  Batch: [ 1504/ 4992]\n",
      "training loss: 0.706639  Batch: [ 1536/ 4992]\n",
      "training loss: 0.683793  Batch: [ 1568/ 4992]\n",
      "training loss: 0.698364  Batch: [ 1600/ 4992]\n",
      "training loss: 0.663400  Batch: [ 1632/ 4992]\n",
      "training loss: 0.691740  Batch: [ 1664/ 4992]\n",
      "training loss: 0.691590  Batch: [ 1696/ 4992]\n",
      "training loss: 0.689252  Batch: [ 1728/ 4992]\n",
      "training loss: 0.700800  Batch: [ 1760/ 4992]\n",
      "training loss: 0.697541  Batch: [ 1792/ 4992]\n",
      "training loss: 0.686124  Batch: [ 1824/ 4992]\n",
      "training loss: 0.687330  Batch: [ 1856/ 4992]\n",
      "training loss: 0.677845  Batch: [ 1888/ 4992]\n",
      "training loss: 0.691197  Batch: [ 1920/ 4992]\n",
      "training loss: 0.679262  Batch: [ 1952/ 4992]\n",
      "training loss: 0.687703  Batch: [ 1984/ 4992]\n",
      "training loss: 0.689663  Batch: [ 2016/ 4992]\n",
      "training loss: 0.685935  Batch: [ 2048/ 4992]\n",
      "training loss: 0.701215  Batch: [ 2080/ 4992]\n",
      "training loss: 0.703662  Batch: [ 2112/ 4992]\n",
      "training loss: 0.700721  Batch: [ 2144/ 4992]\n",
      "training loss: 0.701649  Batch: [ 2176/ 4992]\n",
      "training loss: 0.703456  Batch: [ 2208/ 4992]\n",
      "training loss: 0.691891  Batch: [ 2240/ 4992]\n",
      "training loss: 0.690214  Batch: [ 2272/ 4992]\n",
      "training loss: 0.690861  Batch: [ 2304/ 4992]\n",
      "training loss: 0.699592  Batch: [ 2336/ 4992]\n",
      "training loss: 0.683396  Batch: [ 2368/ 4992]\n",
      "training loss: 0.688439  Batch: [ 2400/ 4992]\n",
      "training loss: 0.695076  Batch: [ 2432/ 4992]\n",
      "training loss: 0.663256  Batch: [ 2464/ 4992]\n",
      "training loss: 0.694005  Batch: [ 2496/ 4992]\n",
      "training loss: 0.658884  Batch: [ 2528/ 4992]\n",
      "training loss: 0.695931  Batch: [ 2560/ 4992]\n",
      "training loss: 0.721560  Batch: [ 2592/ 4992]\n",
      "training loss: 0.720745  Batch: [ 2624/ 4992]\n",
      "training loss: 0.624512  Batch: [ 2656/ 4992]\n",
      "training loss: 0.716128  Batch: [ 2688/ 4992]\n",
      "training loss: 0.711846  Batch: [ 2720/ 4992]\n",
      "training loss: 0.692871  Batch: [ 2752/ 4992]\n",
      "training loss: 0.673591  Batch: [ 2784/ 4992]\n",
      "training loss: 0.696609  Batch: [ 2816/ 4992]\n",
      "training loss: 0.655539  Batch: [ 2848/ 4992]\n",
      "training loss: 0.697204  Batch: [ 2880/ 4992]\n",
      "training loss: 0.714038  Batch: [ 2912/ 4992]\n",
      "training loss: 0.671039  Batch: [ 2944/ 4992]\n",
      "training loss: 0.694882  Batch: [ 2976/ 4992]\n",
      "training loss: 0.677639  Batch: [ 3008/ 4992]\n",
      "training loss: 0.686159  Batch: [ 3040/ 4992]\n",
      "training loss: 0.688061  Batch: [ 3072/ 4992]\n",
      "training loss: 0.666166  Batch: [ 3104/ 4992]\n",
      "training loss: 0.696535  Batch: [ 3136/ 4992]\n",
      "training loss: 0.691299  Batch: [ 3168/ 4992]\n",
      "training loss: 0.677673  Batch: [ 3200/ 4992]\n",
      "training loss: 0.692042  Batch: [ 3232/ 4992]\n",
      "training loss: 0.686642  Batch: [ 3264/ 4992]\n",
      "training loss: 0.680385  Batch: [ 3296/ 4992]\n",
      "training loss: 0.680481  Batch: [ 3328/ 4992]\n",
      "training loss: 0.672489  Batch: [ 3360/ 4992]\n",
      "training loss: 0.654807  Batch: [ 3392/ 4992]\n",
      "training loss: 0.672038  Batch: [ 3424/ 4992]\n",
      "training loss: 0.706175  Batch: [ 3456/ 4992]\n",
      "training loss: 0.686743  Batch: [ 3488/ 4992]\n",
      "training loss: 0.688547  Batch: [ 3520/ 4992]\n",
      "training loss: 0.696317  Batch: [ 3552/ 4992]\n",
      "training loss: 0.683919  Batch: [ 3584/ 4992]\n",
      "training loss: 0.652440  Batch: [ 3616/ 4992]\n",
      "training loss: 0.688547  Batch: [ 3648/ 4992]\n",
      "training loss: 0.661964  Batch: [ 3680/ 4992]\n",
      "training loss: 0.696338  Batch: [ 3712/ 4992]\n",
      "training loss: 0.703354  Batch: [ 3744/ 4992]\n",
      "training loss: 0.688293  Batch: [ 3776/ 4992]\n",
      "training loss: 0.651416  Batch: [ 3808/ 4992]\n",
      "training loss: 0.698050  Batch: [ 3840/ 4992]\n",
      "training loss: 0.692123  Batch: [ 3872/ 4992]\n",
      "training loss: 0.688475  Batch: [ 3904/ 4992]\n",
      "training loss: 0.705636  Batch: [ 3936/ 4992]\n",
      "training loss: 0.684924  Batch: [ 3968/ 4992]\n",
      "training loss: 0.661334  Batch: [ 4000/ 4992]\n",
      "training loss: 0.656435  Batch: [ 4032/ 4992]\n",
      "training loss: 0.734026  Batch: [ 4064/ 4992]\n",
      "training loss: 0.713382  Batch: [ 4096/ 4992]\n",
      "training loss: 0.671250  Batch: [ 4128/ 4992]\n",
      "training loss: 0.718260  Batch: [ 4160/ 4992]\n",
      "training loss: 0.679632  Batch: [ 4192/ 4992]\n",
      "training loss: 0.713350  Batch: [ 4224/ 4992]\n",
      "training loss: 0.672507  Batch: [ 4256/ 4992]\n",
      "training loss: 0.680890  Batch: [ 4288/ 4992]\n",
      "training loss: 0.668200  Batch: [ 4320/ 4992]\n",
      "training loss: 0.711255  Batch: [ 4352/ 4992]\n",
      "training loss: 0.681971  Batch: [ 4384/ 4992]\n",
      "training loss: 0.686282  Batch: [ 4416/ 4992]\n",
      "training loss: 0.693678  Batch: [ 4448/ 4992]\n",
      "training loss: 0.699791  Batch: [ 4480/ 4992]\n",
      "training loss: 0.695948  Batch: [ 4512/ 4992]\n",
      "training loss: 0.670348  Batch: [ 4544/ 4992]\n",
      "training loss: 0.681477  Batch: [ 4576/ 4992]\n",
      "training loss: 0.706004  Batch: [ 4608/ 4992]\n",
      "training loss: 0.691284  Batch: [ 4640/ 4992]\n",
      "training loss: 0.664860  Batch: [ 4672/ 4992]\n",
      "training loss: 0.685092  Batch: [ 4704/ 4992]\n",
      "training loss: 0.687202  Batch: [ 4736/ 4992]\n",
      "training loss: 0.710561  Batch: [ 4768/ 4992]\n",
      "training loss: 0.702806  Batch: [ 4800/ 4992]\n",
      "training loss: 0.695876  Batch: [ 4832/ 4992]\n",
      "training loss: 0.683755  Batch: [ 4864/ 4992]\n",
      "training loss: 0.679446  Batch: [ 4896/ 4992]\n",
      "training loss: 0.719714  Batch: [ 4928/ 4992]\n",
      "training loss: 0.670335  Batch: [ 4960/ 4992]\n",
      "training loss: 0.696341  Batch: [ 4992/ 4992]\n",
      "Epoch 1/50, Training Loss: 22.0935\n",
      "validation loss: 0.686797  Batch: [   32/ 1070]\n",
      "validation loss: 0.669711  Batch: [   64/ 1070]\n",
      "validation loss: 0.685272  Batch: [   96/ 1070]\n",
      "validation loss: 0.681646  Batch: [  128/ 1070]\n",
      "validation loss: 0.699482  Batch: [  160/ 1070]\n",
      "validation loss: 0.699651  Batch: [  192/ 1070]\n",
      "validation loss: 0.691970  Batch: [  224/ 1070]\n",
      "validation loss: 0.691367  Batch: [  256/ 1070]\n",
      "validation loss: 0.701348  Batch: [  288/ 1070]\n",
      "validation loss: 0.682897  Batch: [  320/ 1070]\n",
      "validation loss: 0.691662  Batch: [  352/ 1070]\n",
      "validation loss: 0.690275  Batch: [  384/ 1070]\n",
      "validation loss: 0.693754  Batch: [  416/ 1070]\n",
      "validation loss: 0.693479  Batch: [  448/ 1070]\n",
      "validation loss: 0.675736  Batch: [  480/ 1070]\n",
      "validation loss: 0.674797  Batch: [  512/ 1070]\n",
      "validation loss: 0.687267  Batch: [  544/ 1070]\n",
      "validation loss: 0.680355  Batch: [  576/ 1070]\n",
      "validation loss: 0.681666  Batch: [  608/ 1070]\n",
      "validation loss: 0.696382  Batch: [  640/ 1070]\n",
      "validation loss: 0.694795  Batch: [  672/ 1070]\n",
      "validation loss: 0.669625  Batch: [  704/ 1070]\n",
      "validation loss: 0.673790  Batch: [  736/ 1070]\n",
      "validation loss: 0.686332  Batch: [  768/ 1070]\n",
      "validation loss: 0.693090  Batch: [  800/ 1070]\n",
      "validation loss: 0.691067  Batch: [  832/ 1070]\n",
      "validation loss: 0.699364  Batch: [  864/ 1070]\n",
      "validation loss: 0.681533  Batch: [  896/ 1070]\n",
      "validation loss: 0.677820  Batch: [  928/ 1070]\n",
      "validation loss: 0.694469  Batch: [  960/ 1070]\n",
      "validation loss: 0.671392  Batch: [  992/ 1070]\n",
      "validation loss: 0.703200  Batch: [ 1024/ 1070]\n",
      "validation loss: 0.669320  Batch: [ 1056/ 1070]\n",
      "validation loss: 0.682917  Batch: [ 1070/ 1070]\n",
      "Epoch 1/50, Validation Loss: 0.6867, Validation Accuracy: 53.27%\n",
      "\n",
      "\n",
      "Epoch 2 \n",
      "---------------------\n",
      "training loss: 0.683830  Batch: [   32/ 4992]\n",
      "training loss: 0.680879  Batch: [   64/ 4992]\n",
      "training loss: 0.690897  Batch: [   96/ 4992]\n",
      "training loss: 0.705583  Batch: [  128/ 4992]\n",
      "training loss: 0.678112  Batch: [  160/ 4992]\n",
      "training loss: 0.703143  Batch: [  192/ 4992]\n",
      "training loss: 0.704242  Batch: [  224/ 4992]\n",
      "training loss: 0.682552  Batch: [  256/ 4992]\n",
      "training loss: 0.688553  Batch: [  288/ 4992]\n",
      "training loss: 0.685880  Batch: [  320/ 4992]\n",
      "training loss: 0.695764  Batch: [  352/ 4992]\n",
      "training loss: 0.668441  Batch: [  384/ 4992]\n",
      "training loss: 0.704159  Batch: [  416/ 4992]\n",
      "training loss: 0.685370  Batch: [  448/ 4992]\n",
      "training loss: 0.693794  Batch: [  480/ 4992]\n",
      "training loss: 0.677086  Batch: [  512/ 4992]\n",
      "training loss: 0.690632  Batch: [  544/ 4992]\n",
      "training loss: 0.681324  Batch: [  576/ 4992]\n",
      "training loss: 0.689159  Batch: [  608/ 4992]\n",
      "training loss: 0.678968  Batch: [  640/ 4992]\n",
      "training loss: 0.672921  Batch: [  672/ 4992]\n",
      "training loss: 0.674692  Batch: [  704/ 4992]\n",
      "training loss: 0.681377  Batch: [  736/ 4992]\n",
      "training loss: 0.692764  Batch: [  768/ 4992]\n",
      "training loss: 0.668565  Batch: [  800/ 4992]\n",
      "training loss: 0.659990  Batch: [  832/ 4992]\n",
      "training loss: 0.682770  Batch: [  864/ 4992]\n",
      "training loss: 0.660891  Batch: [  896/ 4992]\n",
      "training loss: 0.707112  Batch: [  928/ 4992]\n",
      "training loss: 0.682576  Batch: [  960/ 4992]\n",
      "training loss: 0.677009  Batch: [  992/ 4992]\n",
      "training loss: 0.715438  Batch: [ 1024/ 4992]\n",
      "training loss: 0.697079  Batch: [ 1056/ 4992]\n",
      "training loss: 0.689112  Batch: [ 1088/ 4992]\n",
      "training loss: 0.679797  Batch: [ 1120/ 4992]\n",
      "training loss: 0.672455  Batch: [ 1152/ 4992]\n",
      "training loss: 0.685368  Batch: [ 1184/ 4992]\n",
      "training loss: 0.620938  Batch: [ 1216/ 4992]\n",
      "training loss: 0.673653  Batch: [ 1248/ 4992]\n",
      "training loss: 0.672665  Batch: [ 1280/ 4992]\n",
      "training loss: 0.670981  Batch: [ 1312/ 4992]\n",
      "training loss: 0.696413  Batch: [ 1344/ 4992]\n",
      "training loss: 0.731207  Batch: [ 1376/ 4992]\n",
      "training loss: 0.660052  Batch: [ 1408/ 4992]\n",
      "training loss: 0.708382  Batch: [ 1440/ 4992]\n",
      "training loss: 0.674646  Batch: [ 1472/ 4992]\n",
      "training loss: 0.708492  Batch: [ 1504/ 4992]\n",
      "training loss: 0.690709  Batch: [ 1536/ 4992]\n",
      "training loss: 0.698596  Batch: [ 1568/ 4992]\n",
      "training loss: 0.677715  Batch: [ 1600/ 4992]\n",
      "training loss: 0.679787  Batch: [ 1632/ 4992]\n",
      "training loss: 0.692482  Batch: [ 1664/ 4992]\n",
      "training loss: 0.684216  Batch: [ 1696/ 4992]\n",
      "training loss: 0.682906  Batch: [ 1728/ 4992]\n",
      "training loss: 0.679608  Batch: [ 1760/ 4992]\n",
      "training loss: 0.691480  Batch: [ 1792/ 4992]\n",
      "training loss: 0.672530  Batch: [ 1824/ 4992]\n",
      "training loss: 0.690661  Batch: [ 1856/ 4992]\n",
      "training loss: 0.670668  Batch: [ 1888/ 4992]\n",
      "training loss: 0.696017  Batch: [ 1920/ 4992]\n",
      "training loss: 0.678362  Batch: [ 1952/ 4992]\n",
      "training loss: 0.694976  Batch: [ 1984/ 4992]\n",
      "training loss: 0.700801  Batch: [ 2016/ 4992]\n",
      "training loss: 0.690497  Batch: [ 2048/ 4992]\n",
      "training loss: 0.677115  Batch: [ 2080/ 4992]\n",
      "training loss: 0.659791  Batch: [ 2112/ 4992]\n",
      "training loss: 0.670379  Batch: [ 2144/ 4992]\n",
      "training loss: 0.674345  Batch: [ 2176/ 4992]\n",
      "training loss: 0.679290  Batch: [ 2208/ 4992]\n",
      "training loss: 0.689226  Batch: [ 2240/ 4992]\n",
      "training loss: 0.664980  Batch: [ 2272/ 4992]\n",
      "training loss: 0.667108  Batch: [ 2304/ 4992]\n",
      "training loss: 0.663532  Batch: [ 2336/ 4992]\n",
      "training loss: 0.713806  Batch: [ 2368/ 4992]\n",
      "training loss: 0.710867  Batch: [ 2400/ 4992]\n",
      "training loss: 0.659027  Batch: [ 2432/ 4992]\n",
      "training loss: 0.673782  Batch: [ 2464/ 4992]\n",
      "training loss: 0.681991  Batch: [ 2496/ 4992]\n",
      "training loss: 0.692821  Batch: [ 2528/ 4992]\n",
      "training loss: 0.682610  Batch: [ 2560/ 4992]\n",
      "training loss: 0.672963  Batch: [ 2592/ 4992]\n",
      "training loss: 0.663745  Batch: [ 2624/ 4992]\n",
      "training loss: 0.627536  Batch: [ 2656/ 4992]\n",
      "training loss: 0.680931  Batch: [ 2688/ 4992]\n",
      "training loss: 0.648859  Batch: [ 2720/ 4992]\n",
      "training loss: 0.685429  Batch: [ 2752/ 4992]\n",
      "training loss: 0.733433  Batch: [ 2784/ 4992]\n",
      "training loss: 0.712627  Batch: [ 2816/ 4992]\n",
      "training loss: 0.694829  Batch: [ 2848/ 4992]\n",
      "training loss: 0.646847  Batch: [ 2880/ 4992]\n",
      "training loss: 0.657142  Batch: [ 2912/ 4992]\n",
      "training loss: 0.672806  Batch: [ 2944/ 4992]\n",
      "training loss: 0.689352  Batch: [ 2976/ 4992]\n",
      "training loss: 0.684077  Batch: [ 3008/ 4992]\n",
      "training loss: 0.683986  Batch: [ 3040/ 4992]\n",
      "training loss: 0.695828  Batch: [ 3072/ 4992]\n",
      "training loss: 0.652882  Batch: [ 3104/ 4992]\n",
      "training loss: 0.679868  Batch: [ 3136/ 4992]\n",
      "training loss: 0.650390  Batch: [ 3168/ 4992]\n",
      "training loss: 0.706991  Batch: [ 3200/ 4992]\n",
      "training loss: 0.697633  Batch: [ 3232/ 4992]\n",
      "training loss: 0.689527  Batch: [ 3264/ 4992]\n",
      "training loss: 0.653964  Batch: [ 3296/ 4992]\n",
      "training loss: 0.710646  Batch: [ 3328/ 4992]\n",
      "training loss: 0.688926  Batch: [ 3360/ 4992]\n",
      "training loss: 0.707131  Batch: [ 3392/ 4992]\n",
      "training loss: 0.653481  Batch: [ 3424/ 4992]\n",
      "training loss: 0.680094  Batch: [ 3456/ 4992]\n",
      "training loss: 0.690145  Batch: [ 3488/ 4992]\n",
      "training loss: 0.648180  Batch: [ 3520/ 4992]\n",
      "training loss: 0.694201  Batch: [ 3552/ 4992]\n",
      "training loss: 0.654149  Batch: [ 3584/ 4992]\n",
      "training loss: 0.627995  Batch: [ 3616/ 4992]\n",
      "training loss: 0.702136  Batch: [ 3648/ 4992]\n",
      "training loss: 0.628496  Batch: [ 3680/ 4992]\n",
      "training loss: 0.726521  Batch: [ 3712/ 4992]\n",
      "training loss: 0.668868  Batch: [ 3744/ 4992]\n",
      "training loss: 0.671778  Batch: [ 3776/ 4992]\n",
      "training loss: 0.650305  Batch: [ 3808/ 4992]\n",
      "training loss: 0.697766  Batch: [ 3840/ 4992]\n",
      "training loss: 0.654708  Batch: [ 3872/ 4992]\n",
      "training loss: 0.692338  Batch: [ 3904/ 4992]\n",
      "training loss: 0.664273  Batch: [ 3936/ 4992]\n",
      "training loss: 0.656007  Batch: [ 3968/ 4992]\n",
      "training loss: 0.684218  Batch: [ 4000/ 4992]\n",
      "training loss: 0.707151  Batch: [ 4032/ 4992]\n",
      "training loss: 0.599468  Batch: [ 4064/ 4992]\n",
      "training loss: 0.692345  Batch: [ 4096/ 4992]\n",
      "training loss: 0.651526  Batch: [ 4128/ 4992]\n",
      "training loss: 0.654938  Batch: [ 4160/ 4992]\n",
      "training loss: 0.663562  Batch: [ 4192/ 4992]\n",
      "training loss: 0.685838  Batch: [ 4224/ 4992]\n",
      "training loss: 0.700564  Batch: [ 4256/ 4992]\n",
      "training loss: 0.702857  Batch: [ 4288/ 4992]\n",
      "training loss: 0.748211  Batch: [ 4320/ 4992]\n",
      "training loss: 0.677771  Batch: [ 4352/ 4992]\n",
      "training loss: 0.671290  Batch: [ 4384/ 4992]\n",
      "training loss: 0.645125  Batch: [ 4416/ 4992]\n",
      "training loss: 0.672966  Batch: [ 4448/ 4992]\n",
      "training loss: 0.710699  Batch: [ 4480/ 4992]\n",
      "training loss: 0.626763  Batch: [ 4512/ 4992]\n",
      "training loss: 0.673858  Batch: [ 4544/ 4992]\n",
      "training loss: 0.621137  Batch: [ 4576/ 4992]\n",
      "training loss: 0.656567  Batch: [ 4608/ 4992]\n",
      "training loss: 0.713352  Batch: [ 4640/ 4992]\n",
      "training loss: 0.639221  Batch: [ 4672/ 4992]\n",
      "training loss: 0.682396  Batch: [ 4704/ 4992]\n",
      "training loss: 0.663012  Batch: [ 4736/ 4992]\n",
      "training loss: 0.685843  Batch: [ 4768/ 4992]\n",
      "training loss: 0.702036  Batch: [ 4800/ 4992]\n",
      "training loss: 0.675538  Batch: [ 4832/ 4992]\n",
      "training loss: 0.659531  Batch: [ 4864/ 4992]\n",
      "training loss: 0.655949  Batch: [ 4896/ 4992]\n",
      "training loss: 0.676974  Batch: [ 4928/ 4992]\n",
      "training loss: 0.674501  Batch: [ 4960/ 4992]\n",
      "training loss: 0.616082  Batch: [ 4992/ 4992]\n",
      "Epoch 2/50, Training Loss: 21.7353\n",
      "validation loss: 0.700334  Batch: [   32/ 1070]\n",
      "validation loss: 0.681863  Batch: [   64/ 1070]\n",
      "validation loss: 0.650195  Batch: [   96/ 1070]\n",
      "validation loss: 0.664937  Batch: [  128/ 1070]\n",
      "validation loss: 0.688962  Batch: [  160/ 1070]\n",
      "validation loss: 0.646556  Batch: [  192/ 1070]\n",
      "validation loss: 0.676575  Batch: [  224/ 1070]\n",
      "validation loss: 0.680926  Batch: [  256/ 1070]\n",
      "validation loss: 0.703815  Batch: [  288/ 1070]\n",
      "validation loss: 0.632715  Batch: [  320/ 1070]\n",
      "validation loss: 0.637783  Batch: [  352/ 1070]\n",
      "validation loss: 0.653640  Batch: [  384/ 1070]\n",
      "validation loss: 0.626862  Batch: [  416/ 1070]\n",
      "validation loss: 0.650659  Batch: [  448/ 1070]\n",
      "validation loss: 0.706183  Batch: [  480/ 1070]\n",
      "validation loss: 0.651388  Batch: [  512/ 1070]\n",
      "validation loss: 0.655610  Batch: [  544/ 1070]\n",
      "validation loss: 0.692156  Batch: [  576/ 1070]\n",
      "validation loss: 0.652951  Batch: [  608/ 1070]\n",
      "validation loss: 0.681621  Batch: [  640/ 1070]\n",
      "validation loss: 0.623556  Batch: [  672/ 1070]\n",
      "validation loss: 0.665702  Batch: [  704/ 1070]\n",
      "validation loss: 0.710547  Batch: [  736/ 1070]\n",
      "validation loss: 0.624819  Batch: [  768/ 1070]\n",
      "validation loss: 0.675283  Batch: [  800/ 1070]\n",
      "validation loss: 0.677260  Batch: [  832/ 1070]\n",
      "validation loss: 0.708511  Batch: [  864/ 1070]\n",
      "validation loss: 0.674102  Batch: [  896/ 1070]\n",
      "validation loss: 0.669756  Batch: [  928/ 1070]\n",
      "validation loss: 0.669287  Batch: [  960/ 1070]\n",
      "validation loss: 0.688955  Batch: [  992/ 1070]\n",
      "validation loss: 0.672637  Batch: [ 1024/ 1070]\n",
      "validation loss: 0.686303  Batch: [ 1056/ 1070]\n",
      "validation loss: 0.689278  Batch: [ 1070/ 1070]\n",
      "Epoch 2/50, Validation Loss: 0.6694, Validation Accuracy: 60.28%\n",
      "\n",
      "\n",
      "Epoch 3 \n",
      "---------------------\n",
      "training loss: 0.688103  Batch: [   32/ 4992]\n",
      "training loss: 0.652033  Batch: [   64/ 4992]\n",
      "training loss: 0.657790  Batch: [   96/ 4992]\n",
      "training loss: 0.666213  Batch: [  128/ 4992]\n",
      "training loss: 0.670622  Batch: [  160/ 4992]\n",
      "training loss: 0.644628  Batch: [  192/ 4992]\n",
      "training loss: 0.678356  Batch: [  224/ 4992]\n",
      "training loss: 0.668676  Batch: [  256/ 4992]\n",
      "training loss: 0.647581  Batch: [  288/ 4992]\n",
      "training loss: 0.673230  Batch: [  320/ 4992]\n",
      "training loss: 0.647449  Batch: [  352/ 4992]\n",
      "training loss: 0.665514  Batch: [  384/ 4992]\n",
      "training loss: 0.745582  Batch: [  416/ 4992]\n",
      "training loss: 0.589899  Batch: [  448/ 4992]\n",
      "training loss: 0.624086  Batch: [  480/ 4992]\n",
      "training loss: 0.670870  Batch: [  512/ 4992]\n",
      "training loss: 0.651899  Batch: [  544/ 4992]\n",
      "training loss: 0.653208  Batch: [  576/ 4992]\n",
      "training loss: 0.638960  Batch: [  608/ 4992]\n",
      "training loss: 0.674033  Batch: [  640/ 4992]\n",
      "training loss: 0.671102  Batch: [  672/ 4992]\n",
      "training loss: 0.599293  Batch: [  704/ 4992]\n",
      "training loss: 0.612037  Batch: [  736/ 4992]\n",
      "training loss: 0.687573  Batch: [  768/ 4992]\n",
      "training loss: 0.744759  Batch: [  800/ 4992]\n",
      "training loss: 0.657406  Batch: [  832/ 4992]\n",
      "training loss: 0.709995  Batch: [  864/ 4992]\n",
      "training loss: 0.657186  Batch: [  896/ 4992]\n",
      "training loss: 0.622843  Batch: [  928/ 4992]\n",
      "training loss: 0.675014  Batch: [  960/ 4992]\n",
      "training loss: 0.654128  Batch: [  992/ 4992]\n",
      "training loss: 0.611031  Batch: [ 1024/ 4992]\n",
      "training loss: 0.644233  Batch: [ 1056/ 4992]\n",
      "training loss: 0.683780  Batch: [ 1088/ 4992]\n",
      "training loss: 0.741574  Batch: [ 1120/ 4992]\n",
      "training loss: 0.694681  Batch: [ 1152/ 4992]\n",
      "training loss: 0.609282  Batch: [ 1184/ 4992]\n",
      "training loss: 0.677234  Batch: [ 1216/ 4992]\n",
      "training loss: 0.642438  Batch: [ 1248/ 4992]\n",
      "training loss: 0.630463  Batch: [ 1280/ 4992]\n",
      "training loss: 0.591205  Batch: [ 1312/ 4992]\n",
      "training loss: 0.637091  Batch: [ 1344/ 4992]\n",
      "training loss: 0.615290  Batch: [ 1376/ 4992]\n",
      "training loss: 0.711353  Batch: [ 1408/ 4992]\n",
      "training loss: 0.712689  Batch: [ 1440/ 4992]\n",
      "training loss: 0.732336  Batch: [ 1472/ 4992]\n",
      "training loss: 0.605492  Batch: [ 1504/ 4992]\n",
      "training loss: 0.646357  Batch: [ 1536/ 4992]\n",
      "training loss: 0.683575  Batch: [ 1568/ 4992]\n",
      "training loss: 0.684327  Batch: [ 1600/ 4992]\n",
      "training loss: 0.735313  Batch: [ 1632/ 4992]\n",
      "training loss: 0.651248  Batch: [ 1664/ 4992]\n",
      "training loss: 0.637269  Batch: [ 1696/ 4992]\n",
      "training loss: 0.686918  Batch: [ 1728/ 4992]\n",
      "training loss: 0.697807  Batch: [ 1760/ 4992]\n",
      "training loss: 0.644857  Batch: [ 1792/ 4992]\n",
      "training loss: 0.592475  Batch: [ 1824/ 4992]\n",
      "training loss: 0.622794  Batch: [ 1856/ 4992]\n",
      "training loss: 0.639456  Batch: [ 1888/ 4992]\n",
      "training loss: 0.616843  Batch: [ 1920/ 4992]\n",
      "training loss: 0.653851  Batch: [ 1952/ 4992]\n",
      "training loss: 0.686883  Batch: [ 1984/ 4992]\n",
      "training loss: 0.579015  Batch: [ 2016/ 4992]\n",
      "training loss: 0.619334  Batch: [ 2048/ 4992]\n",
      "training loss: 0.632571  Batch: [ 2080/ 4992]\n",
      "training loss: 0.657154  Batch: [ 2112/ 4992]\n",
      "training loss: 0.693110  Batch: [ 2144/ 4992]\n",
      "training loss: 0.652112  Batch: [ 2176/ 4992]\n",
      "training loss: 0.677968  Batch: [ 2208/ 4992]\n",
      "training loss: 0.605464  Batch: [ 2240/ 4992]\n",
      "training loss: 0.671307  Batch: [ 2272/ 4992]\n",
      "training loss: 0.717173  Batch: [ 2304/ 4992]\n",
      "training loss: 0.629312  Batch: [ 2336/ 4992]\n",
      "training loss: 0.699970  Batch: [ 2368/ 4992]\n",
      "training loss: 0.665500  Batch: [ 2400/ 4992]\n",
      "training loss: 0.729252  Batch: [ 2432/ 4992]\n",
      "training loss: 0.860481  Batch: [ 2464/ 4992]\n",
      "training loss: 0.761239  Batch: [ 2496/ 4992]\n",
      "training loss: 0.693849  Batch: [ 2528/ 4992]\n",
      "training loss: 0.609340  Batch: [ 2560/ 4992]\n",
      "training loss: 0.664094  Batch: [ 2592/ 4992]\n",
      "training loss: 0.637330  Batch: [ 2624/ 4992]\n",
      "training loss: 0.690944  Batch: [ 2656/ 4992]\n",
      "training loss: 0.625734  Batch: [ 2688/ 4992]\n",
      "training loss: 0.619207  Batch: [ 2720/ 4992]\n",
      "training loss: 0.675235  Batch: [ 2752/ 4992]\n",
      "training loss: 0.632784  Batch: [ 2784/ 4992]\n",
      "training loss: 0.636794  Batch: [ 2816/ 4992]\n",
      "training loss: 0.610053  Batch: [ 2848/ 4992]\n",
      "training loss: 0.700342  Batch: [ 2880/ 4992]\n",
      "training loss: 0.646599  Batch: [ 2912/ 4992]\n",
      "training loss: 0.643035  Batch: [ 2944/ 4992]\n",
      "training loss: 0.618765  Batch: [ 2976/ 4992]\n",
      "training loss: 0.652571  Batch: [ 3008/ 4992]\n",
      "training loss: 0.632027  Batch: [ 3040/ 4992]\n",
      "training loss: 0.695092  Batch: [ 3072/ 4992]\n",
      "training loss: 0.633429  Batch: [ 3104/ 4992]\n",
      "training loss: 0.658606  Batch: [ 3136/ 4992]\n",
      "training loss: 0.671018  Batch: [ 3168/ 4992]\n",
      "training loss: 0.694993  Batch: [ 3200/ 4992]\n",
      "training loss: 0.688221  Batch: [ 3232/ 4992]\n",
      "training loss: 0.670644  Batch: [ 3264/ 4992]\n",
      "training loss: 0.640344  Batch: [ 3296/ 4992]\n",
      "training loss: 0.734995  Batch: [ 3328/ 4992]\n",
      "training loss: 0.631954  Batch: [ 3360/ 4992]\n",
      "training loss: 0.764320  Batch: [ 3392/ 4992]\n",
      "training loss: 0.856750  Batch: [ 3424/ 4992]\n",
      "training loss: 0.713511  Batch: [ 3456/ 4992]\n",
      "training loss: 0.634364  Batch: [ 3488/ 4992]\n",
      "training loss: 0.661101  Batch: [ 3520/ 4992]\n",
      "training loss: 0.597654  Batch: [ 3552/ 4992]\n",
      "training loss: 0.720148  Batch: [ 3584/ 4992]\n",
      "training loss: 0.731644  Batch: [ 3616/ 4992]\n",
      "training loss: 0.784908  Batch: [ 3648/ 4992]\n",
      "training loss: 0.641512  Batch: [ 3680/ 4992]\n",
      "training loss: 0.772677  Batch: [ 3712/ 4992]\n",
      "training loss: 0.730406  Batch: [ 3744/ 4992]\n",
      "training loss: 0.663373  Batch: [ 3776/ 4992]\n",
      "training loss: 0.757075  Batch: [ 3808/ 4992]\n",
      "training loss: 0.634800  Batch: [ 3840/ 4992]\n",
      "training loss: 0.738776  Batch: [ 3872/ 4992]\n",
      "training loss: 0.677389  Batch: [ 3904/ 4992]\n",
      "training loss: 0.695227  Batch: [ 3936/ 4992]\n",
      "training loss: 0.686621  Batch: [ 3968/ 4992]\n",
      "training loss: 0.656141  Batch: [ 4000/ 4992]\n",
      "training loss: 0.670027  Batch: [ 4032/ 4992]\n",
      "training loss: 0.680144  Batch: [ 4064/ 4992]\n",
      "training loss: 0.662505  Batch: [ 4096/ 4992]\n",
      "training loss: 0.691774  Batch: [ 4128/ 4992]\n",
      "training loss: 0.675818  Batch: [ 4160/ 4992]\n",
      "training loss: 0.672832  Batch: [ 4192/ 4992]\n",
      "training loss: 0.695895  Batch: [ 4224/ 4992]\n",
      "training loss: 0.687669  Batch: [ 4256/ 4992]\n",
      "training loss: 0.648292  Batch: [ 4288/ 4992]\n",
      "training loss: 0.656331  Batch: [ 4320/ 4992]\n",
      "training loss: 0.680576  Batch: [ 4352/ 4992]\n",
      "training loss: 0.644793  Batch: [ 4384/ 4992]\n",
      "training loss: 0.651185  Batch: [ 4416/ 4992]\n",
      "training loss: 0.654856  Batch: [ 4448/ 4992]\n",
      "training loss: 0.652702  Batch: [ 4480/ 4992]\n",
      "training loss: 0.688499  Batch: [ 4512/ 4992]\n",
      "training loss: 0.674600  Batch: [ 4544/ 4992]\n",
      "training loss: 0.638280  Batch: [ 4576/ 4992]\n",
      "training loss: 0.642714  Batch: [ 4608/ 4992]\n",
      "training loss: 0.671678  Batch: [ 4640/ 4992]\n",
      "training loss: 0.713548  Batch: [ 4672/ 4992]\n",
      "training loss: 0.699270  Batch: [ 4704/ 4992]\n",
      "training loss: 0.675070  Batch: [ 4736/ 4992]\n",
      "training loss: 0.674504  Batch: [ 4768/ 4992]\n",
      "training loss: 0.638237  Batch: [ 4800/ 4992]\n",
      "training loss: 0.647139  Batch: [ 4832/ 4992]\n",
      "training loss: 0.646168  Batch: [ 4864/ 4992]\n",
      "training loss: 0.753583  Batch: [ 4896/ 4992]\n",
      "training loss: 0.668638  Batch: [ 4928/ 4992]\n",
      "training loss: 0.664676  Batch: [ 4960/ 4992]\n",
      "training loss: 0.661660  Batch: [ 4992/ 4992]\n",
      "Epoch 3/50, Training Loss: 21.4115\n",
      "validation loss: 0.629936  Batch: [   32/ 1070]\n",
      "validation loss: 0.700742  Batch: [   64/ 1070]\n",
      "validation loss: 0.672817  Batch: [   96/ 1070]\n",
      "validation loss: 0.678812  Batch: [  128/ 1070]\n",
      "validation loss: 0.653018  Batch: [  160/ 1070]\n",
      "validation loss: 0.663809  Batch: [  192/ 1070]\n",
      "validation loss: 0.626567  Batch: [  224/ 1070]\n",
      "validation loss: 0.639454  Batch: [  256/ 1070]\n",
      "validation loss: 0.652097  Batch: [  288/ 1070]\n",
      "validation loss: 0.615852  Batch: [  320/ 1070]\n",
      "validation loss: 0.646677  Batch: [  352/ 1070]\n",
      "validation loss: 0.727376  Batch: [  384/ 1070]\n",
      "validation loss: 0.622856  Batch: [  416/ 1070]\n",
      "validation loss: 0.645132  Batch: [  448/ 1070]\n",
      "validation loss: 0.644773  Batch: [  480/ 1070]\n",
      "validation loss: 0.653107  Batch: [  512/ 1070]\n",
      "validation loss: 0.615343  Batch: [  544/ 1070]\n",
      "validation loss: 0.638149  Batch: [  576/ 1070]\n",
      "validation loss: 0.710597  Batch: [  608/ 1070]\n",
      "validation loss: 0.651968  Batch: [  640/ 1070]\n",
      "validation loss: 0.624014  Batch: [  672/ 1070]\n",
      "validation loss: 0.672642  Batch: [  704/ 1070]\n",
      "validation loss: 0.654360  Batch: [  736/ 1070]\n",
      "validation loss: 0.646070  Batch: [  768/ 1070]\n",
      "validation loss: 0.641195  Batch: [  800/ 1070]\n",
      "validation loss: 0.639803  Batch: [  832/ 1070]\n",
      "validation loss: 0.686832  Batch: [  864/ 1070]\n",
      "validation loss: 0.627169  Batch: [  896/ 1070]\n",
      "validation loss: 0.653648  Batch: [  928/ 1070]\n",
      "validation loss: 0.626019  Batch: [  960/ 1070]\n",
      "validation loss: 0.687503  Batch: [  992/ 1070]\n",
      "validation loss: 0.634573  Batch: [ 1024/ 1070]\n",
      "validation loss: 0.683569  Batch: [ 1056/ 1070]\n",
      "validation loss: 0.649686  Batch: [ 1070/ 1070]\n",
      "Epoch 3/50, Validation Loss: 0.6535, Validation Accuracy: 62.71%\n",
      "\n",
      "\n",
      "Epoch 4 \n",
      "---------------------\n",
      "training loss: 0.709625  Batch: [   32/ 4992]\n",
      "training loss: 0.695615  Batch: [   64/ 4992]\n",
      "training loss: 0.616149  Batch: [   96/ 4992]\n",
      "training loss: 0.644961  Batch: [  128/ 4992]\n",
      "training loss: 0.690100  Batch: [  160/ 4992]\n",
      "training loss: 0.608829  Batch: [  192/ 4992]\n",
      "training loss: 0.639938  Batch: [  224/ 4992]\n",
      "training loss: 0.651755  Batch: [  256/ 4992]\n",
      "training loss: 0.659153  Batch: [  288/ 4992]\n",
      "training loss: 0.631149  Batch: [  320/ 4992]\n",
      "training loss: 0.621136  Batch: [  352/ 4992]\n",
      "training loss: 0.641382  Batch: [  384/ 4992]\n",
      "training loss: 0.637035  Batch: [  416/ 4992]\n",
      "training loss: 0.601783  Batch: [  448/ 4992]\n",
      "training loss: 0.658311  Batch: [  480/ 4992]\n",
      "training loss: 0.634733  Batch: [  512/ 4992]\n",
      "training loss: 0.642133  Batch: [  544/ 4992]\n",
      "training loss: 0.661626  Batch: [  576/ 4992]\n",
      "training loss: 0.609771  Batch: [  608/ 4992]\n",
      "training loss: 0.648523  Batch: [  640/ 4992]\n",
      "training loss: 0.589899  Batch: [  672/ 4992]\n",
      "training loss: 0.642985  Batch: [  704/ 4992]\n",
      "training loss: 0.662905  Batch: [  736/ 4992]\n",
      "training loss: 0.636885  Batch: [  768/ 4992]\n",
      "training loss: 0.662628  Batch: [  800/ 4992]\n",
      "training loss: 0.631473  Batch: [  832/ 4992]\n",
      "training loss: 0.595742  Batch: [  864/ 4992]\n",
      "training loss: 0.679218  Batch: [  896/ 4992]\n",
      "training loss: 0.673650  Batch: [  928/ 4992]\n",
      "training loss: 0.631008  Batch: [  960/ 4992]\n",
      "training loss: 0.650598  Batch: [  992/ 4992]\n",
      "training loss: 0.628068  Batch: [ 1024/ 4992]\n",
      "training loss: 0.603896  Batch: [ 1056/ 4992]\n",
      "training loss: 0.642646  Batch: [ 1088/ 4992]\n",
      "training loss: 0.679213  Batch: [ 1120/ 4992]\n",
      "training loss: 0.531798  Batch: [ 1152/ 4992]\n",
      "training loss: 0.614366  Batch: [ 1184/ 4992]\n",
      "training loss: 0.620107  Batch: [ 1216/ 4992]\n",
      "training loss: 0.545801  Batch: [ 1248/ 4992]\n",
      "training loss: 0.677445  Batch: [ 1280/ 4992]\n",
      "training loss: 0.614373  Batch: [ 1312/ 4992]\n",
      "training loss: 0.607104  Batch: [ 1344/ 4992]\n",
      "training loss: 0.626321  Batch: [ 1376/ 4992]\n",
      "training loss: 0.603329  Batch: [ 1408/ 4992]\n",
      "training loss: 0.651735  Batch: [ 1440/ 4992]\n",
      "training loss: 0.708313  Batch: [ 1472/ 4992]\n",
      "training loss: 0.644165  Batch: [ 1504/ 4992]\n",
      "training loss: 0.587710  Batch: [ 1536/ 4992]\n",
      "training loss: 0.604396  Batch: [ 1568/ 4992]\n",
      "training loss: 0.560966  Batch: [ 1600/ 4992]\n",
      "training loss: 0.609468  Batch: [ 1632/ 4992]\n",
      "training loss: 0.632715  Batch: [ 1664/ 4992]\n",
      "training loss: 0.618190  Batch: [ 1696/ 4992]\n",
      "training loss: 0.614671  Batch: [ 1728/ 4992]\n",
      "training loss: 0.566760  Batch: [ 1760/ 4992]\n",
      "training loss: 0.622099  Batch: [ 1792/ 4992]\n",
      "training loss: 0.748033  Batch: [ 1824/ 4992]\n",
      "training loss: 0.526611  Batch: [ 1856/ 4992]\n",
      "training loss: 0.692934  Batch: [ 1888/ 4992]\n",
      "training loss: 0.510979  Batch: [ 1920/ 4992]\n",
      "training loss: 0.535736  Batch: [ 1952/ 4992]\n",
      "training loss: 0.686927  Batch: [ 1984/ 4992]\n",
      "training loss: 0.641024  Batch: [ 2016/ 4992]\n",
      "training loss: 0.562178  Batch: [ 2048/ 4992]\n",
      "training loss: 0.686998  Batch: [ 2080/ 4992]\n",
      "training loss: 0.699001  Batch: [ 2112/ 4992]\n",
      "training loss: 0.652053  Batch: [ 2144/ 4992]\n",
      "training loss: 0.692890  Batch: [ 2176/ 4992]\n",
      "training loss: 0.572965  Batch: [ 2208/ 4992]\n",
      "training loss: 0.538665  Batch: [ 2240/ 4992]\n",
      "training loss: 0.680698  Batch: [ 2272/ 4992]\n",
      "training loss: 0.635841  Batch: [ 2304/ 4992]\n",
      "training loss: 0.746547  Batch: [ 2336/ 4992]\n",
      "training loss: 0.630503  Batch: [ 2368/ 4992]\n",
      "training loss: 0.692614  Batch: [ 2400/ 4992]\n",
      "training loss: 0.683157  Batch: [ 2432/ 4992]\n",
      "training loss: 0.620524  Batch: [ 2464/ 4992]\n",
      "training loss: 0.577960  Batch: [ 2496/ 4992]\n",
      "training loss: 0.572998  Batch: [ 2528/ 4992]\n",
      "training loss: 0.650417  Batch: [ 2560/ 4992]\n",
      "training loss: 0.600260  Batch: [ 2592/ 4992]\n",
      "training loss: 0.570070  Batch: [ 2624/ 4992]\n",
      "training loss: 0.673371  Batch: [ 2656/ 4992]\n",
      "training loss: 0.590160  Batch: [ 2688/ 4992]\n",
      "training loss: 0.627915  Batch: [ 2720/ 4992]\n",
      "training loss: 0.570994  Batch: [ 2752/ 4992]\n",
      "training loss: 0.597305  Batch: [ 2784/ 4992]\n",
      "training loss: 0.563520  Batch: [ 2816/ 4992]\n",
      "training loss: 0.538408  Batch: [ 2848/ 4992]\n",
      "training loss: 0.712541  Batch: [ 2880/ 4992]\n",
      "training loss: 0.589266  Batch: [ 2912/ 4992]\n",
      "training loss: 0.579464  Batch: [ 2944/ 4992]\n",
      "training loss: 0.593131  Batch: [ 2976/ 4992]\n",
      "training loss: 0.556214  Batch: [ 3008/ 4992]\n",
      "training loss: 0.662374  Batch: [ 3040/ 4992]\n",
      "training loss: 0.535058  Batch: [ 3072/ 4992]\n",
      "training loss: 0.561369  Batch: [ 3104/ 4992]\n",
      "training loss: 0.628174  Batch: [ 3136/ 4992]\n",
      "training loss: 0.680051  Batch: [ 3168/ 4992]\n",
      "training loss: 0.586285  Batch: [ 3200/ 4992]\n",
      "training loss: 0.561954  Batch: [ 3232/ 4992]\n",
      "training loss: 0.513180  Batch: [ 3264/ 4992]\n",
      "training loss: 0.664229  Batch: [ 3296/ 4992]\n",
      "training loss: 0.564218  Batch: [ 3328/ 4992]\n",
      "training loss: 0.615848  Batch: [ 3360/ 4992]\n",
      "training loss: 0.639408  Batch: [ 3392/ 4992]\n",
      "training loss: 0.596453  Batch: [ 3424/ 4992]\n",
      "training loss: 0.466324  Batch: [ 3456/ 4992]\n",
      "training loss: 0.527667  Batch: [ 3488/ 4992]\n",
      "training loss: 0.706505  Batch: [ 3520/ 4992]\n",
      "training loss: 0.612863  Batch: [ 3552/ 4992]\n",
      "training loss: 0.581855  Batch: [ 3584/ 4992]\n",
      "training loss: 0.543911  Batch: [ 3616/ 4992]\n",
      "training loss: 0.596864  Batch: [ 3648/ 4992]\n",
      "training loss: 0.623481  Batch: [ 3680/ 4992]\n",
      "training loss: 0.658653  Batch: [ 3712/ 4992]\n",
      "training loss: 0.589141  Batch: [ 3744/ 4992]\n",
      "training loss: 0.614903  Batch: [ 3776/ 4992]\n",
      "training loss: 0.471212  Batch: [ 3808/ 4992]\n",
      "training loss: 0.621837  Batch: [ 3840/ 4992]\n",
      "training loss: 0.594065  Batch: [ 3872/ 4992]\n",
      "training loss: 0.616295  Batch: [ 3904/ 4992]\n",
      "training loss: 0.542401  Batch: [ 3936/ 4992]\n",
      "training loss: 0.656922  Batch: [ 3968/ 4992]\n",
      "training loss: 0.626991  Batch: [ 4000/ 4992]\n",
      "training loss: 0.527151  Batch: [ 4032/ 4992]\n",
      "training loss: 0.653580  Batch: [ 4064/ 4992]\n",
      "training loss: 0.560210  Batch: [ 4096/ 4992]\n",
      "training loss: 0.557088  Batch: [ 4128/ 4992]\n",
      "training loss: 0.706892  Batch: [ 4160/ 4992]\n",
      "training loss: 0.529454  Batch: [ 4192/ 4992]\n",
      "training loss: 0.599068  Batch: [ 4224/ 4992]\n",
      "training loss: 0.625773  Batch: [ 4256/ 4992]\n",
      "training loss: 0.606224  Batch: [ 4288/ 4992]\n",
      "training loss: 0.651176  Batch: [ 4320/ 4992]\n",
      "training loss: 0.574185  Batch: [ 4352/ 4992]\n",
      "training loss: 0.547190  Batch: [ 4384/ 4992]\n",
      "training loss: 0.609793  Batch: [ 4416/ 4992]\n",
      "training loss: 0.539411  Batch: [ 4448/ 4992]\n",
      "training loss: 0.539908  Batch: [ 4480/ 4992]\n",
      "training loss: 0.618188  Batch: [ 4512/ 4992]\n",
      "training loss: 0.550435  Batch: [ 4544/ 4992]\n",
      "training loss: 0.659993  Batch: [ 4576/ 4992]\n",
      "training loss: 0.596946  Batch: [ 4608/ 4992]\n",
      "training loss: 0.604167  Batch: [ 4640/ 4992]\n",
      "training loss: 0.496764  Batch: [ 4672/ 4992]\n",
      "training loss: 0.582191  Batch: [ 4704/ 4992]\n",
      "training loss: 0.657323  Batch: [ 4736/ 4992]\n",
      "training loss: 0.496211  Batch: [ 4768/ 4992]\n",
      "training loss: 0.554811  Batch: [ 4800/ 4992]\n",
      "training loss: 0.554412  Batch: [ 4832/ 4992]\n",
      "training loss: 0.504005  Batch: [ 4864/ 4992]\n",
      "training loss: 0.681416  Batch: [ 4896/ 4992]\n",
      "training loss: 0.606686  Batch: [ 4928/ 4992]\n",
      "training loss: 0.446459  Batch: [ 4960/ 4992]\n",
      "training loss: 0.541596  Batch: [ 4992/ 4992]\n",
      "Epoch 4/50, Training Loss: 19.5726\n",
      "validation loss: 0.655089  Batch: [   32/ 1070]\n",
      "validation loss: 0.699279  Batch: [   64/ 1070]\n",
      "validation loss: 0.488626  Batch: [   96/ 1070]\n",
      "validation loss: 0.565794  Batch: [  128/ 1070]\n",
      "validation loss: 0.599325  Batch: [  160/ 1070]\n",
      "validation loss: 0.647098  Batch: [  192/ 1070]\n",
      "validation loss: 0.660474  Batch: [  224/ 1070]\n",
      "validation loss: 0.623972  Batch: [  256/ 1070]\n",
      "validation loss: 0.466699  Batch: [  288/ 1070]\n",
      "validation loss: 0.659349  Batch: [  320/ 1070]\n",
      "validation loss: 0.576951  Batch: [  352/ 1070]\n",
      "validation loss: 0.432602  Batch: [  384/ 1070]\n",
      "validation loss: 0.507398  Batch: [  416/ 1070]\n",
      "validation loss: 0.514042  Batch: [  448/ 1070]\n",
      "validation loss: 0.710484  Batch: [  480/ 1070]\n",
      "validation loss: 0.524874  Batch: [  512/ 1070]\n",
      "validation loss: 0.748019  Batch: [  544/ 1070]\n",
      "validation loss: 0.454976  Batch: [  576/ 1070]\n",
      "validation loss: 0.558795  Batch: [  608/ 1070]\n",
      "validation loss: 0.561882  Batch: [  640/ 1070]\n",
      "validation loss: 0.603683  Batch: [  672/ 1070]\n",
      "validation loss: 0.533274  Batch: [  704/ 1070]\n",
      "validation loss: 0.527221  Batch: [  736/ 1070]\n",
      "validation loss: 0.627114  Batch: [  768/ 1070]\n",
      "validation loss: 0.517617  Batch: [  800/ 1070]\n",
      "validation loss: 0.574277  Batch: [  832/ 1070]\n",
      "validation loss: 0.496775  Batch: [  864/ 1070]\n",
      "validation loss: 0.535266  Batch: [  896/ 1070]\n",
      "validation loss: 0.520094  Batch: [  928/ 1070]\n",
      "validation loss: 0.711554  Batch: [  960/ 1070]\n",
      "validation loss: 0.544256  Batch: [  992/ 1070]\n",
      "validation loss: 0.475170  Batch: [ 1024/ 1070]\n",
      "validation loss: 0.736452  Batch: [ 1056/ 1070]\n",
      "validation loss: 0.672942  Batch: [ 1070/ 1070]\n",
      "Epoch 4/50, Validation Loss: 0.5788, Validation Accuracy: 69.35%\n",
      "\n",
      "\n",
      "Epoch 5 \n",
      "---------------------\n",
      "training loss: 0.676912  Batch: [   32/ 4992]\n",
      "training loss: 0.540503  Batch: [   64/ 4992]\n",
      "training loss: 0.607650  Batch: [   96/ 4992]\n",
      "training loss: 0.597790  Batch: [  128/ 4992]\n",
      "training loss: 0.556844  Batch: [  160/ 4992]\n",
      "training loss: 0.578172  Batch: [  192/ 4992]\n",
      "training loss: 0.521213  Batch: [  224/ 4992]\n",
      "training loss: 0.500255  Batch: [  256/ 4992]\n",
      "training loss: 0.659389  Batch: [  288/ 4992]\n",
      "training loss: 0.538282  Batch: [  320/ 4992]\n",
      "training loss: 0.649741  Batch: [  352/ 4992]\n",
      "training loss: 0.607739  Batch: [  384/ 4992]\n",
      "training loss: 0.698074  Batch: [  416/ 4992]\n",
      "training loss: 0.531351  Batch: [  448/ 4992]\n",
      "training loss: 0.634143  Batch: [  480/ 4992]\n",
      "training loss: 0.647178  Batch: [  512/ 4992]\n",
      "training loss: 0.431652  Batch: [  544/ 4992]\n",
      "training loss: 0.469498  Batch: [  576/ 4992]\n",
      "training loss: 0.552036  Batch: [  608/ 4992]\n",
      "training loss: 0.561992  Batch: [  640/ 4992]\n",
      "training loss: 0.645826  Batch: [  672/ 4992]\n",
      "training loss: 0.601597  Batch: [  704/ 4992]\n",
      "training loss: 0.530016  Batch: [  736/ 4992]\n",
      "training loss: 0.673002  Batch: [  768/ 4992]\n",
      "training loss: 0.642153  Batch: [  800/ 4992]\n",
      "training loss: 0.742860  Batch: [  832/ 4992]\n",
      "training loss: 0.599067  Batch: [  864/ 4992]\n",
      "training loss: 0.603330  Batch: [  896/ 4992]\n",
      "training loss: 0.583205  Batch: [  928/ 4992]\n",
      "training loss: 0.513699  Batch: [  960/ 4992]\n",
      "training loss: 0.553069  Batch: [  992/ 4992]\n",
      "training loss: 0.474572  Batch: [ 1024/ 4992]\n",
      "training loss: 0.642808  Batch: [ 1056/ 4992]\n",
      "training loss: 0.610535  Batch: [ 1088/ 4992]\n",
      "training loss: 0.653482  Batch: [ 1120/ 4992]\n",
      "training loss: 0.575710  Batch: [ 1152/ 4992]\n",
      "training loss: 0.483855  Batch: [ 1184/ 4992]\n",
      "training loss: 0.625301  Batch: [ 1216/ 4992]\n",
      "training loss: 0.741829  Batch: [ 1248/ 4992]\n",
      "training loss: 0.593907  Batch: [ 1280/ 4992]\n",
      "training loss: 0.562958  Batch: [ 1312/ 4992]\n",
      "training loss: 0.669317  Batch: [ 1344/ 4992]\n",
      "training loss: 0.572229  Batch: [ 1376/ 4992]\n",
      "training loss: 0.595258  Batch: [ 1408/ 4992]\n",
      "training loss: 0.588428  Batch: [ 1440/ 4992]\n",
      "training loss: 0.630217  Batch: [ 1472/ 4992]\n",
      "training loss: 0.665642  Batch: [ 1504/ 4992]\n",
      "training loss: 0.586833  Batch: [ 1536/ 4992]\n",
      "training loss: 0.588250  Batch: [ 1568/ 4992]\n",
      "training loss: 0.522714  Batch: [ 1600/ 4992]\n",
      "training loss: 0.722259  Batch: [ 1632/ 4992]\n",
      "training loss: 0.729702  Batch: [ 1664/ 4992]\n",
      "training loss: 0.585613  Batch: [ 1696/ 4992]\n",
      "training loss: 0.460893  Batch: [ 1728/ 4992]\n",
      "training loss: 0.540892  Batch: [ 1760/ 4992]\n",
      "training loss: 0.551602  Batch: [ 1792/ 4992]\n",
      "training loss: 0.619960  Batch: [ 1824/ 4992]\n",
      "training loss: 0.581551  Batch: [ 1856/ 4992]\n",
      "training loss: 0.535622  Batch: [ 1888/ 4992]\n",
      "training loss: 0.542082  Batch: [ 1920/ 4992]\n",
      "training loss: 0.569500  Batch: [ 1952/ 4992]\n",
      "training loss: 0.539462  Batch: [ 1984/ 4992]\n",
      "training loss: 0.489218  Batch: [ 2016/ 4992]\n",
      "training loss: 0.520845  Batch: [ 2048/ 4992]\n",
      "training loss: 0.549379  Batch: [ 2080/ 4992]\n",
      "training loss: 0.708793  Batch: [ 2112/ 4992]\n",
      "training loss: 0.815318  Batch: [ 2144/ 4992]\n",
      "training loss: 0.540790  Batch: [ 2176/ 4992]\n",
      "training loss: 0.559348  Batch: [ 2208/ 4992]\n",
      "training loss: 0.624474  Batch: [ 2240/ 4992]\n",
      "training loss: 0.572259  Batch: [ 2272/ 4992]\n",
      "training loss: 0.551997  Batch: [ 2304/ 4992]\n",
      "training loss: 0.684130  Batch: [ 2336/ 4992]\n",
      "training loss: 0.525760  Batch: [ 2368/ 4992]\n",
      "training loss: 0.504950  Batch: [ 2400/ 4992]\n",
      "training loss: 0.618659  Batch: [ 2432/ 4992]\n",
      "training loss: 0.627488  Batch: [ 2464/ 4992]\n",
      "training loss: 0.604890  Batch: [ 2496/ 4992]\n",
      "training loss: 0.579674  Batch: [ 2528/ 4992]\n",
      "training loss: 0.531245  Batch: [ 2560/ 4992]\n",
      "training loss: 0.421998  Batch: [ 2592/ 4992]\n",
      "training loss: 0.578121  Batch: [ 2624/ 4992]\n",
      "training loss: 0.518995  Batch: [ 2656/ 4992]\n",
      "training loss: 0.532444  Batch: [ 2688/ 4992]\n",
      "training loss: 0.534146  Batch: [ 2720/ 4992]\n",
      "training loss: 0.559686  Batch: [ 2752/ 4992]\n",
      "training loss: 0.678097  Batch: [ 2784/ 4992]\n",
      "training loss: 0.521808  Batch: [ 2816/ 4992]\n",
      "training loss: 0.620176  Batch: [ 2848/ 4992]\n",
      "training loss: 0.483091  Batch: [ 2880/ 4992]\n",
      "training loss: 0.471214  Batch: [ 2912/ 4992]\n",
      "training loss: 0.608371  Batch: [ 2944/ 4992]\n",
      "training loss: 0.580185  Batch: [ 2976/ 4992]\n",
      "training loss: 0.704725  Batch: [ 3008/ 4992]\n",
      "training loss: 0.523959  Batch: [ 3040/ 4992]\n",
      "training loss: 0.571533  Batch: [ 3072/ 4992]\n",
      "training loss: 0.716697  Batch: [ 3104/ 4992]\n",
      "training loss: 0.690423  Batch: [ 3136/ 4992]\n",
      "training loss: 0.485882  Batch: [ 3168/ 4992]\n",
      "training loss: 0.470433  Batch: [ 3200/ 4992]\n",
      "training loss: 0.471295  Batch: [ 3232/ 4992]\n",
      "training loss: 0.549282  Batch: [ 3264/ 4992]\n",
      "training loss: 0.618351  Batch: [ 3296/ 4992]\n",
      "training loss: 0.510135  Batch: [ 3328/ 4992]\n",
      "training loss: 0.561698  Batch: [ 3360/ 4992]\n",
      "training loss: 0.538662  Batch: [ 3392/ 4992]\n",
      "training loss: 0.574516  Batch: [ 3424/ 4992]\n",
      "training loss: 0.470479  Batch: [ 3456/ 4992]\n",
      "training loss: 0.742230  Batch: [ 3488/ 4992]\n",
      "training loss: 0.553152  Batch: [ 3520/ 4992]\n",
      "training loss: 0.612069  Batch: [ 3552/ 4992]\n",
      "training loss: 0.646839  Batch: [ 3584/ 4992]\n",
      "training loss: 0.573466  Batch: [ 3616/ 4992]\n",
      "training loss: 0.721632  Batch: [ 3648/ 4992]\n",
      "training loss: 0.611732  Batch: [ 3680/ 4992]\n",
      "training loss: 0.492424  Batch: [ 3712/ 4992]\n",
      "training loss: 0.564920  Batch: [ 3744/ 4992]\n",
      "training loss: 0.503799  Batch: [ 3776/ 4992]\n",
      "training loss: 0.510007  Batch: [ 3808/ 4992]\n",
      "training loss: 0.500399  Batch: [ 3840/ 4992]\n",
      "training loss: 0.670207  Batch: [ 3872/ 4992]\n",
      "training loss: 0.561646  Batch: [ 3904/ 4992]\n",
      "training loss: 0.526803  Batch: [ 3936/ 4992]\n",
      "training loss: 0.624372  Batch: [ 3968/ 4992]\n",
      "training loss: 0.355980  Batch: [ 4000/ 4992]\n",
      "training loss: 0.723868  Batch: [ 4032/ 4992]\n",
      "training loss: 0.683608  Batch: [ 4064/ 4992]\n",
      "training loss: 0.695154  Batch: [ 4096/ 4992]\n",
      "training loss: 0.592504  Batch: [ 4128/ 4992]\n",
      "training loss: 0.581737  Batch: [ 4160/ 4992]\n",
      "training loss: 0.468253  Batch: [ 4192/ 4992]\n",
      "training loss: 0.553910  Batch: [ 4224/ 4992]\n",
      "training loss: 0.525154  Batch: [ 4256/ 4992]\n",
      "training loss: 0.577569  Batch: [ 4288/ 4992]\n",
      "training loss: 0.556955  Batch: [ 4320/ 4992]\n",
      "training loss: 0.668546  Batch: [ 4352/ 4992]\n",
      "training loss: 0.589049  Batch: [ 4384/ 4992]\n",
      "training loss: 0.540258  Batch: [ 4416/ 4992]\n",
      "training loss: 0.474705  Batch: [ 4448/ 4992]\n",
      "training loss: 0.628925  Batch: [ 4480/ 4992]\n",
      "training loss: 0.604477  Batch: [ 4512/ 4992]\n",
      "training loss: 0.544204  Batch: [ 4544/ 4992]\n",
      "training loss: 0.644567  Batch: [ 4576/ 4992]\n",
      "training loss: 0.527656  Batch: [ 4608/ 4992]\n",
      "training loss: 0.651812  Batch: [ 4640/ 4992]\n",
      "training loss: 0.525447  Batch: [ 4672/ 4992]\n",
      "training loss: 0.592607  Batch: [ 4704/ 4992]\n",
      "training loss: 0.620365  Batch: [ 4736/ 4992]\n",
      "training loss: 0.630878  Batch: [ 4768/ 4992]\n",
      "training loss: 0.627247  Batch: [ 4800/ 4992]\n",
      "training loss: 0.567694  Batch: [ 4832/ 4992]\n",
      "training loss: 0.581348  Batch: [ 4864/ 4992]\n",
      "training loss: 0.534804  Batch: [ 4896/ 4992]\n",
      "training loss: 0.610269  Batch: [ 4928/ 4992]\n",
      "training loss: 0.502289  Batch: [ 4960/ 4992]\n",
      "training loss: 0.510048  Batch: [ 4992/ 4992]\n",
      "Epoch 5/50, Training Loss: 18.6040\n",
      "validation loss: 0.596481  Batch: [   32/ 1070]\n",
      "validation loss: 0.603964  Batch: [   64/ 1070]\n",
      "validation loss: 0.530873  Batch: [   96/ 1070]\n",
      "validation loss: 0.635810  Batch: [  128/ 1070]\n",
      "validation loss: 0.646313  Batch: [  160/ 1070]\n",
      "validation loss: 0.585101  Batch: [  192/ 1070]\n",
      "validation loss: 0.536475  Batch: [  224/ 1070]\n",
      "validation loss: 0.506881  Batch: [  256/ 1070]\n",
      "validation loss: 0.695518  Batch: [  288/ 1070]\n",
      "validation loss: 0.489268  Batch: [  320/ 1070]\n",
      "validation loss: 0.639707  Batch: [  352/ 1070]\n",
      "validation loss: 0.641724  Batch: [  384/ 1070]\n",
      "validation loss: 0.599208  Batch: [  416/ 1070]\n",
      "validation loss: 0.439182  Batch: [  448/ 1070]\n",
      "validation loss: 0.451932  Batch: [  480/ 1070]\n",
      "validation loss: 0.527093  Batch: [  512/ 1070]\n",
      "validation loss: 0.510451  Batch: [  544/ 1070]\n",
      "validation loss: 0.596081  Batch: [  576/ 1070]\n",
      "validation loss: 0.531420  Batch: [  608/ 1070]\n",
      "validation loss: 0.557286  Batch: [  640/ 1070]\n",
      "validation loss: 0.659737  Batch: [  672/ 1070]\n",
      "validation loss: 0.502354  Batch: [  704/ 1070]\n",
      "validation loss: 0.526166  Batch: [  736/ 1070]\n",
      "validation loss: 0.528665  Batch: [  768/ 1070]\n",
      "validation loss: 0.619704  Batch: [  800/ 1070]\n",
      "validation loss: 0.675655  Batch: [  832/ 1070]\n",
      "validation loss: 0.639316  Batch: [  864/ 1070]\n",
      "validation loss: 0.535866  Batch: [  896/ 1070]\n",
      "validation loss: 0.579184  Batch: [  928/ 1070]\n",
      "validation loss: 0.630012  Batch: [  960/ 1070]\n",
      "validation loss: 0.606422  Batch: [  992/ 1070]\n",
      "validation loss: 0.631546  Batch: [ 1024/ 1070]\n",
      "validation loss: 0.605423  Batch: [ 1056/ 1070]\n",
      "validation loss: 0.763326  Batch: [ 1070/ 1070]\n",
      "Epoch 5/50, Validation Loss: 0.5800, Validation Accuracy: 70.56%\n",
      "\n",
      "\n",
      "Epoch 6 \n",
      "---------------------\n",
      "training loss: 0.643843  Batch: [   32/ 4992]\n",
      "training loss: 0.511314  Batch: [   64/ 4992]\n",
      "training loss: 0.499398  Batch: [   96/ 4992]\n",
      "training loss: 0.687504  Batch: [  128/ 4992]\n",
      "training loss: 0.517007  Batch: [  160/ 4992]\n",
      "training loss: 0.507877  Batch: [  192/ 4992]\n",
      "training loss: 0.513709  Batch: [  224/ 4992]\n",
      "training loss: 0.700828  Batch: [  256/ 4992]\n",
      "training loss: 0.622689  Batch: [  288/ 4992]\n",
      "training loss: 0.504832  Batch: [  320/ 4992]\n",
      "training loss: 0.428243  Batch: [  352/ 4992]\n",
      "training loss: 0.643469  Batch: [  384/ 4992]\n",
      "training loss: 0.640888  Batch: [  416/ 4992]\n",
      "training loss: 0.514151  Batch: [  448/ 4992]\n",
      "training loss: 0.551381  Batch: [  480/ 4992]\n",
      "training loss: 0.522222  Batch: [  512/ 4992]\n",
      "training loss: 0.659539  Batch: [  544/ 4992]\n",
      "training loss: 0.644057  Batch: [  576/ 4992]\n",
      "training loss: 0.462057  Batch: [  608/ 4992]\n",
      "training loss: 0.798650  Batch: [  640/ 4992]\n",
      "training loss: 0.618341  Batch: [  672/ 4992]\n",
      "training loss: 0.528509  Batch: [  704/ 4992]\n",
      "training loss: 0.499319  Batch: [  736/ 4992]\n",
      "training loss: 0.660977  Batch: [  768/ 4992]\n",
      "training loss: 0.588916  Batch: [  800/ 4992]\n",
      "training loss: 0.467355  Batch: [  832/ 4992]\n",
      "training loss: 0.455552  Batch: [  864/ 4992]\n",
      "training loss: 0.594438  Batch: [  896/ 4992]\n",
      "training loss: 0.548124  Batch: [  928/ 4992]\n",
      "training loss: 0.556933  Batch: [  960/ 4992]\n",
      "training loss: 0.476187  Batch: [  992/ 4992]\n",
      "training loss: 0.535443  Batch: [ 1024/ 4992]\n",
      "training loss: 0.676625  Batch: [ 1056/ 4992]\n",
      "training loss: 0.518147  Batch: [ 1088/ 4992]\n",
      "training loss: 0.480801  Batch: [ 1120/ 4992]\n",
      "training loss: 0.480719  Batch: [ 1152/ 4992]\n",
      "training loss: 0.542473  Batch: [ 1184/ 4992]\n",
      "training loss: 0.666824  Batch: [ 1216/ 4992]\n",
      "training loss: 0.576280  Batch: [ 1248/ 4992]\n",
      "training loss: 0.510943  Batch: [ 1280/ 4992]\n",
      "training loss: 0.487705  Batch: [ 1312/ 4992]\n",
      "training loss: 0.534519  Batch: [ 1344/ 4992]\n",
      "training loss: 0.470554  Batch: [ 1376/ 4992]\n",
      "training loss: 0.529830  Batch: [ 1408/ 4992]\n",
      "training loss: 0.513134  Batch: [ 1440/ 4992]\n",
      "training loss: 0.567636  Batch: [ 1472/ 4992]\n",
      "training loss: 0.543708  Batch: [ 1504/ 4992]\n",
      "training loss: 0.619497  Batch: [ 1536/ 4992]\n",
      "training loss: 0.443606  Batch: [ 1568/ 4992]\n",
      "training loss: 0.514940  Batch: [ 1600/ 4992]\n",
      "training loss: 0.414483  Batch: [ 1632/ 4992]\n",
      "training loss: 0.589159  Batch: [ 1664/ 4992]\n",
      "training loss: 0.498592  Batch: [ 1696/ 4992]\n",
      "training loss: 0.689594  Batch: [ 1728/ 4992]\n",
      "training loss: 0.580589  Batch: [ 1760/ 4992]\n",
      "training loss: 0.642080  Batch: [ 1792/ 4992]\n",
      "training loss: 0.627886  Batch: [ 1824/ 4992]\n",
      "training loss: 0.641612  Batch: [ 1856/ 4992]\n",
      "training loss: 0.633007  Batch: [ 1888/ 4992]\n",
      "training loss: 0.571457  Batch: [ 1920/ 4992]\n",
      "training loss: 0.582451  Batch: [ 1952/ 4992]\n",
      "training loss: 0.763331  Batch: [ 1984/ 4992]\n",
      "training loss: 0.697493  Batch: [ 2016/ 4992]\n",
      "training loss: 0.538249  Batch: [ 2048/ 4992]\n",
      "training loss: 0.631657  Batch: [ 2080/ 4992]\n",
      "training loss: 0.539392  Batch: [ 2112/ 4992]\n",
      "training loss: 0.484878  Batch: [ 2144/ 4992]\n",
      "training loss: 0.547038  Batch: [ 2176/ 4992]\n",
      "training loss: 0.651640  Batch: [ 2208/ 4992]\n",
      "training loss: 0.590093  Batch: [ 2240/ 4992]\n",
      "training loss: 0.598072  Batch: [ 2272/ 4992]\n",
      "training loss: 0.533070  Batch: [ 2304/ 4992]\n",
      "training loss: 0.546148  Batch: [ 2336/ 4992]\n",
      "training loss: 0.467575  Batch: [ 2368/ 4992]\n",
      "training loss: 0.685583  Batch: [ 2400/ 4992]\n",
      "training loss: 0.590430  Batch: [ 2432/ 4992]\n",
      "training loss: 0.519348  Batch: [ 2464/ 4992]\n",
      "training loss: 0.501709  Batch: [ 2496/ 4992]\n",
      "training loss: 0.608307  Batch: [ 2528/ 4992]\n",
      "training loss: 0.586457  Batch: [ 2560/ 4992]\n",
      "training loss: 0.575759  Batch: [ 2592/ 4992]\n",
      "training loss: 0.568064  Batch: [ 2624/ 4992]\n",
      "training loss: 0.540883  Batch: [ 2656/ 4992]\n",
      "training loss: 0.518700  Batch: [ 2688/ 4992]\n",
      "training loss: 0.580899  Batch: [ 2720/ 4992]\n",
      "training loss: 0.589809  Batch: [ 2752/ 4992]\n",
      "training loss: 0.588111  Batch: [ 2784/ 4992]\n",
      "training loss: 0.666475  Batch: [ 2816/ 4992]\n",
      "training loss: 0.626404  Batch: [ 2848/ 4992]\n",
      "training loss: 0.470482  Batch: [ 2880/ 4992]\n",
      "training loss: 0.599304  Batch: [ 2912/ 4992]\n",
      "training loss: 0.540591  Batch: [ 2944/ 4992]\n",
      "training loss: 0.590052  Batch: [ 2976/ 4992]\n",
      "training loss: 0.579070  Batch: [ 3008/ 4992]\n",
      "training loss: 0.630122  Batch: [ 3040/ 4992]\n",
      "training loss: 0.595907  Batch: [ 3072/ 4992]\n",
      "training loss: 0.457556  Batch: [ 3104/ 4992]\n",
      "training loss: 0.612595  Batch: [ 3136/ 4992]\n",
      "training loss: 0.488556  Batch: [ 3168/ 4992]\n",
      "training loss: 0.584409  Batch: [ 3200/ 4992]\n",
      "training loss: 0.456628  Batch: [ 3232/ 4992]\n",
      "training loss: 0.504542  Batch: [ 3264/ 4992]\n",
      "training loss: 0.442957  Batch: [ 3296/ 4992]\n",
      "training loss: 0.580554  Batch: [ 3328/ 4992]\n",
      "training loss: 0.577811  Batch: [ 3360/ 4992]\n",
      "training loss: 0.542218  Batch: [ 3392/ 4992]\n",
      "training loss: 0.432822  Batch: [ 3424/ 4992]\n",
      "training loss: 0.628117  Batch: [ 3456/ 4992]\n",
      "training loss: 0.551481  Batch: [ 3488/ 4992]\n",
      "training loss: 0.655659  Batch: [ 3520/ 4992]\n",
      "training loss: 0.576849  Batch: [ 3552/ 4992]\n",
      "training loss: 0.630886  Batch: [ 3584/ 4992]\n",
      "training loss: 0.462647  Batch: [ 3616/ 4992]\n",
      "training loss: 0.534576  Batch: [ 3648/ 4992]\n",
      "training loss: 0.520864  Batch: [ 3680/ 4992]\n",
      "training loss: 0.540662  Batch: [ 3712/ 4992]\n",
      "training loss: 0.514888  Batch: [ 3744/ 4992]\n",
      "training loss: 0.647510  Batch: [ 3776/ 4992]\n",
      "training loss: 0.662474  Batch: [ 3808/ 4992]\n",
      "training loss: 0.519686  Batch: [ 3840/ 4992]\n",
      "training loss: 0.570630  Batch: [ 3872/ 4992]\n",
      "training loss: 0.489225  Batch: [ 3904/ 4992]\n",
      "training loss: 0.611011  Batch: [ 3936/ 4992]\n",
      "training loss: 0.576711  Batch: [ 3968/ 4992]\n",
      "training loss: 0.607593  Batch: [ 4000/ 4992]\n",
      "training loss: 0.586865  Batch: [ 4032/ 4992]\n",
      "training loss: 0.472322  Batch: [ 4064/ 4992]\n",
      "training loss: 0.616977  Batch: [ 4096/ 4992]\n",
      "training loss: 0.639902  Batch: [ 4128/ 4992]\n",
      "training loss: 0.532844  Batch: [ 4160/ 4992]\n",
      "training loss: 0.609540  Batch: [ 4192/ 4992]\n",
      "training loss: 0.539777  Batch: [ 4224/ 4992]\n",
      "training loss: 0.565161  Batch: [ 4256/ 4992]\n",
      "training loss: 0.602443  Batch: [ 4288/ 4992]\n",
      "training loss: 0.527740  Batch: [ 4320/ 4992]\n",
      "training loss: 0.627936  Batch: [ 4352/ 4992]\n",
      "training loss: 0.623728  Batch: [ 4384/ 4992]\n",
      "training loss: 0.575175  Batch: [ 4416/ 4992]\n",
      "training loss: 0.460312  Batch: [ 4448/ 4992]\n",
      "training loss: 0.544976  Batch: [ 4480/ 4992]\n",
      "training loss: 0.592723  Batch: [ 4512/ 4992]\n",
      "training loss: 0.464141  Batch: [ 4544/ 4992]\n",
      "training loss: 0.569914  Batch: [ 4576/ 4992]\n",
      "training loss: 0.466255  Batch: [ 4608/ 4992]\n",
      "training loss: 0.492591  Batch: [ 4640/ 4992]\n",
      "training loss: 0.505158  Batch: [ 4672/ 4992]\n",
      "training loss: 0.589636  Batch: [ 4704/ 4992]\n",
      "training loss: 0.513274  Batch: [ 4736/ 4992]\n",
      "training loss: 0.575745  Batch: [ 4768/ 4992]\n",
      "training loss: 0.590220  Batch: [ 4800/ 4992]\n",
      "training loss: 0.378662  Batch: [ 4832/ 4992]\n",
      "training loss: 0.611033  Batch: [ 4864/ 4992]\n",
      "training loss: 0.528871  Batch: [ 4896/ 4992]\n",
      "training loss: 0.513165  Batch: [ 4928/ 4992]\n",
      "training loss: 0.587137  Batch: [ 4960/ 4992]\n",
      "training loss: 0.532285  Batch: [ 4992/ 4992]\n",
      "Epoch 6/50, Training Loss: 17.9718\n",
      "validation loss: 0.506060  Batch: [   32/ 1070]\n",
      "validation loss: 0.436299  Batch: [   64/ 1070]\n",
      "validation loss: 0.459494  Batch: [   96/ 1070]\n",
      "validation loss: 0.507658  Batch: [  128/ 1070]\n",
      "validation loss: 0.536345  Batch: [  160/ 1070]\n",
      "validation loss: 0.538696  Batch: [  192/ 1070]\n",
      "validation loss: 0.501754  Batch: [  224/ 1070]\n",
      "validation loss: 0.695113  Batch: [  256/ 1070]\n",
      "validation loss: 0.522759  Batch: [  288/ 1070]\n",
      "validation loss: 0.620386  Batch: [  320/ 1070]\n",
      "validation loss: 0.513997  Batch: [  352/ 1070]\n",
      "validation loss: 0.432618  Batch: [  384/ 1070]\n",
      "validation loss: 0.459259  Batch: [  416/ 1070]\n",
      "validation loss: 0.522934  Batch: [  448/ 1070]\n",
      "validation loss: 0.482820  Batch: [  480/ 1070]\n",
      "validation loss: 0.445053  Batch: [  512/ 1070]\n",
      "validation loss: 0.425802  Batch: [  544/ 1070]\n",
      "validation loss: 0.632975  Batch: [  576/ 1070]\n",
      "validation loss: 0.473370  Batch: [  608/ 1070]\n",
      "validation loss: 0.454159  Batch: [  640/ 1070]\n",
      "validation loss: 0.504201  Batch: [  672/ 1070]\n",
      "validation loss: 0.484990  Batch: [  704/ 1070]\n",
      "validation loss: 0.806656  Batch: [  736/ 1070]\n",
      "validation loss: 0.462378  Batch: [  768/ 1070]\n",
      "validation loss: 0.475663  Batch: [  800/ 1070]\n",
      "validation loss: 0.584744  Batch: [  832/ 1070]\n",
      "validation loss: 0.628447  Batch: [  864/ 1070]\n",
      "validation loss: 0.410632  Batch: [  896/ 1070]\n",
      "validation loss: 0.507783  Batch: [  928/ 1070]\n",
      "validation loss: 0.486333  Batch: [  960/ 1070]\n",
      "validation loss: 0.695707  Batch: [  992/ 1070]\n",
      "validation loss: 0.538646  Batch: [ 1024/ 1070]\n",
      "validation loss: 0.564724  Batch: [ 1056/ 1070]\n",
      "validation loss: 0.468800  Batch: [ 1070/ 1070]\n",
      "Epoch 6/50, Validation Loss: 0.5241, Validation Accuracy: 75.14%\n",
      "\n",
      "\n",
      "Epoch 7 \n",
      "---------------------\n",
      "training loss: 0.421187  Batch: [   32/ 4992]\n",
      "training loss: 0.491376  Batch: [   64/ 4992]\n",
      "training loss: 0.687123  Batch: [   96/ 4992]\n",
      "training loss: 0.626944  Batch: [  128/ 4992]\n",
      "training loss: 0.511362  Batch: [  160/ 4992]\n",
      "training loss: 0.491530  Batch: [  192/ 4992]\n",
      "training loss: 0.487286  Batch: [  224/ 4992]\n",
      "training loss: 0.389523  Batch: [  256/ 4992]\n",
      "training loss: 0.473552  Batch: [  288/ 4992]\n",
      "training loss: 0.403271  Batch: [  320/ 4992]\n",
      "training loss: 0.589776  Batch: [  352/ 4992]\n",
      "training loss: 0.469516  Batch: [  384/ 4992]\n",
      "training loss: 0.643904  Batch: [  416/ 4992]\n",
      "training loss: 0.463967  Batch: [  448/ 4992]\n",
      "training loss: 0.517689  Batch: [  480/ 4992]\n",
      "training loss: 0.595406  Batch: [  512/ 4992]\n",
      "training loss: 0.731446  Batch: [  544/ 4992]\n",
      "training loss: 0.618556  Batch: [  576/ 4992]\n",
      "training loss: 0.450840  Batch: [  608/ 4992]\n",
      "training loss: 0.442882  Batch: [  640/ 4992]\n",
      "training loss: 0.584143  Batch: [  672/ 4992]\n",
      "training loss: 0.625390  Batch: [  704/ 4992]\n",
      "training loss: 0.602678  Batch: [  736/ 4992]\n",
      "training loss: 0.624052  Batch: [  768/ 4992]\n",
      "training loss: 0.420503  Batch: [  800/ 4992]\n",
      "training loss: 0.419551  Batch: [  832/ 4992]\n",
      "training loss: 0.468552  Batch: [  864/ 4992]\n",
      "training loss: 0.486861  Batch: [  896/ 4992]\n",
      "training loss: 0.526206  Batch: [  928/ 4992]\n",
      "training loss: 0.597294  Batch: [  960/ 4992]\n",
      "training loss: 0.515776  Batch: [  992/ 4992]\n",
      "training loss: 0.446052  Batch: [ 1024/ 4992]\n",
      "training loss: 0.684678  Batch: [ 1056/ 4992]\n",
      "training loss: 0.509080  Batch: [ 1088/ 4992]\n",
      "training loss: 0.422494  Batch: [ 1120/ 4992]\n",
      "training loss: 0.462148  Batch: [ 1152/ 4992]\n",
      "training loss: 0.418083  Batch: [ 1184/ 4992]\n",
      "training loss: 0.439533  Batch: [ 1216/ 4992]\n",
      "training loss: 0.516294  Batch: [ 1248/ 4992]\n",
      "training loss: 0.365718  Batch: [ 1280/ 4992]\n",
      "training loss: 0.574038  Batch: [ 1312/ 4992]\n",
      "training loss: 0.552044  Batch: [ 1344/ 4992]\n",
      "training loss: 0.421230  Batch: [ 1376/ 4992]\n",
      "training loss: 0.781072  Batch: [ 1408/ 4992]\n",
      "training loss: 0.622454  Batch: [ 1440/ 4992]\n",
      "training loss: 0.638089  Batch: [ 1472/ 4992]\n",
      "training loss: 0.523103  Batch: [ 1504/ 4992]\n",
      "training loss: 0.596959  Batch: [ 1536/ 4992]\n",
      "training loss: 0.722960  Batch: [ 1568/ 4992]\n",
      "training loss: 0.586784  Batch: [ 1600/ 4992]\n",
      "training loss: 0.485626  Batch: [ 1632/ 4992]\n",
      "training loss: 0.574802  Batch: [ 1664/ 4992]\n",
      "training loss: 0.470776  Batch: [ 1696/ 4992]\n",
      "training loss: 0.541462  Batch: [ 1728/ 4992]\n",
      "training loss: 0.614871  Batch: [ 1760/ 4992]\n",
      "training loss: 0.512276  Batch: [ 1792/ 4992]\n",
      "training loss: 0.390623  Batch: [ 1824/ 4992]\n",
      "training loss: 0.585842  Batch: [ 1856/ 4992]\n",
      "training loss: 0.435130  Batch: [ 1888/ 4992]\n",
      "training loss: 0.492104  Batch: [ 1920/ 4992]\n",
      "training loss: 0.495880  Batch: [ 1952/ 4992]\n",
      "training loss: 0.459423  Batch: [ 1984/ 4992]\n",
      "training loss: 0.553801  Batch: [ 2016/ 4992]\n",
      "training loss: 0.545432  Batch: [ 2048/ 4992]\n",
      "training loss: 0.506564  Batch: [ 2080/ 4992]\n",
      "training loss: 0.555867  Batch: [ 2112/ 4992]\n",
      "training loss: 0.585341  Batch: [ 2144/ 4992]\n",
      "training loss: 0.601004  Batch: [ 2176/ 4992]\n",
      "training loss: 0.413468  Batch: [ 2208/ 4992]\n",
      "training loss: 0.566791  Batch: [ 2240/ 4992]\n",
      "training loss: 0.592292  Batch: [ 2272/ 4992]\n",
      "training loss: 0.534136  Batch: [ 2304/ 4992]\n",
      "training loss: 0.564326  Batch: [ 2336/ 4992]\n",
      "training loss: 0.417327  Batch: [ 2368/ 4992]\n",
      "training loss: 0.518342  Batch: [ 2400/ 4992]\n",
      "training loss: 0.595339  Batch: [ 2432/ 4992]\n",
      "training loss: 0.476857  Batch: [ 2464/ 4992]\n",
      "training loss: 0.639917  Batch: [ 2496/ 4992]\n",
      "training loss: 0.443484  Batch: [ 2528/ 4992]\n",
      "training loss: 0.442578  Batch: [ 2560/ 4992]\n",
      "training loss: 0.607595  Batch: [ 2592/ 4992]\n",
      "training loss: 0.789068  Batch: [ 2624/ 4992]\n",
      "training loss: 0.510778  Batch: [ 2656/ 4992]\n",
      "training loss: 0.482734  Batch: [ 2688/ 4992]\n",
      "training loss: 0.498968  Batch: [ 2720/ 4992]\n",
      "training loss: 0.718865  Batch: [ 2752/ 4992]\n",
      "training loss: 0.648733  Batch: [ 2784/ 4992]\n",
      "training loss: 0.561013  Batch: [ 2816/ 4992]\n",
      "training loss: 0.494310  Batch: [ 2848/ 4992]\n",
      "training loss: 0.534024  Batch: [ 2880/ 4992]\n",
      "training loss: 0.508800  Batch: [ 2912/ 4992]\n",
      "training loss: 0.467147  Batch: [ 2944/ 4992]\n",
      "training loss: 0.531042  Batch: [ 2976/ 4992]\n",
      "training loss: 0.503048  Batch: [ 3008/ 4992]\n",
      "training loss: 0.554344  Batch: [ 3040/ 4992]\n",
      "training loss: 0.491349  Batch: [ 3072/ 4992]\n",
      "training loss: 0.710829  Batch: [ 3104/ 4992]\n",
      "training loss: 0.472951  Batch: [ 3136/ 4992]\n",
      "training loss: 0.373782  Batch: [ 3168/ 4992]\n",
      "training loss: 0.472444  Batch: [ 3200/ 4992]\n",
      "training loss: 0.574099  Batch: [ 3232/ 4992]\n",
      "training loss: 0.615288  Batch: [ 3264/ 4992]\n",
      "training loss: 0.619278  Batch: [ 3296/ 4992]\n",
      "training loss: 0.453460  Batch: [ 3328/ 4992]\n",
      "training loss: 0.568711  Batch: [ 3360/ 4992]\n",
      "training loss: 0.549464  Batch: [ 3392/ 4992]\n",
      "training loss: 0.568652  Batch: [ 3424/ 4992]\n",
      "training loss: 0.539514  Batch: [ 3456/ 4992]\n",
      "training loss: 0.707949  Batch: [ 3488/ 4992]\n",
      "training loss: 0.363544  Batch: [ 3520/ 4992]\n",
      "training loss: 0.545381  Batch: [ 3552/ 4992]\n",
      "training loss: 0.606972  Batch: [ 3584/ 4992]\n",
      "training loss: 0.553929  Batch: [ 3616/ 4992]\n",
      "training loss: 0.594244  Batch: [ 3648/ 4992]\n",
      "training loss: 0.468345  Batch: [ 3680/ 4992]\n",
      "training loss: 0.505597  Batch: [ 3712/ 4992]\n",
      "training loss: 0.531277  Batch: [ 3744/ 4992]\n",
      "training loss: 0.460784  Batch: [ 3776/ 4992]\n",
      "training loss: 0.459758  Batch: [ 3808/ 4992]\n",
      "training loss: 0.585161  Batch: [ 3840/ 4992]\n",
      "training loss: 0.568954  Batch: [ 3872/ 4992]\n",
      "training loss: 0.446951  Batch: [ 3904/ 4992]\n",
      "training loss: 0.473451  Batch: [ 3936/ 4992]\n",
      "training loss: 0.475340  Batch: [ 3968/ 4992]\n",
      "training loss: 0.565986  Batch: [ 4000/ 4992]\n",
      "training loss: 0.571057  Batch: [ 4032/ 4992]\n",
      "training loss: 0.567878  Batch: [ 4064/ 4992]\n",
      "training loss: 0.600097  Batch: [ 4096/ 4992]\n",
      "training loss: 0.430493  Batch: [ 4128/ 4992]\n",
      "training loss: 0.540021  Batch: [ 4160/ 4992]\n",
      "training loss: 0.548139  Batch: [ 4192/ 4992]\n",
      "training loss: 0.454184  Batch: [ 4224/ 4992]\n",
      "training loss: 0.574278  Batch: [ 4256/ 4992]\n",
      "training loss: 0.500556  Batch: [ 4288/ 4992]\n",
      "training loss: 0.561930  Batch: [ 4320/ 4992]\n",
      "training loss: 0.469207  Batch: [ 4352/ 4992]\n",
      "training loss: 0.484674  Batch: [ 4384/ 4992]\n",
      "training loss: 0.498791  Batch: [ 4416/ 4992]\n",
      "training loss: 0.606435  Batch: [ 4448/ 4992]\n",
      "training loss: 0.470029  Batch: [ 4480/ 4992]\n",
      "training loss: 0.485332  Batch: [ 4512/ 4992]\n",
      "training loss: 0.474396  Batch: [ 4544/ 4992]\n",
      "training loss: 0.556361  Batch: [ 4576/ 4992]\n",
      "training loss: 0.716934  Batch: [ 4608/ 4992]\n",
      "training loss: 0.597715  Batch: [ 4640/ 4992]\n",
      "training loss: 0.730111  Batch: [ 4672/ 4992]\n",
      "training loss: 0.775610  Batch: [ 4704/ 4992]\n",
      "training loss: 0.567554  Batch: [ 4736/ 4992]\n",
      "training loss: 0.686131  Batch: [ 4768/ 4992]\n",
      "training loss: 0.465003  Batch: [ 4800/ 4992]\n",
      "training loss: 0.450863  Batch: [ 4832/ 4992]\n",
      "training loss: 0.663670  Batch: [ 4864/ 4992]\n",
      "training loss: 0.529985  Batch: [ 4896/ 4992]\n",
      "training loss: 0.585224  Batch: [ 4928/ 4992]\n",
      "training loss: 0.412181  Batch: [ 4960/ 4992]\n",
      "training loss: 0.677272  Batch: [ 4992/ 4992]\n",
      "Epoch 7/50, Training Loss: 17.2018\n",
      "validation loss: 0.440935  Batch: [   32/ 1070]\n",
      "validation loss: 0.530322  Batch: [   64/ 1070]\n",
      "validation loss: 0.690280  Batch: [   96/ 1070]\n",
      "validation loss: 0.512501  Batch: [  128/ 1070]\n",
      "validation loss: 0.485458  Batch: [  160/ 1070]\n",
      "validation loss: 0.664672  Batch: [  192/ 1070]\n",
      "validation loss: 0.349972  Batch: [  224/ 1070]\n",
      "validation loss: 0.414806  Batch: [  256/ 1070]\n",
      "validation loss: 0.638610  Batch: [  288/ 1070]\n",
      "validation loss: 0.563634  Batch: [  320/ 1070]\n",
      "validation loss: 0.628586  Batch: [  352/ 1070]\n",
      "validation loss: 0.605808  Batch: [  384/ 1070]\n",
      "validation loss: 0.539636  Batch: [  416/ 1070]\n",
      "validation loss: 0.472160  Batch: [  448/ 1070]\n",
      "validation loss: 0.606701  Batch: [  480/ 1070]\n",
      "validation loss: 0.673545  Batch: [  512/ 1070]\n",
      "validation loss: 0.439037  Batch: [  544/ 1070]\n",
      "validation loss: 0.519197  Batch: [  576/ 1070]\n",
      "validation loss: 0.559169  Batch: [  608/ 1070]\n",
      "validation loss: 0.780215  Batch: [  640/ 1070]\n",
      "validation loss: 0.457722  Batch: [  672/ 1070]\n",
      "validation loss: 0.465084  Batch: [  704/ 1070]\n",
      "validation loss: 0.516335  Batch: [  736/ 1070]\n",
      "validation loss: 0.669914  Batch: [  768/ 1070]\n",
      "validation loss: 0.593814  Batch: [  800/ 1070]\n",
      "validation loss: 0.587448  Batch: [  832/ 1070]\n",
      "validation loss: 0.588155  Batch: [  864/ 1070]\n",
      "validation loss: 0.585061  Batch: [  896/ 1070]\n",
      "validation loss: 0.553929  Batch: [  928/ 1070]\n",
      "validation loss: 0.540702  Batch: [  960/ 1070]\n",
      "validation loss: 0.479319  Batch: [  992/ 1070]\n",
      "validation loss: 0.552796  Batch: [ 1024/ 1070]\n",
      "validation loss: 0.475898  Batch: [ 1056/ 1070]\n",
      "validation loss: 0.581182  Batch: [ 1070/ 1070]\n",
      "Epoch 7/50, Validation Loss: 0.5513, Validation Accuracy: 71.68%\n",
      "\n",
      "\n",
      "Epoch 8 \n",
      "---------------------\n",
      "training loss: 0.609590  Batch: [   32/ 4992]\n",
      "training loss: 0.450456  Batch: [   64/ 4992]\n",
      "training loss: 0.568099  Batch: [   96/ 4992]\n",
      "training loss: 0.489805  Batch: [  128/ 4992]\n",
      "training loss: 0.510522  Batch: [  160/ 4992]\n",
      "training loss: 0.449419  Batch: [  192/ 4992]\n",
      "training loss: 0.581650  Batch: [  224/ 4992]\n",
      "training loss: 0.458619  Batch: [  256/ 4992]\n",
      "training loss: 0.420085  Batch: [  288/ 4992]\n",
      "training loss: 0.536052  Batch: [  320/ 4992]\n",
      "training loss: 0.394258  Batch: [  352/ 4992]\n",
      "training loss: 0.473603  Batch: [  384/ 4992]\n",
      "training loss: 0.644021  Batch: [  416/ 4992]\n",
      "training loss: 0.628794  Batch: [  448/ 4992]\n",
      "training loss: 0.561732  Batch: [  480/ 4992]\n",
      "training loss: 0.487070  Batch: [  512/ 4992]\n",
      "training loss: 0.572469  Batch: [  544/ 4992]\n",
      "training loss: 0.559380  Batch: [  576/ 4992]\n",
      "training loss: 0.598200  Batch: [  608/ 4992]\n",
      "training loss: 0.633353  Batch: [  640/ 4992]\n",
      "training loss: 0.527375  Batch: [  672/ 4992]\n",
      "training loss: 0.499435  Batch: [  704/ 4992]\n",
      "training loss: 0.555508  Batch: [  736/ 4992]\n",
      "training loss: 0.481776  Batch: [  768/ 4992]\n",
      "training loss: 0.528789  Batch: [  800/ 4992]\n",
      "training loss: 0.423860  Batch: [  832/ 4992]\n",
      "training loss: 0.478099  Batch: [  864/ 4992]\n",
      "training loss: 0.525333  Batch: [  896/ 4992]\n",
      "training loss: 0.575780  Batch: [  928/ 4992]\n",
      "training loss: 0.489635  Batch: [  960/ 4992]\n",
      "training loss: 0.558118  Batch: [  992/ 4992]\n",
      "training loss: 0.507873  Batch: [ 1024/ 4992]\n",
      "training loss: 0.560198  Batch: [ 1056/ 4992]\n",
      "training loss: 0.489362  Batch: [ 1088/ 4992]\n",
      "training loss: 0.501032  Batch: [ 1120/ 4992]\n",
      "training loss: 0.524770  Batch: [ 1152/ 4992]\n",
      "training loss: 0.564189  Batch: [ 1184/ 4992]\n",
      "training loss: 0.611103  Batch: [ 1216/ 4992]\n",
      "training loss: 0.741120  Batch: [ 1248/ 4992]\n",
      "training loss: 0.495834  Batch: [ 1280/ 4992]\n",
      "training loss: 0.582347  Batch: [ 1312/ 4992]\n",
      "training loss: 0.397087  Batch: [ 1344/ 4992]\n",
      "training loss: 0.681495  Batch: [ 1376/ 4992]\n",
      "training loss: 0.541884  Batch: [ 1408/ 4992]\n",
      "training loss: 0.608726  Batch: [ 1440/ 4992]\n",
      "training loss: 0.692103  Batch: [ 1472/ 4992]\n",
      "training loss: 0.619330  Batch: [ 1504/ 4992]\n",
      "training loss: 0.477515  Batch: [ 1536/ 4992]\n",
      "training loss: 0.462356  Batch: [ 1568/ 4992]\n",
      "training loss: 0.589724  Batch: [ 1600/ 4992]\n",
      "training loss: 0.600893  Batch: [ 1632/ 4992]\n",
      "training loss: 0.556272  Batch: [ 1664/ 4992]\n",
      "training loss: 0.439366  Batch: [ 1696/ 4992]\n",
      "training loss: 0.659015  Batch: [ 1728/ 4992]\n",
      "training loss: 0.522152  Batch: [ 1760/ 4992]\n",
      "training loss: 0.476890  Batch: [ 1792/ 4992]\n",
      "training loss: 0.379541  Batch: [ 1824/ 4992]\n",
      "training loss: 0.487778  Batch: [ 1856/ 4992]\n",
      "training loss: 0.707124  Batch: [ 1888/ 4992]\n",
      "training loss: 0.527885  Batch: [ 1920/ 4992]\n",
      "training loss: 0.508947  Batch: [ 1952/ 4992]\n",
      "training loss: 0.483677  Batch: [ 1984/ 4992]\n",
      "training loss: 0.599946  Batch: [ 2016/ 4992]\n",
      "training loss: 0.524204  Batch: [ 2048/ 4992]\n",
      "training loss: 0.629429  Batch: [ 2080/ 4992]\n",
      "training loss: 0.608080  Batch: [ 2112/ 4992]\n",
      "training loss: 0.723573  Batch: [ 2144/ 4992]\n",
      "training loss: 0.608275  Batch: [ 2176/ 4992]\n",
      "training loss: 0.486236  Batch: [ 2208/ 4992]\n",
      "training loss: 0.584955  Batch: [ 2240/ 4992]\n",
      "training loss: 0.540466  Batch: [ 2272/ 4992]\n",
      "training loss: 0.360770  Batch: [ 2304/ 4992]\n",
      "training loss: 0.451572  Batch: [ 2336/ 4992]\n",
      "training loss: 0.560756  Batch: [ 2368/ 4992]\n",
      "training loss: 0.347726  Batch: [ 2400/ 4992]\n",
      "training loss: 0.483719  Batch: [ 2432/ 4992]\n",
      "training loss: 0.414211  Batch: [ 2464/ 4992]\n",
      "training loss: 0.643388  Batch: [ 2496/ 4992]\n",
      "training loss: 0.442661  Batch: [ 2528/ 4992]\n",
      "training loss: 0.609610  Batch: [ 2560/ 4992]\n",
      "training loss: 0.438030  Batch: [ 2592/ 4992]\n",
      "training loss: 0.424231  Batch: [ 2624/ 4992]\n",
      "training loss: 0.469476  Batch: [ 2656/ 4992]\n",
      "training loss: 0.426397  Batch: [ 2688/ 4992]\n",
      "training loss: 0.604926  Batch: [ 2720/ 4992]\n",
      "training loss: 0.516032  Batch: [ 2752/ 4992]\n",
      "training loss: 0.483940  Batch: [ 2784/ 4992]\n",
      "training loss: 0.548458  Batch: [ 2816/ 4992]\n",
      "training loss: 0.463398  Batch: [ 2848/ 4992]\n",
      "training loss: 0.587581  Batch: [ 2880/ 4992]\n",
      "training loss: 0.668687  Batch: [ 2912/ 4992]\n",
      "training loss: 0.672304  Batch: [ 2944/ 4992]\n",
      "training loss: 0.547565  Batch: [ 2976/ 4992]\n",
      "training loss: 0.625727  Batch: [ 3008/ 4992]\n",
      "training loss: 0.544201  Batch: [ 3040/ 4992]\n",
      "training loss: 0.585467  Batch: [ 3072/ 4992]\n",
      "training loss: 0.636611  Batch: [ 3104/ 4992]\n",
      "training loss: 0.745919  Batch: [ 3136/ 4992]\n",
      "training loss: 0.532813  Batch: [ 3168/ 4992]\n",
      "training loss: 0.538614  Batch: [ 3200/ 4992]\n",
      "training loss: 0.542496  Batch: [ 3232/ 4992]\n",
      "training loss: 0.476991  Batch: [ 3264/ 4992]\n",
      "training loss: 0.532427  Batch: [ 3296/ 4992]\n",
      "training loss: 0.577194  Batch: [ 3328/ 4992]\n",
      "training loss: 0.499666  Batch: [ 3360/ 4992]\n",
      "training loss: 0.652258  Batch: [ 3392/ 4992]\n",
      "training loss: 0.541593  Batch: [ 3424/ 4992]\n",
      "training loss: 0.571939  Batch: [ 3456/ 4992]\n",
      "training loss: 0.590986  Batch: [ 3488/ 4992]\n",
      "training loss: 0.628896  Batch: [ 3520/ 4992]\n",
      "training loss: 0.468244  Batch: [ 3552/ 4992]\n",
      "training loss: 0.549044  Batch: [ 3584/ 4992]\n",
      "training loss: 0.635548  Batch: [ 3616/ 4992]\n",
      "training loss: 0.554574  Batch: [ 3648/ 4992]\n",
      "training loss: 0.614621  Batch: [ 3680/ 4992]\n",
      "training loss: 0.546334  Batch: [ 3712/ 4992]\n",
      "training loss: 0.567155  Batch: [ 3744/ 4992]\n",
      "training loss: 0.543972  Batch: [ 3776/ 4992]\n",
      "training loss: 0.631347  Batch: [ 3808/ 4992]\n",
      "training loss: 0.583539  Batch: [ 3840/ 4992]\n",
      "training loss: 0.463213  Batch: [ 3872/ 4992]\n",
      "training loss: 0.537745  Batch: [ 3904/ 4992]\n",
      "training loss: 0.478918  Batch: [ 3936/ 4992]\n",
      "training loss: 0.580730  Batch: [ 3968/ 4992]\n",
      "training loss: 0.459881  Batch: [ 4000/ 4992]\n",
      "training loss: 0.484448  Batch: [ 4032/ 4992]\n",
      "training loss: 0.527346  Batch: [ 4064/ 4992]\n",
      "training loss: 0.580607  Batch: [ 4096/ 4992]\n",
      "training loss: 0.489419  Batch: [ 4128/ 4992]\n",
      "training loss: 0.527840  Batch: [ 4160/ 4992]\n",
      "training loss: 0.481394  Batch: [ 4192/ 4992]\n",
      "training loss: 0.510545  Batch: [ 4224/ 4992]\n",
      "training loss: 0.529642  Batch: [ 4256/ 4992]\n",
      "training loss: 0.555174  Batch: [ 4288/ 4992]\n",
      "training loss: 0.436069  Batch: [ 4320/ 4992]\n",
      "training loss: 0.483435  Batch: [ 4352/ 4992]\n",
      "training loss: 0.576731  Batch: [ 4384/ 4992]\n",
      "training loss: 0.504924  Batch: [ 4416/ 4992]\n",
      "training loss: 0.621081  Batch: [ 4448/ 4992]\n",
      "training loss: 0.682528  Batch: [ 4480/ 4992]\n",
      "training loss: 0.489796  Batch: [ 4512/ 4992]\n",
      "training loss: 0.556435  Batch: [ 4544/ 4992]\n",
      "training loss: 0.482594  Batch: [ 4576/ 4992]\n",
      "training loss: 0.626811  Batch: [ 4608/ 4992]\n",
      "training loss: 0.466129  Batch: [ 4640/ 4992]\n",
      "training loss: 0.489456  Batch: [ 4672/ 4992]\n",
      "training loss: 0.555932  Batch: [ 4704/ 4992]\n",
      "training loss: 0.431907  Batch: [ 4736/ 4992]\n",
      "training loss: 0.590250  Batch: [ 4768/ 4992]\n",
      "training loss: 0.528534  Batch: [ 4800/ 4992]\n",
      "training loss: 0.543396  Batch: [ 4832/ 4992]\n",
      "training loss: 0.484794  Batch: [ 4864/ 4992]\n",
      "training loss: 0.457643  Batch: [ 4896/ 4992]\n",
      "training loss: 0.452319  Batch: [ 4928/ 4992]\n",
      "training loss: 0.472925  Batch: [ 4960/ 4992]\n",
      "training loss: 0.761465  Batch: [ 4992/ 4992]\n",
      "Epoch 8/50, Training Loss: 17.2634\n",
      "validation loss: 0.531825  Batch: [   32/ 1070]\n",
      "validation loss: 0.584529  Batch: [   64/ 1070]\n",
      "validation loss: 0.530525  Batch: [   96/ 1070]\n",
      "validation loss: 0.439194  Batch: [  128/ 1070]\n",
      "validation loss: 0.530070  Batch: [  160/ 1070]\n",
      "validation loss: 0.502530  Batch: [  192/ 1070]\n",
      "validation loss: 0.487450  Batch: [  224/ 1070]\n",
      "validation loss: 0.587562  Batch: [  256/ 1070]\n",
      "validation loss: 0.574974  Batch: [  288/ 1070]\n",
      "validation loss: 0.493254  Batch: [  320/ 1070]\n",
      "validation loss: 0.671158  Batch: [  352/ 1070]\n",
      "validation loss: 0.297882  Batch: [  384/ 1070]\n",
      "validation loss: 0.581199  Batch: [  416/ 1070]\n",
      "validation loss: 0.510648  Batch: [  448/ 1070]\n",
      "validation loss: 0.456376  Batch: [  480/ 1070]\n",
      "validation loss: 0.565086  Batch: [  512/ 1070]\n",
      "validation loss: 0.608098  Batch: [  544/ 1070]\n",
      "validation loss: 0.534814  Batch: [  576/ 1070]\n",
      "validation loss: 0.514328  Batch: [  608/ 1070]\n",
      "validation loss: 0.532165  Batch: [  640/ 1070]\n",
      "validation loss: 0.537120  Batch: [  672/ 1070]\n",
      "validation loss: 0.555909  Batch: [  704/ 1070]\n",
      "validation loss: 0.384278  Batch: [  736/ 1070]\n",
      "validation loss: 0.500910  Batch: [  768/ 1070]\n",
      "validation loss: 0.471987  Batch: [  800/ 1070]\n",
      "validation loss: 0.635399  Batch: [  832/ 1070]\n",
      "validation loss: 0.455267  Batch: [  864/ 1070]\n",
      "validation loss: 0.503300  Batch: [  896/ 1070]\n",
      "validation loss: 0.465660  Batch: [  928/ 1070]\n",
      "validation loss: 0.449238  Batch: [  960/ 1070]\n",
      "validation loss: 0.630066  Batch: [  992/ 1070]\n",
      "validation loss: 0.547575  Batch: [ 1024/ 1070]\n",
      "validation loss: 0.544407  Batch: [ 1056/ 1070]\n",
      "validation loss: 0.451509  Batch: [ 1070/ 1070]\n",
      "Epoch 8/50, Validation Loss: 0.5207, Validation Accuracy: 72.80%\n",
      "\n",
      "\n",
      "Epoch 9 \n",
      "---------------------\n",
      "training loss: 0.669703  Batch: [   32/ 4992]\n",
      "training loss: 0.475032  Batch: [   64/ 4992]\n",
      "training loss: 0.545201  Batch: [   96/ 4992]\n",
      "training loss: 0.409005  Batch: [  128/ 4992]\n",
      "training loss: 0.472799  Batch: [  160/ 4992]\n",
      "training loss: 0.554990  Batch: [  192/ 4992]\n",
      "training loss: 0.530255  Batch: [  224/ 4992]\n",
      "training loss: 0.608537  Batch: [  256/ 4992]\n",
      "training loss: 0.524817  Batch: [  288/ 4992]\n",
      "training loss: 0.545116  Batch: [  320/ 4992]\n",
      "training loss: 0.464031  Batch: [  352/ 4992]\n",
      "training loss: 0.479988  Batch: [  384/ 4992]\n",
      "training loss: 0.616176  Batch: [  416/ 4992]\n",
      "training loss: 0.679423  Batch: [  448/ 4992]\n",
      "training loss: 0.510900  Batch: [  480/ 4992]\n",
      "training loss: 0.402038  Batch: [  512/ 4992]\n",
      "training loss: 0.489417  Batch: [  544/ 4992]\n",
      "training loss: 0.642546  Batch: [  576/ 4992]\n",
      "training loss: 0.521721  Batch: [  608/ 4992]\n",
      "training loss: 0.590457  Batch: [  640/ 4992]\n",
      "training loss: 0.582878  Batch: [  672/ 4992]\n",
      "training loss: 0.441985  Batch: [  704/ 4992]\n",
      "training loss: 0.634541  Batch: [  736/ 4992]\n",
      "training loss: 0.549995  Batch: [  768/ 4992]\n",
      "training loss: 0.558673  Batch: [  800/ 4992]\n",
      "training loss: 0.401225  Batch: [  832/ 4992]\n",
      "training loss: 0.481646  Batch: [  864/ 4992]\n",
      "training loss: 0.670724  Batch: [  896/ 4992]\n",
      "training loss: 0.516385  Batch: [  928/ 4992]\n",
      "training loss: 0.527402  Batch: [  960/ 4992]\n",
      "training loss: 0.585128  Batch: [  992/ 4992]\n",
      "training loss: 0.548408  Batch: [ 1024/ 4992]\n",
      "training loss: 0.543555  Batch: [ 1056/ 4992]\n",
      "training loss: 0.570625  Batch: [ 1088/ 4992]\n",
      "training loss: 0.480880  Batch: [ 1120/ 4992]\n",
      "training loss: 0.518698  Batch: [ 1152/ 4992]\n",
      "training loss: 0.525112  Batch: [ 1184/ 4992]\n",
      "training loss: 0.552926  Batch: [ 1216/ 4992]\n",
      "training loss: 0.485782  Batch: [ 1248/ 4992]\n",
      "training loss: 0.435998  Batch: [ 1280/ 4992]\n",
      "training loss: 0.592177  Batch: [ 1312/ 4992]\n",
      "training loss: 0.422150  Batch: [ 1344/ 4992]\n",
      "training loss: 0.388283  Batch: [ 1376/ 4992]\n",
      "training loss: 0.591850  Batch: [ 1408/ 4992]\n",
      "training loss: 0.344670  Batch: [ 1440/ 4992]\n",
      "training loss: 0.340627  Batch: [ 1472/ 4992]\n",
      "training loss: 0.534899  Batch: [ 1504/ 4992]\n",
      "training loss: 0.525018  Batch: [ 1536/ 4992]\n",
      "training loss: 0.371733  Batch: [ 1568/ 4992]\n",
      "training loss: 0.497577  Batch: [ 1600/ 4992]\n",
      "training loss: 0.621355  Batch: [ 1632/ 4992]\n",
      "training loss: 0.633436  Batch: [ 1664/ 4992]\n",
      "training loss: 0.568379  Batch: [ 1696/ 4992]\n",
      "training loss: 0.376594  Batch: [ 1728/ 4992]\n",
      "training loss: 0.530134  Batch: [ 1760/ 4992]\n",
      "training loss: 0.517875  Batch: [ 1792/ 4992]\n",
      "training loss: 0.416231  Batch: [ 1824/ 4992]\n",
      "training loss: 0.547527  Batch: [ 1856/ 4992]\n",
      "training loss: 0.485316  Batch: [ 1888/ 4992]\n",
      "training loss: 0.403868  Batch: [ 1920/ 4992]\n",
      "training loss: 0.467167  Batch: [ 1952/ 4992]\n",
      "training loss: 0.457179  Batch: [ 1984/ 4992]\n",
      "training loss: 0.411476  Batch: [ 2016/ 4992]\n",
      "training loss: 0.496360  Batch: [ 2048/ 4992]\n",
      "training loss: 0.527519  Batch: [ 2080/ 4992]\n",
      "training loss: 0.573804  Batch: [ 2112/ 4992]\n",
      "training loss: 0.599287  Batch: [ 2144/ 4992]\n",
      "training loss: 0.410628  Batch: [ 2176/ 4992]\n",
      "training loss: 0.505233  Batch: [ 2208/ 4992]\n",
      "training loss: 0.536395  Batch: [ 2240/ 4992]\n",
      "training loss: 0.440801  Batch: [ 2272/ 4992]\n",
      "training loss: 0.398102  Batch: [ 2304/ 4992]\n",
      "training loss: 0.620284  Batch: [ 2336/ 4992]\n",
      "training loss: 0.483536  Batch: [ 2368/ 4992]\n",
      "training loss: 0.621755  Batch: [ 2400/ 4992]\n",
      "training loss: 0.556039  Batch: [ 2432/ 4992]\n",
      "training loss: 0.532619  Batch: [ 2464/ 4992]\n",
      "training loss: 0.441179  Batch: [ 2496/ 4992]\n",
      "training loss: 0.613660  Batch: [ 2528/ 4992]\n",
      "training loss: 0.483541  Batch: [ 2560/ 4992]\n",
      "training loss: 0.582305  Batch: [ 2592/ 4992]\n",
      "training loss: 0.526446  Batch: [ 2624/ 4992]\n",
      "training loss: 0.435273  Batch: [ 2656/ 4992]\n",
      "training loss: 0.711044  Batch: [ 2688/ 4992]\n",
      "training loss: 0.554986  Batch: [ 2720/ 4992]\n",
      "training loss: 0.522508  Batch: [ 2752/ 4992]\n",
      "training loss: 0.407961  Batch: [ 2784/ 4992]\n",
      "training loss: 0.503743  Batch: [ 2816/ 4992]\n",
      "training loss: 0.603075  Batch: [ 2848/ 4992]\n",
      "training loss: 0.534465  Batch: [ 2880/ 4992]\n",
      "training loss: 0.666686  Batch: [ 2912/ 4992]\n",
      "training loss: 0.459492  Batch: [ 2944/ 4992]\n",
      "training loss: 0.450452  Batch: [ 2976/ 4992]\n",
      "training loss: 0.510094  Batch: [ 3008/ 4992]\n",
      "training loss: 0.597092  Batch: [ 3040/ 4992]\n",
      "training loss: 0.571541  Batch: [ 3072/ 4992]\n",
      "training loss: 0.522193  Batch: [ 3104/ 4992]\n",
      "training loss: 0.582711  Batch: [ 3136/ 4992]\n",
      "training loss: 0.540442  Batch: [ 3168/ 4992]\n",
      "training loss: 0.560273  Batch: [ 3200/ 4992]\n",
      "training loss: 0.573455  Batch: [ 3232/ 4992]\n",
      "training loss: 0.512494  Batch: [ 3264/ 4992]\n",
      "training loss: 0.389256  Batch: [ 3296/ 4992]\n",
      "training loss: 0.613887  Batch: [ 3328/ 4992]\n",
      "training loss: 0.506163  Batch: [ 3360/ 4992]\n",
      "training loss: 0.557283  Batch: [ 3392/ 4992]\n",
      "training loss: 0.529179  Batch: [ 3424/ 4992]\n",
      "training loss: 0.565000  Batch: [ 3456/ 4992]\n",
      "training loss: 0.529935  Batch: [ 3488/ 4992]\n",
      "training loss: 0.436605  Batch: [ 3520/ 4992]\n",
      "training loss: 0.540529  Batch: [ 3552/ 4992]\n",
      "training loss: 0.576114  Batch: [ 3584/ 4992]\n",
      "training loss: 0.476216  Batch: [ 3616/ 4992]\n",
      "training loss: 0.664378  Batch: [ 3648/ 4992]\n",
      "training loss: 0.503184  Batch: [ 3680/ 4992]\n",
      "training loss: 0.455713  Batch: [ 3712/ 4992]\n",
      "training loss: 0.469696  Batch: [ 3744/ 4992]\n",
      "training loss: 0.557911  Batch: [ 3776/ 4992]\n",
      "training loss: 0.535026  Batch: [ 3808/ 4992]\n",
      "training loss: 0.529492  Batch: [ 3840/ 4992]\n",
      "training loss: 0.571097  Batch: [ 3872/ 4992]\n",
      "training loss: 0.518417  Batch: [ 3904/ 4992]\n",
      "training loss: 0.507064  Batch: [ 3936/ 4992]\n",
      "training loss: 0.459439  Batch: [ 3968/ 4992]\n",
      "training loss: 0.430611  Batch: [ 4000/ 4992]\n",
      "training loss: 0.552562  Batch: [ 4032/ 4992]\n",
      "training loss: 0.360707  Batch: [ 4064/ 4992]\n",
      "training loss: 0.597598  Batch: [ 4096/ 4992]\n",
      "training loss: 0.428122  Batch: [ 4128/ 4992]\n",
      "training loss: 0.433914  Batch: [ 4160/ 4992]\n",
      "training loss: 0.491132  Batch: [ 4192/ 4992]\n",
      "training loss: 0.779144  Batch: [ 4224/ 4992]\n",
      "training loss: 0.433190  Batch: [ 4256/ 4992]\n",
      "training loss: 0.469845  Batch: [ 4288/ 4992]\n",
      "training loss: 0.436284  Batch: [ 4320/ 4992]\n",
      "training loss: 0.552373  Batch: [ 4352/ 4992]\n",
      "training loss: 0.595548  Batch: [ 4384/ 4992]\n",
      "training loss: 0.609950  Batch: [ 4416/ 4992]\n",
      "training loss: 0.482676  Batch: [ 4448/ 4992]\n",
      "training loss: 0.529092  Batch: [ 4480/ 4992]\n",
      "training loss: 0.445887  Batch: [ 4512/ 4992]\n",
      "training loss: 0.403455  Batch: [ 4544/ 4992]\n",
      "training loss: 0.469621  Batch: [ 4576/ 4992]\n",
      "training loss: 0.409573  Batch: [ 4608/ 4992]\n",
      "training loss: 0.808947  Batch: [ 4640/ 4992]\n",
      "training loss: 0.514693  Batch: [ 4672/ 4992]\n",
      "training loss: 0.472909  Batch: [ 4704/ 4992]\n",
      "training loss: 0.567116  Batch: [ 4736/ 4992]\n",
      "training loss: 0.495639  Batch: [ 4768/ 4992]\n",
      "training loss: 0.564937  Batch: [ 4800/ 4992]\n",
      "training loss: 0.428191  Batch: [ 4832/ 4992]\n",
      "training loss: 0.641159  Batch: [ 4864/ 4992]\n",
      "training loss: 0.487728  Batch: [ 4896/ 4992]\n",
      "training loss: 0.429327  Batch: [ 4928/ 4992]\n",
      "training loss: 0.411336  Batch: [ 4960/ 4992]\n",
      "training loss: 0.679975  Batch: [ 4992/ 4992]\n",
      "Epoch 9/50, Training Loss: 16.6286\n",
      "validation loss: 0.462941  Batch: [   32/ 1070]\n",
      "validation loss: 0.396049  Batch: [   64/ 1070]\n",
      "validation loss: 0.503708  Batch: [   96/ 1070]\n",
      "validation loss: 0.567673  Batch: [  128/ 1070]\n",
      "validation loss: 0.473025  Batch: [  160/ 1070]\n",
      "validation loss: 0.499341  Batch: [  192/ 1070]\n",
      "validation loss: 0.485152  Batch: [  224/ 1070]\n",
      "validation loss: 0.586803  Batch: [  256/ 1070]\n",
      "validation loss: 0.732625  Batch: [  288/ 1070]\n",
      "validation loss: 0.498699  Batch: [  320/ 1070]\n",
      "validation loss: 0.424090  Batch: [  352/ 1070]\n",
      "validation loss: 0.526935  Batch: [  384/ 1070]\n",
      "validation loss: 0.533700  Batch: [  416/ 1070]\n",
      "validation loss: 0.418357  Batch: [  448/ 1070]\n",
      "validation loss: 0.385668  Batch: [  480/ 1070]\n",
      "validation loss: 0.768064  Batch: [  512/ 1070]\n",
      "validation loss: 0.502721  Batch: [  544/ 1070]\n",
      "validation loss: 0.504429  Batch: [  576/ 1070]\n",
      "validation loss: 0.411996  Batch: [  608/ 1070]\n",
      "validation loss: 0.495246  Batch: [  640/ 1070]\n",
      "validation loss: 0.382749  Batch: [  672/ 1070]\n",
      "validation loss: 0.434132  Batch: [  704/ 1070]\n",
      "validation loss: 0.566964  Batch: [  736/ 1070]\n",
      "validation loss: 0.661211  Batch: [  768/ 1070]\n",
      "validation loss: 0.516884  Batch: [  800/ 1070]\n",
      "validation loss: 0.438829  Batch: [  832/ 1070]\n",
      "validation loss: 0.542398  Batch: [  864/ 1070]\n",
      "validation loss: 0.335263  Batch: [  896/ 1070]\n",
      "validation loss: 0.437900  Batch: [  928/ 1070]\n",
      "validation loss: 0.420517  Batch: [  960/ 1070]\n",
      "validation loss: 0.536865  Batch: [  992/ 1070]\n",
      "validation loss: 0.640255  Batch: [ 1024/ 1070]\n",
      "validation loss: 0.605613  Batch: [ 1056/ 1070]\n",
      "validation loss: 0.439311  Batch: [ 1070/ 1070]\n",
      "Epoch 9/50, Validation Loss: 0.5051, Validation Accuracy: 73.27%\n",
      "\n",
      "\n",
      "Epoch 10 \n",
      "---------------------\n",
      "training loss: 0.553063  Batch: [   32/ 4992]\n",
      "training loss: 0.431024  Batch: [   64/ 4992]\n",
      "training loss: 0.479817  Batch: [   96/ 4992]\n",
      "training loss: 0.526932  Batch: [  128/ 4992]\n",
      "training loss: 0.511566  Batch: [  160/ 4992]\n",
      "training loss: 0.496604  Batch: [  192/ 4992]\n",
      "training loss: 0.546113  Batch: [  224/ 4992]\n",
      "training loss: 0.532973  Batch: [  256/ 4992]\n",
      "training loss: 0.579287  Batch: [  288/ 4992]\n",
      "training loss: 0.576827  Batch: [  320/ 4992]\n",
      "training loss: 0.479648  Batch: [  352/ 4992]\n",
      "training loss: 0.459616  Batch: [  384/ 4992]\n",
      "training loss: 0.519555  Batch: [  416/ 4992]\n",
      "training loss: 0.581712  Batch: [  448/ 4992]\n",
      "training loss: 0.552351  Batch: [  480/ 4992]\n",
      "training loss: 0.483953  Batch: [  512/ 4992]\n",
      "training loss: 0.442151  Batch: [  544/ 4992]\n",
      "training loss: 0.507095  Batch: [  576/ 4992]\n",
      "training loss: 0.371893  Batch: [  608/ 4992]\n",
      "training loss: 0.562178  Batch: [  640/ 4992]\n",
      "training loss: 0.482399  Batch: [  672/ 4992]\n",
      "training loss: 0.448178  Batch: [  704/ 4992]\n",
      "training loss: 0.561898  Batch: [  736/ 4992]\n",
      "training loss: 0.380063  Batch: [  768/ 4992]\n",
      "training loss: 0.496197  Batch: [  800/ 4992]\n",
      "training loss: 0.529044  Batch: [  832/ 4992]\n",
      "training loss: 0.458596  Batch: [  864/ 4992]\n",
      "training loss: 0.488323  Batch: [  896/ 4992]\n",
      "training loss: 0.513459  Batch: [  928/ 4992]\n",
      "training loss: 0.455217  Batch: [  960/ 4992]\n",
      "training loss: 0.435498  Batch: [  992/ 4992]\n",
      "training loss: 0.523682  Batch: [ 1024/ 4992]\n",
      "training loss: 0.451143  Batch: [ 1056/ 4992]\n",
      "training loss: 0.469591  Batch: [ 1088/ 4992]\n",
      "training loss: 0.506920  Batch: [ 1120/ 4992]\n",
      "training loss: 0.490928  Batch: [ 1152/ 4992]\n",
      "training loss: 0.482401  Batch: [ 1184/ 4992]\n",
      "training loss: 0.452315  Batch: [ 1216/ 4992]\n",
      "training loss: 0.482356  Batch: [ 1248/ 4992]\n",
      "training loss: 0.630411  Batch: [ 1280/ 4992]\n",
      "training loss: 0.523891  Batch: [ 1312/ 4992]\n",
      "training loss: 0.405785  Batch: [ 1344/ 4992]\n",
      "training loss: 0.587760  Batch: [ 1376/ 4992]\n",
      "training loss: 0.646772  Batch: [ 1408/ 4992]\n",
      "training loss: 0.583610  Batch: [ 1440/ 4992]\n",
      "training loss: 0.565365  Batch: [ 1472/ 4992]\n",
      "training loss: 0.511562  Batch: [ 1504/ 4992]\n",
      "training loss: 0.590441  Batch: [ 1536/ 4992]\n",
      "training loss: 0.507756  Batch: [ 1568/ 4992]\n",
      "training loss: 0.432678  Batch: [ 1600/ 4992]\n",
      "training loss: 0.533433  Batch: [ 1632/ 4992]\n",
      "training loss: 0.453136  Batch: [ 1664/ 4992]\n",
      "training loss: 0.448544  Batch: [ 1696/ 4992]\n",
      "training loss: 0.451293  Batch: [ 1728/ 4992]\n",
      "training loss: 0.432852  Batch: [ 1760/ 4992]\n",
      "training loss: 0.389444  Batch: [ 1792/ 4992]\n",
      "training loss: 0.491822  Batch: [ 1824/ 4992]\n",
      "training loss: 0.552409  Batch: [ 1856/ 4992]\n",
      "training loss: 0.437075  Batch: [ 1888/ 4992]\n",
      "training loss: 0.566046  Batch: [ 1920/ 4992]\n",
      "training loss: 0.560760  Batch: [ 1952/ 4992]\n",
      "training loss: 0.461058  Batch: [ 1984/ 4992]\n",
      "training loss: 0.450469  Batch: [ 2016/ 4992]\n",
      "training loss: 0.556607  Batch: [ 2048/ 4992]\n",
      "training loss: 0.474531  Batch: [ 2080/ 4992]\n",
      "training loss: 0.416051  Batch: [ 2112/ 4992]\n",
      "training loss: 0.461727  Batch: [ 2144/ 4992]\n",
      "training loss: 0.402138  Batch: [ 2176/ 4992]\n",
      "training loss: 0.433438  Batch: [ 2208/ 4992]\n",
      "training loss: 0.563156  Batch: [ 2240/ 4992]\n",
      "training loss: 0.415183  Batch: [ 2272/ 4992]\n",
      "training loss: 0.478414  Batch: [ 2304/ 4992]\n",
      "training loss: 0.421167  Batch: [ 2336/ 4992]\n",
      "training loss: 0.419311  Batch: [ 2368/ 4992]\n",
      "training loss: 0.512037  Batch: [ 2400/ 4992]\n",
      "training loss: 0.467334  Batch: [ 2432/ 4992]\n",
      "training loss: 0.402562  Batch: [ 2464/ 4992]\n",
      "training loss: 0.574547  Batch: [ 2496/ 4992]\n",
      "training loss: 0.629476  Batch: [ 2528/ 4992]\n",
      "training loss: 0.406038  Batch: [ 2560/ 4992]\n",
      "training loss: 0.567214  Batch: [ 2592/ 4992]\n",
      "training loss: 0.457146  Batch: [ 2624/ 4992]\n",
      "training loss: 0.607713  Batch: [ 2656/ 4992]\n",
      "training loss: 0.516609  Batch: [ 2688/ 4992]\n",
      "training loss: 0.644072  Batch: [ 2720/ 4992]\n",
      "training loss: 0.636177  Batch: [ 2752/ 4992]\n",
      "training loss: 0.536385  Batch: [ 2784/ 4992]\n",
      "training loss: 0.444273  Batch: [ 2816/ 4992]\n",
      "training loss: 0.457510  Batch: [ 2848/ 4992]\n",
      "training loss: 0.428615  Batch: [ 2880/ 4992]\n",
      "training loss: 0.407924  Batch: [ 2912/ 4992]\n",
      "training loss: 0.454459  Batch: [ 2944/ 4992]\n",
      "training loss: 0.295062  Batch: [ 2976/ 4992]\n",
      "training loss: 0.407422  Batch: [ 3008/ 4992]\n",
      "training loss: 0.422255  Batch: [ 3040/ 4992]\n",
      "training loss: 0.589677  Batch: [ 3072/ 4992]\n",
      "training loss: 0.734139  Batch: [ 3104/ 4992]\n",
      "training loss: 0.597917  Batch: [ 3136/ 4992]\n",
      "training loss: 0.651870  Batch: [ 3168/ 4992]\n",
      "training loss: 0.553019  Batch: [ 3200/ 4992]\n",
      "training loss: 0.479069  Batch: [ 3232/ 4992]\n",
      "training loss: 0.507876  Batch: [ 3264/ 4992]\n",
      "training loss: 0.548698  Batch: [ 3296/ 4992]\n",
      "training loss: 0.428479  Batch: [ 3328/ 4992]\n",
      "training loss: 0.360686  Batch: [ 3360/ 4992]\n",
      "training loss: 0.534672  Batch: [ 3392/ 4992]\n",
      "training loss: 0.595063  Batch: [ 3424/ 4992]\n",
      "training loss: 0.596590  Batch: [ 3456/ 4992]\n",
      "training loss: 0.484606  Batch: [ 3488/ 4992]\n",
      "training loss: 0.795617  Batch: [ 3520/ 4992]\n",
      "training loss: 0.399079  Batch: [ 3552/ 4992]\n",
      "training loss: 0.507120  Batch: [ 3584/ 4992]\n",
      "training loss: 0.351980  Batch: [ 3616/ 4992]\n",
      "training loss: 0.778393  Batch: [ 3648/ 4992]\n",
      "training loss: 0.581075  Batch: [ 3680/ 4992]\n",
      "training loss: 0.614904  Batch: [ 3712/ 4992]\n",
      "training loss: 0.579367  Batch: [ 3744/ 4992]\n",
      "training loss: 0.497032  Batch: [ 3776/ 4992]\n",
      "training loss: 0.489981  Batch: [ 3808/ 4992]\n",
      "training loss: 0.530714  Batch: [ 3840/ 4992]\n",
      "training loss: 0.510950  Batch: [ 3872/ 4992]\n",
      "training loss: 0.583764  Batch: [ 3904/ 4992]\n",
      "training loss: 0.544136  Batch: [ 3936/ 4992]\n",
      "training loss: 0.708660  Batch: [ 3968/ 4992]\n",
      "training loss: 0.727858  Batch: [ 4000/ 4992]\n",
      "training loss: 0.429557  Batch: [ 4032/ 4992]\n",
      "training loss: 0.666601  Batch: [ 4064/ 4992]\n",
      "training loss: 0.644112  Batch: [ 4096/ 4992]\n",
      "training loss: 0.530772  Batch: [ 4128/ 4992]\n",
      "training loss: 0.522264  Batch: [ 4160/ 4992]\n",
      "training loss: 0.632238  Batch: [ 4192/ 4992]\n",
      "training loss: 0.480899  Batch: [ 4224/ 4992]\n",
      "training loss: 0.472038  Batch: [ 4256/ 4992]\n",
      "training loss: 0.493067  Batch: [ 4288/ 4992]\n",
      "training loss: 0.454799  Batch: [ 4320/ 4992]\n",
      "training loss: 0.411421  Batch: [ 4352/ 4992]\n",
      "training loss: 0.537085  Batch: [ 4384/ 4992]\n",
      "training loss: 0.606530  Batch: [ 4416/ 4992]\n",
      "training loss: 0.594687  Batch: [ 4448/ 4992]\n",
      "training loss: 0.477065  Batch: [ 4480/ 4992]\n",
      "training loss: 0.545058  Batch: [ 4512/ 4992]\n",
      "training loss: 0.584601  Batch: [ 4544/ 4992]\n",
      "training loss: 0.555938  Batch: [ 4576/ 4992]\n",
      "training loss: 0.558360  Batch: [ 4608/ 4992]\n",
      "training loss: 0.514358  Batch: [ 4640/ 4992]\n",
      "training loss: 0.440923  Batch: [ 4672/ 4992]\n",
      "training loss: 0.532102  Batch: [ 4704/ 4992]\n",
      "training loss: 0.565794  Batch: [ 4736/ 4992]\n",
      "training loss: 0.390718  Batch: [ 4768/ 4992]\n",
      "training loss: 0.516257  Batch: [ 4800/ 4992]\n",
      "training loss: 0.582105  Batch: [ 4832/ 4992]\n",
      "training loss: 0.551543  Batch: [ 4864/ 4992]\n",
      "training loss: 0.522308  Batch: [ 4896/ 4992]\n",
      "training loss: 0.473022  Batch: [ 4928/ 4992]\n",
      "training loss: 0.536111  Batch: [ 4960/ 4992]\n",
      "training loss: 0.537673  Batch: [ 4992/ 4992]\n",
      "Epoch 10/50, Training Loss: 16.4218\n",
      "validation loss: 0.576475  Batch: [   32/ 1070]\n",
      "validation loss: 0.470524  Batch: [   64/ 1070]\n",
      "validation loss: 0.590844  Batch: [   96/ 1070]\n",
      "validation loss: 0.368239  Batch: [  128/ 1070]\n",
      "validation loss: 0.459140  Batch: [  160/ 1070]\n",
      "validation loss: 0.560899  Batch: [  192/ 1070]\n",
      "validation loss: 0.480799  Batch: [  224/ 1070]\n",
      "validation loss: 0.444594  Batch: [  256/ 1070]\n",
      "validation loss: 0.390384  Batch: [  288/ 1070]\n",
      "validation loss: 0.499409  Batch: [  320/ 1070]\n",
      "validation loss: 0.404741  Batch: [  352/ 1070]\n",
      "validation loss: 0.361487  Batch: [  384/ 1070]\n",
      "validation loss: 0.525522  Batch: [  416/ 1070]\n",
      "validation loss: 0.408046  Batch: [  448/ 1070]\n",
      "validation loss: 0.530825  Batch: [  480/ 1070]\n",
      "validation loss: 0.418548  Batch: [  512/ 1070]\n",
      "validation loss: 0.699310  Batch: [  544/ 1070]\n",
      "validation loss: 0.501472  Batch: [  576/ 1070]\n",
      "validation loss: 0.589955  Batch: [  608/ 1070]\n",
      "validation loss: 0.503682  Batch: [  640/ 1070]\n",
      "validation loss: 0.397975  Batch: [  672/ 1070]\n",
      "validation loss: 0.608739  Batch: [  704/ 1070]\n",
      "validation loss: 0.579242  Batch: [  736/ 1070]\n",
      "validation loss: 0.480262  Batch: [  768/ 1070]\n",
      "validation loss: 0.437178  Batch: [  800/ 1070]\n",
      "validation loss: 0.515614  Batch: [  832/ 1070]\n",
      "validation loss: 0.676697  Batch: [  864/ 1070]\n",
      "validation loss: 0.508556  Batch: [  896/ 1070]\n",
      "validation loss: 0.398333  Batch: [  928/ 1070]\n",
      "validation loss: 0.348577  Batch: [  960/ 1070]\n",
      "validation loss: 0.451958  Batch: [  992/ 1070]\n",
      "validation loss: 0.479253  Batch: [ 1024/ 1070]\n",
      "validation loss: 0.324133  Batch: [ 1056/ 1070]\n",
      "validation loss: 0.616735  Batch: [ 1070/ 1070]\n",
      "Epoch 10/50, Validation Loss: 0.4863, Validation Accuracy: 76.73%\n",
      "\n",
      "\n",
      "Epoch 11 \n",
      "---------------------\n",
      "training loss: 0.463202  Batch: [   32/ 4992]\n",
      "training loss: 0.397799  Batch: [   64/ 4992]\n",
      "training loss: 0.515333  Batch: [   96/ 4992]\n",
      "training loss: 0.526792  Batch: [  128/ 4992]\n",
      "training loss: 0.471815  Batch: [  160/ 4992]\n",
      "training loss: 0.469230  Batch: [  192/ 4992]\n",
      "training loss: 0.451957  Batch: [  224/ 4992]\n",
      "training loss: 0.556037  Batch: [  256/ 4992]\n",
      "training loss: 0.290545  Batch: [  288/ 4992]\n",
      "training loss: 0.429245  Batch: [  320/ 4992]\n",
      "training loss: 0.361544  Batch: [  352/ 4992]\n",
      "training loss: 0.360708  Batch: [  384/ 4992]\n",
      "training loss: 0.419480  Batch: [  416/ 4992]\n",
      "training loss: 0.414657  Batch: [  448/ 4992]\n",
      "training loss: 0.518517  Batch: [  480/ 4992]\n",
      "training loss: 0.477746  Batch: [  512/ 4992]\n",
      "training loss: 0.464141  Batch: [  544/ 4992]\n",
      "training loss: 0.469389  Batch: [  576/ 4992]\n",
      "training loss: 0.580214  Batch: [  608/ 4992]\n",
      "training loss: 0.472357  Batch: [  640/ 4992]\n",
      "training loss: 0.432368  Batch: [  672/ 4992]\n",
      "training loss: 0.513684  Batch: [  704/ 4992]\n",
      "training loss: 0.541327  Batch: [  736/ 4992]\n",
      "training loss: 0.561449  Batch: [  768/ 4992]\n",
      "training loss: 0.661202  Batch: [  800/ 4992]\n",
      "training loss: 0.460028  Batch: [  832/ 4992]\n",
      "training loss: 0.571766  Batch: [  864/ 4992]\n",
      "training loss: 0.542169  Batch: [  896/ 4992]\n",
      "training loss: 0.524062  Batch: [  928/ 4992]\n",
      "training loss: 0.472627  Batch: [  960/ 4992]\n",
      "training loss: 0.561637  Batch: [  992/ 4992]\n",
      "training loss: 0.538042  Batch: [ 1024/ 4992]\n",
      "training loss: 0.469942  Batch: [ 1056/ 4992]\n",
      "training loss: 0.420365  Batch: [ 1088/ 4992]\n",
      "training loss: 0.549317  Batch: [ 1120/ 4992]\n",
      "training loss: 0.535167  Batch: [ 1152/ 4992]\n",
      "training loss: 0.446047  Batch: [ 1184/ 4992]\n",
      "training loss: 0.377161  Batch: [ 1216/ 4992]\n",
      "training loss: 0.397167  Batch: [ 1248/ 4992]\n",
      "training loss: 0.565833  Batch: [ 1280/ 4992]\n",
      "training loss: 0.438508  Batch: [ 1312/ 4992]\n",
      "training loss: 0.615289  Batch: [ 1344/ 4992]\n",
      "training loss: 0.674689  Batch: [ 1376/ 4992]\n",
      "training loss: 0.488086  Batch: [ 1408/ 4992]\n",
      "training loss: 0.394819  Batch: [ 1440/ 4992]\n",
      "training loss: 0.490671  Batch: [ 1472/ 4992]\n",
      "training loss: 0.590844  Batch: [ 1504/ 4992]\n",
      "training loss: 0.600739  Batch: [ 1536/ 4992]\n",
      "training loss: 0.395846  Batch: [ 1568/ 4992]\n",
      "training loss: 0.490909  Batch: [ 1600/ 4992]\n",
      "training loss: 0.438138  Batch: [ 1632/ 4992]\n",
      "training loss: 0.567162  Batch: [ 1664/ 4992]\n",
      "training loss: 0.818231  Batch: [ 1696/ 4992]\n",
      "training loss: 0.617528  Batch: [ 1728/ 4992]\n",
      "training loss: 0.444371  Batch: [ 1760/ 4992]\n",
      "training loss: 0.561728  Batch: [ 1792/ 4992]\n",
      "training loss: 0.535163  Batch: [ 1824/ 4992]\n",
      "training loss: 0.484533  Batch: [ 1856/ 4992]\n",
      "training loss: 0.536115  Batch: [ 1888/ 4992]\n",
      "training loss: 0.414832  Batch: [ 1920/ 4992]\n",
      "training loss: 0.530404  Batch: [ 1952/ 4992]\n",
      "training loss: 0.537040  Batch: [ 1984/ 4992]\n",
      "training loss: 0.524276  Batch: [ 2016/ 4992]\n",
      "training loss: 0.543705  Batch: [ 2048/ 4992]\n",
      "training loss: 0.392890  Batch: [ 2080/ 4992]\n",
      "training loss: 0.409636  Batch: [ 2112/ 4992]\n",
      "training loss: 0.525441  Batch: [ 2144/ 4992]\n",
      "training loss: 0.493693  Batch: [ 2176/ 4992]\n",
      "training loss: 0.410647  Batch: [ 2208/ 4992]\n",
      "training loss: 0.421270  Batch: [ 2240/ 4992]\n",
      "training loss: 0.496898  Batch: [ 2272/ 4992]\n",
      "training loss: 0.425011  Batch: [ 2304/ 4992]\n",
      "training loss: 0.499426  Batch: [ 2336/ 4992]\n",
      "training loss: 0.432092  Batch: [ 2368/ 4992]\n",
      "training loss: 0.440708  Batch: [ 2400/ 4992]\n",
      "training loss: 0.404774  Batch: [ 2432/ 4992]\n",
      "training loss: 0.748703  Batch: [ 2464/ 4992]\n",
      "training loss: 0.370409  Batch: [ 2496/ 4992]\n",
      "training loss: 0.462311  Batch: [ 2528/ 4992]\n",
      "training loss: 0.531759  Batch: [ 2560/ 4992]\n",
      "training loss: 0.418646  Batch: [ 2592/ 4992]\n",
      "training loss: 0.530834  Batch: [ 2624/ 4992]\n",
      "training loss: 0.565536  Batch: [ 2656/ 4992]\n",
      "training loss: 0.720045  Batch: [ 2688/ 4992]\n",
      "training loss: 0.474651  Batch: [ 2720/ 4992]\n",
      "training loss: 0.468020  Batch: [ 2752/ 4992]\n",
      "training loss: 0.476730  Batch: [ 2784/ 4992]\n",
      "training loss: 0.475516  Batch: [ 2816/ 4992]\n",
      "training loss: 0.382992  Batch: [ 2848/ 4992]\n",
      "training loss: 0.484042  Batch: [ 2880/ 4992]\n",
      "training loss: 0.556301  Batch: [ 2912/ 4992]\n",
      "training loss: 0.610275  Batch: [ 2944/ 4992]\n",
      "training loss: 0.562149  Batch: [ 2976/ 4992]\n",
      "training loss: 0.467035  Batch: [ 3008/ 4992]\n",
      "training loss: 0.624319  Batch: [ 3040/ 4992]\n",
      "training loss: 0.430410  Batch: [ 3072/ 4992]\n",
      "training loss: 0.612067  Batch: [ 3104/ 4992]\n",
      "training loss: 0.383854  Batch: [ 3136/ 4992]\n",
      "training loss: 0.424820  Batch: [ 3168/ 4992]\n",
      "training loss: 0.453270  Batch: [ 3200/ 4992]\n",
      "training loss: 0.459197  Batch: [ 3232/ 4992]\n",
      "training loss: 0.532556  Batch: [ 3264/ 4992]\n",
      "training loss: 0.469377  Batch: [ 3296/ 4992]\n",
      "training loss: 0.595844  Batch: [ 3328/ 4992]\n",
      "training loss: 0.532564  Batch: [ 3360/ 4992]\n",
      "training loss: 0.462644  Batch: [ 3392/ 4992]\n",
      "training loss: 0.491376  Batch: [ 3424/ 4992]\n",
      "training loss: 0.423652  Batch: [ 3456/ 4992]\n",
      "training loss: 0.368739  Batch: [ 3488/ 4992]\n",
      "training loss: 0.617408  Batch: [ 3520/ 4992]\n",
      "training loss: 0.530830  Batch: [ 3552/ 4992]\n",
      "training loss: 0.477792  Batch: [ 3584/ 4992]\n",
      "training loss: 0.516732  Batch: [ 3616/ 4992]\n",
      "training loss: 0.561268  Batch: [ 3648/ 4992]\n",
      "training loss: 0.440605  Batch: [ 3680/ 4992]\n",
      "training loss: 0.536351  Batch: [ 3712/ 4992]\n",
      "training loss: 0.481925  Batch: [ 3744/ 4992]\n",
      "training loss: 0.463274  Batch: [ 3776/ 4992]\n",
      "training loss: 0.573338  Batch: [ 3808/ 4992]\n",
      "training loss: 0.396536  Batch: [ 3840/ 4992]\n",
      "training loss: 0.355330  Batch: [ 3872/ 4992]\n",
      "training loss: 0.530955  Batch: [ 3904/ 4992]\n",
      "training loss: 0.374637  Batch: [ 3936/ 4992]\n",
      "training loss: 0.425851  Batch: [ 3968/ 4992]\n",
      "training loss: 0.545092  Batch: [ 4000/ 4992]\n",
      "training loss: 0.435834  Batch: [ 4032/ 4992]\n",
      "training loss: 0.682221  Batch: [ 4064/ 4992]\n",
      "training loss: 0.514868  Batch: [ 4096/ 4992]\n",
      "training loss: 0.447717  Batch: [ 4128/ 4992]\n",
      "training loss: 0.450602  Batch: [ 4160/ 4992]\n",
      "training loss: 0.368395  Batch: [ 4192/ 4992]\n",
      "training loss: 0.631242  Batch: [ 4224/ 4992]\n",
      "training loss: 0.483970  Batch: [ 4256/ 4992]\n",
      "training loss: 0.416386  Batch: [ 4288/ 4992]\n",
      "training loss: 0.376946  Batch: [ 4320/ 4992]\n",
      "training loss: 0.369730  Batch: [ 4352/ 4992]\n",
      "training loss: 0.477824  Batch: [ 4384/ 4992]\n",
      "training loss: 0.555924  Batch: [ 4416/ 4992]\n",
      "training loss: 0.462544  Batch: [ 4448/ 4992]\n",
      "training loss: 0.473367  Batch: [ 4480/ 4992]\n",
      "training loss: 0.448466  Batch: [ 4512/ 4992]\n",
      "training loss: 0.486727  Batch: [ 4544/ 4992]\n",
      "training loss: 0.514349  Batch: [ 4576/ 4992]\n",
      "training loss: 0.440943  Batch: [ 4608/ 4992]\n",
      "training loss: 0.485658  Batch: [ 4640/ 4992]\n",
      "training loss: 0.480709  Batch: [ 4672/ 4992]\n",
      "training loss: 0.317075  Batch: [ 4704/ 4992]\n",
      "training loss: 0.474983  Batch: [ 4736/ 4992]\n",
      "training loss: 0.461560  Batch: [ 4768/ 4992]\n",
      "training loss: 0.587957  Batch: [ 4800/ 4992]\n",
      "training loss: 0.439225  Batch: [ 4832/ 4992]\n",
      "training loss: 0.574012  Batch: [ 4864/ 4992]\n",
      "training loss: 0.537805  Batch: [ 4896/ 4992]\n",
      "training loss: 0.652206  Batch: [ 4928/ 4992]\n",
      "training loss: 0.312408  Batch: [ 4960/ 4992]\n",
      "training loss: 0.401256  Batch: [ 4992/ 4992]\n",
      "Epoch 11/50, Training Loss: 15.6922\n",
      "validation loss: 0.571604  Batch: [   32/ 1070]\n",
      "validation loss: 0.539935  Batch: [   64/ 1070]\n",
      "validation loss: 0.484471  Batch: [   96/ 1070]\n",
      "validation loss: 0.537964  Batch: [  128/ 1070]\n",
      "validation loss: 0.527845  Batch: [  160/ 1070]\n",
      "validation loss: 0.456460  Batch: [  192/ 1070]\n",
      "validation loss: 0.493840  Batch: [  224/ 1070]\n",
      "validation loss: 0.391462  Batch: [  256/ 1070]\n",
      "validation loss: 0.417801  Batch: [  288/ 1070]\n",
      "validation loss: 0.574700  Batch: [  320/ 1070]\n",
      "validation loss: 0.404226  Batch: [  352/ 1070]\n",
      "validation loss: 0.555758  Batch: [  384/ 1070]\n",
      "validation loss: 0.425713  Batch: [  416/ 1070]\n",
      "validation loss: 0.529830  Batch: [  448/ 1070]\n",
      "validation loss: 0.478827  Batch: [  480/ 1070]\n",
      "validation loss: 0.615530  Batch: [  512/ 1070]\n",
      "validation loss: 0.443259  Batch: [  544/ 1070]\n",
      "validation loss: 0.428562  Batch: [  576/ 1070]\n",
      "validation loss: 0.552449  Batch: [  608/ 1070]\n",
      "validation loss: 0.492725  Batch: [  640/ 1070]\n",
      "validation loss: 0.456179  Batch: [  672/ 1070]\n",
      "validation loss: 0.569195  Batch: [  704/ 1070]\n",
      "validation loss: 0.423943  Batch: [  736/ 1070]\n",
      "validation loss: 0.515329  Batch: [  768/ 1070]\n",
      "validation loss: 0.465229  Batch: [  800/ 1070]\n",
      "validation loss: 0.435388  Batch: [  832/ 1070]\n",
      "validation loss: 0.637896  Batch: [  864/ 1070]\n",
      "validation loss: 0.470225  Batch: [  896/ 1070]\n",
      "validation loss: 0.538984  Batch: [  928/ 1070]\n",
      "validation loss: 0.563046  Batch: [  960/ 1070]\n",
      "validation loss: 0.508068  Batch: [  992/ 1070]\n",
      "validation loss: 0.463781  Batch: [ 1024/ 1070]\n",
      "validation loss: 0.705271  Batch: [ 1056/ 1070]\n",
      "validation loss: 0.587413  Batch: [ 1070/ 1070]\n",
      "Epoch 11/50, Validation Loss: 0.5064, Validation Accuracy: 75.14%\n",
      "\n",
      "\n",
      "Epoch 12 \n",
      "---------------------\n",
      "training loss: 0.415715  Batch: [   32/ 4992]\n",
      "training loss: 0.688371  Batch: [   64/ 4992]\n",
      "training loss: 0.573014  Batch: [   96/ 4992]\n",
      "training loss: 0.684692  Batch: [  128/ 4992]\n",
      "training loss: 0.543679  Batch: [  160/ 4992]\n",
      "training loss: 0.304243  Batch: [  192/ 4992]\n",
      "training loss: 0.389708  Batch: [  224/ 4992]\n",
      "training loss: 0.469392  Batch: [  256/ 4992]\n",
      "training loss: 0.401773  Batch: [  288/ 4992]\n",
      "training loss: 0.435980  Batch: [  320/ 4992]\n",
      "training loss: 0.665205  Batch: [  352/ 4992]\n",
      "training loss: 0.496237  Batch: [  384/ 4992]\n",
      "training loss: 0.412418  Batch: [  416/ 4992]\n",
      "training loss: 0.446702  Batch: [  448/ 4992]\n",
      "training loss: 0.497994  Batch: [  480/ 4992]\n",
      "training loss: 0.444115  Batch: [  512/ 4992]\n",
      "training loss: 0.604990  Batch: [  544/ 4992]\n",
      "training loss: 0.485077  Batch: [  576/ 4992]\n",
      "training loss: 0.453194  Batch: [  608/ 4992]\n",
      "training loss: 0.491863  Batch: [  640/ 4992]\n",
      "training loss: 0.432880  Batch: [  672/ 4992]\n",
      "training loss: 0.650639  Batch: [  704/ 4992]\n",
      "training loss: 0.591922  Batch: [  736/ 4992]\n",
      "training loss: 0.562578  Batch: [  768/ 4992]\n",
      "training loss: 0.607666  Batch: [  800/ 4992]\n",
      "training loss: 0.436432  Batch: [  832/ 4992]\n",
      "training loss: 0.403429  Batch: [  864/ 4992]\n",
      "training loss: 0.552237  Batch: [  896/ 4992]\n",
      "training loss: 0.375293  Batch: [  928/ 4992]\n",
      "training loss: 0.433223  Batch: [  960/ 4992]\n",
      "training loss: 0.358910  Batch: [  992/ 4992]\n",
      "training loss: 0.421834  Batch: [ 1024/ 4992]\n",
      "training loss: 0.494840  Batch: [ 1056/ 4992]\n",
      "training loss: 0.477965  Batch: [ 1088/ 4992]\n",
      "training loss: 0.618364  Batch: [ 1120/ 4992]\n",
      "training loss: 0.351869  Batch: [ 1152/ 4992]\n",
      "training loss: 0.355424  Batch: [ 1184/ 4992]\n",
      "training loss: 0.392189  Batch: [ 1216/ 4992]\n",
      "training loss: 0.574822  Batch: [ 1248/ 4992]\n",
      "training loss: 0.474883  Batch: [ 1280/ 4992]\n",
      "training loss: 0.383337  Batch: [ 1312/ 4992]\n",
      "training loss: 0.648083  Batch: [ 1344/ 4992]\n",
      "training loss: 0.424377  Batch: [ 1376/ 4992]\n",
      "training loss: 0.359944  Batch: [ 1408/ 4992]\n",
      "training loss: 0.479873  Batch: [ 1440/ 4992]\n",
      "training loss: 0.451976  Batch: [ 1472/ 4992]\n",
      "training loss: 0.504401  Batch: [ 1504/ 4992]\n",
      "training loss: 0.410330  Batch: [ 1536/ 4992]\n",
      "training loss: 0.472960  Batch: [ 1568/ 4992]\n",
      "training loss: 0.497245  Batch: [ 1600/ 4992]\n",
      "training loss: 0.517742  Batch: [ 1632/ 4992]\n",
      "training loss: 0.496743  Batch: [ 1664/ 4992]\n",
      "training loss: 0.476891  Batch: [ 1696/ 4992]\n",
      "training loss: 0.512585  Batch: [ 1728/ 4992]\n",
      "training loss: 0.574959  Batch: [ 1760/ 4992]\n",
      "training loss: 0.388763  Batch: [ 1792/ 4992]\n",
      "training loss: 0.588885  Batch: [ 1824/ 4992]\n",
      "training loss: 0.493520  Batch: [ 1856/ 4992]\n",
      "training loss: 0.407504  Batch: [ 1888/ 4992]\n",
      "training loss: 0.412556  Batch: [ 1920/ 4992]\n",
      "training loss: 0.715573  Batch: [ 1952/ 4992]\n",
      "training loss: 0.476561  Batch: [ 1984/ 4992]\n",
      "training loss: 0.354054  Batch: [ 2016/ 4992]\n",
      "training loss: 0.516502  Batch: [ 2048/ 4992]\n",
      "training loss: 0.462256  Batch: [ 2080/ 4992]\n",
      "training loss: 0.542020  Batch: [ 2112/ 4992]\n",
      "training loss: 0.432891  Batch: [ 2144/ 4992]\n",
      "training loss: 0.526139  Batch: [ 2176/ 4992]\n",
      "training loss: 0.385881  Batch: [ 2208/ 4992]\n",
      "training loss: 0.480942  Batch: [ 2240/ 4992]\n",
      "training loss: 0.458448  Batch: [ 2272/ 4992]\n",
      "training loss: 0.678158  Batch: [ 2304/ 4992]\n",
      "training loss: 0.449093  Batch: [ 2336/ 4992]\n",
      "training loss: 0.569227  Batch: [ 2368/ 4992]\n",
      "training loss: 0.384088  Batch: [ 2400/ 4992]\n",
      "training loss: 0.560960  Batch: [ 2432/ 4992]\n",
      "training loss: 0.384407  Batch: [ 2464/ 4992]\n",
      "training loss: 0.421402  Batch: [ 2496/ 4992]\n",
      "training loss: 0.550329  Batch: [ 2528/ 4992]\n",
      "training loss: 0.436724  Batch: [ 2560/ 4992]\n",
      "training loss: 0.476027  Batch: [ 2592/ 4992]\n",
      "training loss: 0.612104  Batch: [ 2624/ 4992]\n",
      "training loss: 0.538716  Batch: [ 2656/ 4992]\n",
      "training loss: 0.552028  Batch: [ 2688/ 4992]\n",
      "training loss: 0.427645  Batch: [ 2720/ 4992]\n",
      "training loss: 0.486705  Batch: [ 2752/ 4992]\n",
      "training loss: 0.653922  Batch: [ 2784/ 4992]\n",
      "training loss: 0.541885  Batch: [ 2816/ 4992]\n",
      "training loss: 0.488672  Batch: [ 2848/ 4992]\n",
      "training loss: 0.677332  Batch: [ 2880/ 4992]\n",
      "training loss: 0.451011  Batch: [ 2912/ 4992]\n",
      "training loss: 0.585544  Batch: [ 2944/ 4992]\n",
      "training loss: 0.481769  Batch: [ 2976/ 4992]\n",
      "training loss: 0.443763  Batch: [ 3008/ 4992]\n",
      "training loss: 0.405035  Batch: [ 3040/ 4992]\n",
      "training loss: 0.531566  Batch: [ 3072/ 4992]\n",
      "training loss: 0.561176  Batch: [ 3104/ 4992]\n",
      "training loss: 0.397815  Batch: [ 3136/ 4992]\n",
      "training loss: 0.390223  Batch: [ 3168/ 4992]\n",
      "training loss: 0.470666  Batch: [ 3200/ 4992]\n",
      "training loss: 0.443279  Batch: [ 3232/ 4992]\n",
      "training loss: 0.472252  Batch: [ 3264/ 4992]\n",
      "training loss: 0.368543  Batch: [ 3296/ 4992]\n",
      "training loss: 0.517739  Batch: [ 3328/ 4992]\n",
      "training loss: 0.547014  Batch: [ 3360/ 4992]\n",
      "training loss: 0.416471  Batch: [ 3392/ 4992]\n",
      "training loss: 0.465085  Batch: [ 3424/ 4992]\n",
      "training loss: 0.626039  Batch: [ 3456/ 4992]\n",
      "training loss: 0.466038  Batch: [ 3488/ 4992]\n",
      "training loss: 0.433135  Batch: [ 3520/ 4992]\n",
      "training loss: 0.367512  Batch: [ 3552/ 4992]\n",
      "training loss: 0.448788  Batch: [ 3584/ 4992]\n",
      "training loss: 0.470412  Batch: [ 3616/ 4992]\n",
      "training loss: 0.404559  Batch: [ 3648/ 4992]\n",
      "training loss: 0.524284  Batch: [ 3680/ 4992]\n",
      "training loss: 0.423188  Batch: [ 3712/ 4992]\n",
      "training loss: 0.438062  Batch: [ 3744/ 4992]\n",
      "training loss: 0.344397  Batch: [ 3776/ 4992]\n",
      "training loss: 0.486590  Batch: [ 3808/ 4992]\n",
      "training loss: 0.515442  Batch: [ 3840/ 4992]\n",
      "training loss: 0.541718  Batch: [ 3872/ 4992]\n",
      "training loss: 0.485286  Batch: [ 3904/ 4992]\n",
      "training loss: 0.383458  Batch: [ 3936/ 4992]\n",
      "training loss: 0.378223  Batch: [ 3968/ 4992]\n",
      "training loss: 0.436384  Batch: [ 4000/ 4992]\n",
      "training loss: 0.327377  Batch: [ 4032/ 4992]\n",
      "training loss: 0.371392  Batch: [ 4064/ 4992]\n",
      "training loss: 0.417503  Batch: [ 4096/ 4992]\n",
      "training loss: 0.410761  Batch: [ 4128/ 4992]\n",
      "training loss: 0.579297  Batch: [ 4160/ 4992]\n",
      "training loss: 0.481313  Batch: [ 4192/ 4992]\n",
      "training loss: 0.462098  Batch: [ 4224/ 4992]\n",
      "training loss: 0.429231  Batch: [ 4256/ 4992]\n",
      "training loss: 0.770421  Batch: [ 4288/ 4992]\n",
      "training loss: 0.345628  Batch: [ 4320/ 4992]\n",
      "training loss: 0.481615  Batch: [ 4352/ 4992]\n",
      "training loss: 0.471324  Batch: [ 4384/ 4992]\n",
      "training loss: 0.605917  Batch: [ 4416/ 4992]\n",
      "training loss: 0.391735  Batch: [ 4448/ 4992]\n",
      "training loss: 0.551254  Batch: [ 4480/ 4992]\n",
      "training loss: 0.446923  Batch: [ 4512/ 4992]\n",
      "training loss: 0.451723  Batch: [ 4544/ 4992]\n",
      "training loss: 0.437981  Batch: [ 4576/ 4992]\n",
      "training loss: 0.600289  Batch: [ 4608/ 4992]\n",
      "training loss: 0.433815  Batch: [ 4640/ 4992]\n",
      "training loss: 0.404981  Batch: [ 4672/ 4992]\n",
      "training loss: 0.456482  Batch: [ 4704/ 4992]\n",
      "training loss: 0.593579  Batch: [ 4736/ 4992]\n",
      "training loss: 0.301757  Batch: [ 4768/ 4992]\n",
      "training loss: 0.439621  Batch: [ 4800/ 4992]\n",
      "training loss: 0.392706  Batch: [ 4832/ 4992]\n",
      "training loss: 0.574715  Batch: [ 4864/ 4992]\n",
      "training loss: 0.455893  Batch: [ 4896/ 4992]\n",
      "training loss: 0.406782  Batch: [ 4928/ 4992]\n",
      "training loss: 0.531534  Batch: [ 4960/ 4992]\n",
      "training loss: 0.303470  Batch: [ 4992/ 4992]\n",
      "Epoch 12/50, Training Loss: 15.3162\n",
      "validation loss: 0.469320  Batch: [   32/ 1070]\n",
      "validation loss: 0.557921  Batch: [   64/ 1070]\n",
      "validation loss: 0.396188  Batch: [   96/ 1070]\n",
      "validation loss: 0.325572  Batch: [  128/ 1070]\n",
      "validation loss: 0.422052  Batch: [  160/ 1070]\n",
      "validation loss: 0.426209  Batch: [  192/ 1070]\n",
      "validation loss: 0.547565  Batch: [  224/ 1070]\n",
      "validation loss: 0.355985  Batch: [  256/ 1070]\n",
      "validation loss: 0.445250  Batch: [  288/ 1070]\n",
      "validation loss: 0.416326  Batch: [  320/ 1070]\n",
      "validation loss: 0.477995  Batch: [  352/ 1070]\n",
      "validation loss: 0.451930  Batch: [  384/ 1070]\n",
      "validation loss: 0.623808  Batch: [  416/ 1070]\n",
      "validation loss: 0.544051  Batch: [  448/ 1070]\n",
      "validation loss: 0.374461  Batch: [  480/ 1070]\n",
      "validation loss: 0.493527  Batch: [  512/ 1070]\n",
      "validation loss: 0.311987  Batch: [  544/ 1070]\n",
      "validation loss: 0.458085  Batch: [  576/ 1070]\n",
      "validation loss: 0.538969  Batch: [  608/ 1070]\n",
      "validation loss: 0.433070  Batch: [  640/ 1070]\n",
      "validation loss: 0.580020  Batch: [  672/ 1070]\n",
      "validation loss: 0.463861  Batch: [  704/ 1070]\n",
      "validation loss: 0.578476  Batch: [  736/ 1070]\n",
      "validation loss: 0.352404  Batch: [  768/ 1070]\n",
      "validation loss: 0.394924  Batch: [  800/ 1070]\n",
      "validation loss: 0.442771  Batch: [  832/ 1070]\n",
      "validation loss: 0.434442  Batch: [  864/ 1070]\n",
      "validation loss: 0.546344  Batch: [  896/ 1070]\n",
      "validation loss: 0.395876  Batch: [  928/ 1070]\n",
      "validation loss: 0.541029  Batch: [  960/ 1070]\n",
      "validation loss: 0.423053  Batch: [  992/ 1070]\n",
      "validation loss: 0.520409  Batch: [ 1024/ 1070]\n",
      "validation loss: 0.524413  Batch: [ 1056/ 1070]\n",
      "validation loss: 0.622267  Batch: [ 1070/ 1070]\n",
      "Epoch 12/50, Validation Loss: 0.4648, Validation Accuracy: 77.85%\n",
      "\n",
      "\n",
      "Epoch 13 \n",
      "---------------------\n",
      "training loss: 0.432501  Batch: [   32/ 4992]\n",
      "training loss: 0.493624  Batch: [   64/ 4992]\n",
      "training loss: 0.498312  Batch: [   96/ 4992]\n",
      "training loss: 0.453495  Batch: [  128/ 4992]\n",
      "training loss: 0.415709  Batch: [  160/ 4992]\n",
      "training loss: 0.473150  Batch: [  192/ 4992]\n",
      "training loss: 0.335941  Batch: [  224/ 4992]\n",
      "training loss: 0.520624  Batch: [  256/ 4992]\n",
      "training loss: 0.615796  Batch: [  288/ 4992]\n",
      "training loss: 0.543011  Batch: [  320/ 4992]\n",
      "training loss: 0.586455  Batch: [  352/ 4992]\n",
      "training loss: 0.560067  Batch: [  384/ 4992]\n",
      "training loss: 0.652365  Batch: [  416/ 4992]\n",
      "training loss: 0.586753  Batch: [  448/ 4992]\n",
      "training loss: 0.435213  Batch: [  480/ 4992]\n",
      "training loss: 0.411433  Batch: [  512/ 4992]\n",
      "training loss: 0.526754  Batch: [  544/ 4992]\n",
      "training loss: 0.465003  Batch: [  576/ 4992]\n",
      "training loss: 0.424185  Batch: [  608/ 4992]\n",
      "training loss: 0.649939  Batch: [  640/ 4992]\n",
      "training loss: 0.608534  Batch: [  672/ 4992]\n",
      "training loss: 0.315590  Batch: [  704/ 4992]\n",
      "training loss: 0.523666  Batch: [  736/ 4992]\n",
      "training loss: 0.670887  Batch: [  768/ 4992]\n",
      "training loss: 0.422825  Batch: [  800/ 4992]\n",
      "training loss: 0.515064  Batch: [  832/ 4992]\n",
      "training loss: 0.388635  Batch: [  864/ 4992]\n",
      "training loss: 0.550514  Batch: [  896/ 4992]\n",
      "training loss: 0.599085  Batch: [  928/ 4992]\n",
      "training loss: 0.617796  Batch: [  960/ 4992]\n",
      "training loss: 0.669747  Batch: [  992/ 4992]\n",
      "training loss: 0.596879  Batch: [ 1024/ 4992]\n",
      "training loss: 0.573678  Batch: [ 1056/ 4992]\n",
      "training loss: 0.535164  Batch: [ 1088/ 4992]\n",
      "training loss: 0.378571  Batch: [ 1120/ 4992]\n",
      "training loss: 0.477539  Batch: [ 1152/ 4992]\n",
      "training loss: 0.357751  Batch: [ 1184/ 4992]\n",
      "training loss: 0.451069  Batch: [ 1216/ 4992]\n",
      "training loss: 0.487714  Batch: [ 1248/ 4992]\n",
      "training loss: 0.674057  Batch: [ 1280/ 4992]\n",
      "training loss: 0.841728  Batch: [ 1312/ 4992]\n",
      "training loss: 0.616111  Batch: [ 1344/ 4992]\n",
      "training loss: 0.598861  Batch: [ 1376/ 4992]\n",
      "training loss: 0.548938  Batch: [ 1408/ 4992]\n",
      "training loss: 0.620111  Batch: [ 1440/ 4992]\n",
      "training loss: 0.551557  Batch: [ 1472/ 4992]\n",
      "training loss: 0.480197  Batch: [ 1504/ 4992]\n",
      "training loss: 0.468109  Batch: [ 1536/ 4992]\n",
      "training loss: 0.555670  Batch: [ 1568/ 4992]\n",
      "training loss: 0.603611  Batch: [ 1600/ 4992]\n",
      "training loss: 0.569308  Batch: [ 1632/ 4992]\n",
      "training loss: 0.493104  Batch: [ 1664/ 4992]\n",
      "training loss: 0.579622  Batch: [ 1696/ 4992]\n",
      "training loss: 0.477603  Batch: [ 1728/ 4992]\n",
      "training loss: 0.404868  Batch: [ 1760/ 4992]\n",
      "training loss: 0.359703  Batch: [ 1792/ 4992]\n",
      "training loss: 0.557349  Batch: [ 1824/ 4992]\n",
      "training loss: 0.481924  Batch: [ 1856/ 4992]\n",
      "training loss: 0.553486  Batch: [ 1888/ 4992]\n",
      "training loss: 0.485208  Batch: [ 1920/ 4992]\n",
      "training loss: 0.485807  Batch: [ 1952/ 4992]\n",
      "training loss: 0.446913  Batch: [ 1984/ 4992]\n",
      "training loss: 0.484741  Batch: [ 2016/ 4992]\n",
      "training loss: 0.433197  Batch: [ 2048/ 4992]\n",
      "training loss: 0.533962  Batch: [ 2080/ 4992]\n",
      "training loss: 0.401821  Batch: [ 2112/ 4992]\n",
      "training loss: 0.559719  Batch: [ 2144/ 4992]\n",
      "training loss: 0.382470  Batch: [ 2176/ 4992]\n",
      "training loss: 0.586948  Batch: [ 2208/ 4992]\n",
      "training loss: 0.505979  Batch: [ 2240/ 4992]\n",
      "training loss: 0.484233  Batch: [ 2272/ 4992]\n",
      "training loss: 0.451149  Batch: [ 2304/ 4992]\n",
      "training loss: 0.569237  Batch: [ 2336/ 4992]\n",
      "training loss: 0.408205  Batch: [ 2368/ 4992]\n",
      "training loss: 0.561252  Batch: [ 2400/ 4992]\n",
      "training loss: 0.535702  Batch: [ 2432/ 4992]\n",
      "training loss: 0.506428  Batch: [ 2464/ 4992]\n",
      "training loss: 0.485886  Batch: [ 2496/ 4992]\n",
      "training loss: 0.446646  Batch: [ 2528/ 4992]\n",
      "training loss: 0.502510  Batch: [ 2560/ 4992]\n",
      "training loss: 0.384178  Batch: [ 2592/ 4992]\n",
      "training loss: 0.414980  Batch: [ 2624/ 4992]\n",
      "training loss: 0.592827  Batch: [ 2656/ 4992]\n",
      "training loss: 0.384410  Batch: [ 2688/ 4992]\n",
      "training loss: 0.545395  Batch: [ 2720/ 4992]\n",
      "training loss: 0.462628  Batch: [ 2752/ 4992]\n",
      "training loss: 0.393235  Batch: [ 2784/ 4992]\n",
      "training loss: 0.402503  Batch: [ 2816/ 4992]\n",
      "training loss: 0.384228  Batch: [ 2848/ 4992]\n",
      "training loss: 0.405862  Batch: [ 2880/ 4992]\n",
      "training loss: 0.478177  Batch: [ 2912/ 4992]\n",
      "training loss: 0.410902  Batch: [ 2944/ 4992]\n",
      "training loss: 0.393718  Batch: [ 2976/ 4992]\n",
      "training loss: 0.521658  Batch: [ 3008/ 4992]\n",
      "training loss: 0.673350  Batch: [ 3040/ 4992]\n",
      "training loss: 0.473566  Batch: [ 3072/ 4992]\n",
      "training loss: 0.436136  Batch: [ 3104/ 4992]\n",
      "training loss: 0.390720  Batch: [ 3136/ 4992]\n",
      "training loss: 0.450410  Batch: [ 3168/ 4992]\n",
      "training loss: 0.472465  Batch: [ 3200/ 4992]\n",
      "training loss: 0.532417  Batch: [ 3232/ 4992]\n",
      "training loss: 0.386860  Batch: [ 3264/ 4992]\n",
      "training loss: 0.467910  Batch: [ 3296/ 4992]\n",
      "training loss: 0.514672  Batch: [ 3328/ 4992]\n",
      "training loss: 0.463324  Batch: [ 3360/ 4992]\n",
      "training loss: 0.459090  Batch: [ 3392/ 4992]\n",
      "training loss: 0.424743  Batch: [ 3424/ 4992]\n",
      "training loss: 0.614642  Batch: [ 3456/ 4992]\n",
      "training loss: 0.614483  Batch: [ 3488/ 4992]\n",
      "training loss: 0.429931  Batch: [ 3520/ 4992]\n",
      "training loss: 0.476867  Batch: [ 3552/ 4992]\n",
      "training loss: 0.507839  Batch: [ 3584/ 4992]\n",
      "training loss: 0.529797  Batch: [ 3616/ 4992]\n",
      "training loss: 0.442750  Batch: [ 3648/ 4992]\n",
      "training loss: 0.428076  Batch: [ 3680/ 4992]\n",
      "training loss: 0.613815  Batch: [ 3712/ 4992]\n",
      "training loss: 0.453318  Batch: [ 3744/ 4992]\n",
      "training loss: 0.422624  Batch: [ 3776/ 4992]\n",
      "training loss: 0.505642  Batch: [ 3808/ 4992]\n",
      "training loss: 0.478550  Batch: [ 3840/ 4992]\n",
      "training loss: 0.525490  Batch: [ 3872/ 4992]\n",
      "training loss: 0.436888  Batch: [ 3904/ 4992]\n",
      "training loss: 0.339586  Batch: [ 3936/ 4992]\n",
      "training loss: 0.488558  Batch: [ 3968/ 4992]\n",
      "training loss: 0.554662  Batch: [ 4000/ 4992]\n",
      "training loss: 0.491139  Batch: [ 4032/ 4992]\n",
      "training loss: 0.519080  Batch: [ 4064/ 4992]\n",
      "training loss: 0.309079  Batch: [ 4096/ 4992]\n",
      "training loss: 0.537428  Batch: [ 4128/ 4992]\n",
      "training loss: 0.493251  Batch: [ 4160/ 4992]\n",
      "training loss: 0.589461  Batch: [ 4192/ 4992]\n",
      "training loss: 0.716110  Batch: [ 4224/ 4992]\n",
      "training loss: 0.491828  Batch: [ 4256/ 4992]\n",
      "training loss: 0.505396  Batch: [ 4288/ 4992]\n",
      "training loss: 0.497227  Batch: [ 4320/ 4992]\n",
      "training loss: 0.430704  Batch: [ 4352/ 4992]\n",
      "training loss: 0.351953  Batch: [ 4384/ 4992]\n",
      "training loss: 0.395410  Batch: [ 4416/ 4992]\n",
      "training loss: 0.417836  Batch: [ 4448/ 4992]\n",
      "training loss: 0.394814  Batch: [ 4480/ 4992]\n",
      "training loss: 0.419675  Batch: [ 4512/ 4992]\n",
      "training loss: 0.537871  Batch: [ 4544/ 4992]\n",
      "training loss: 0.352772  Batch: [ 4576/ 4992]\n",
      "training loss: 0.574391  Batch: [ 4608/ 4992]\n",
      "training loss: 0.544663  Batch: [ 4640/ 4992]\n",
      "training loss: 0.533711  Batch: [ 4672/ 4992]\n",
      "training loss: 0.420957  Batch: [ 4704/ 4992]\n",
      "training loss: 0.447412  Batch: [ 4736/ 4992]\n",
      "training loss: 0.446470  Batch: [ 4768/ 4992]\n",
      "training loss: 0.603321  Batch: [ 4800/ 4992]\n",
      "training loss: 0.381936  Batch: [ 4832/ 4992]\n",
      "training loss: 0.329397  Batch: [ 4864/ 4992]\n",
      "training loss: 0.498044  Batch: [ 4896/ 4992]\n",
      "training loss: 0.472299  Batch: [ 4928/ 4992]\n",
      "training loss: 0.536916  Batch: [ 4960/ 4992]\n",
      "training loss: 0.417535  Batch: [ 4992/ 4992]\n",
      "Epoch 13/50, Training Loss: 15.7860\n",
      "validation loss: 0.533862  Batch: [   32/ 1070]\n",
      "validation loss: 0.489170  Batch: [   64/ 1070]\n",
      "validation loss: 0.381601  Batch: [   96/ 1070]\n",
      "validation loss: 0.532747  Batch: [  128/ 1070]\n",
      "validation loss: 0.425503  Batch: [  160/ 1070]\n",
      "validation loss: 0.353476  Batch: [  192/ 1070]\n",
      "validation loss: 0.411586  Batch: [  224/ 1070]\n",
      "validation loss: 0.505828  Batch: [  256/ 1070]\n",
      "validation loss: 0.413576  Batch: [  288/ 1070]\n",
      "validation loss: 0.573516  Batch: [  320/ 1070]\n",
      "validation loss: 0.547666  Batch: [  352/ 1070]\n",
      "validation loss: 0.456936  Batch: [  384/ 1070]\n",
      "validation loss: 0.463550  Batch: [  416/ 1070]\n",
      "validation loss: 0.568179  Batch: [  448/ 1070]\n",
      "validation loss: 0.595529  Batch: [  480/ 1070]\n",
      "validation loss: 0.631946  Batch: [  512/ 1070]\n",
      "validation loss: 0.511470  Batch: [  544/ 1070]\n",
      "validation loss: 0.404646  Batch: [  576/ 1070]\n",
      "validation loss: 0.426748  Batch: [  608/ 1070]\n",
      "validation loss: 0.549288  Batch: [  640/ 1070]\n",
      "validation loss: 0.454443  Batch: [  672/ 1070]\n",
      "validation loss: 0.469131  Batch: [  704/ 1070]\n",
      "validation loss: 0.478488  Batch: [  736/ 1070]\n",
      "validation loss: 0.578104  Batch: [  768/ 1070]\n",
      "validation loss: 0.428799  Batch: [  800/ 1070]\n",
      "validation loss: 0.616936  Batch: [  832/ 1070]\n",
      "validation loss: 0.448182  Batch: [  864/ 1070]\n",
      "validation loss: 0.302546  Batch: [  896/ 1070]\n",
      "validation loss: 0.443512  Batch: [  928/ 1070]\n",
      "validation loss: 0.450406  Batch: [  960/ 1070]\n",
      "validation loss: 0.456110  Batch: [  992/ 1070]\n",
      "validation loss: 0.365693  Batch: [ 1024/ 1070]\n",
      "validation loss: 0.491005  Batch: [ 1056/ 1070]\n",
      "validation loss: 0.261400  Batch: [ 1070/ 1070]\n",
      "Epoch 13/50, Validation Loss: 0.4748, Validation Accuracy: 75.42%\n",
      "\n",
      "\n",
      "Epoch 14 \n",
      "---------------------\n",
      "training loss: 0.449990  Batch: [   32/ 4992]\n",
      "training loss: 0.465749  Batch: [   64/ 4992]\n",
      "training loss: 0.619680  Batch: [   96/ 4992]\n",
      "training loss: 0.445209  Batch: [  128/ 4992]\n",
      "training loss: 0.575050  Batch: [  160/ 4992]\n",
      "training loss: 0.415728  Batch: [  192/ 4992]\n",
      "training loss: 0.469574  Batch: [  224/ 4992]\n",
      "training loss: 0.514300  Batch: [  256/ 4992]\n",
      "training loss: 0.468122  Batch: [  288/ 4992]\n",
      "training loss: 0.428628  Batch: [  320/ 4992]\n",
      "training loss: 0.607691  Batch: [  352/ 4992]\n",
      "training loss: 0.393037  Batch: [  384/ 4992]\n",
      "training loss: 0.591494  Batch: [  416/ 4992]\n",
      "training loss: 0.522900  Batch: [  448/ 4992]\n",
      "training loss: 0.340259  Batch: [  480/ 4992]\n",
      "training loss: 0.354176  Batch: [  512/ 4992]\n",
      "training loss: 0.571458  Batch: [  544/ 4992]\n",
      "training loss: 0.496651  Batch: [  576/ 4992]\n",
      "training loss: 0.451581  Batch: [  608/ 4992]\n",
      "training loss: 0.384628  Batch: [  640/ 4992]\n",
      "training loss: 0.370577  Batch: [  672/ 4992]\n",
      "training loss: 0.429951  Batch: [  704/ 4992]\n",
      "training loss: 0.529842  Batch: [  736/ 4992]\n",
      "training loss: 0.395225  Batch: [  768/ 4992]\n",
      "training loss: 0.380383  Batch: [  800/ 4992]\n",
      "training loss: 0.595481  Batch: [  832/ 4992]\n",
      "training loss: 0.410558  Batch: [  864/ 4992]\n",
      "training loss: 0.658148  Batch: [  896/ 4992]\n",
      "training loss: 0.406399  Batch: [  928/ 4992]\n",
      "training loss: 0.493818  Batch: [  960/ 4992]\n",
      "training loss: 0.531532  Batch: [  992/ 4992]\n",
      "training loss: 0.515458  Batch: [ 1024/ 4992]\n",
      "training loss: 0.630530  Batch: [ 1056/ 4992]\n",
      "training loss: 0.492876  Batch: [ 1088/ 4992]\n",
      "training loss: 0.470667  Batch: [ 1120/ 4992]\n",
      "training loss: 0.524408  Batch: [ 1152/ 4992]\n",
      "training loss: 0.387461  Batch: [ 1184/ 4992]\n",
      "training loss: 0.398532  Batch: [ 1216/ 4992]\n",
      "training loss: 0.440599  Batch: [ 1248/ 4992]\n",
      "training loss: 0.463337  Batch: [ 1280/ 4992]\n",
      "training loss: 0.380510  Batch: [ 1312/ 4992]\n",
      "training loss: 0.516373  Batch: [ 1344/ 4992]\n",
      "training loss: 0.438476  Batch: [ 1376/ 4992]\n",
      "training loss: 0.532825  Batch: [ 1408/ 4992]\n",
      "training loss: 0.569589  Batch: [ 1440/ 4992]\n",
      "training loss: 0.469140  Batch: [ 1472/ 4992]\n",
      "training loss: 0.423200  Batch: [ 1504/ 4992]\n",
      "training loss: 0.644083  Batch: [ 1536/ 4992]\n",
      "training loss: 0.448020  Batch: [ 1568/ 4992]\n",
      "training loss: 0.585071  Batch: [ 1600/ 4992]\n",
      "training loss: 0.642384  Batch: [ 1632/ 4992]\n",
      "training loss: 0.499276  Batch: [ 1664/ 4992]\n",
      "training loss: 0.663958  Batch: [ 1696/ 4992]\n",
      "training loss: 0.410494  Batch: [ 1728/ 4992]\n",
      "training loss: 0.529567  Batch: [ 1760/ 4992]\n",
      "training loss: 0.379013  Batch: [ 1792/ 4992]\n",
      "training loss: 0.504096  Batch: [ 1824/ 4992]\n",
      "training loss: 0.506206  Batch: [ 1856/ 4992]\n",
      "training loss: 0.520647  Batch: [ 1888/ 4992]\n",
      "training loss: 0.445515  Batch: [ 1920/ 4992]\n",
      "training loss: 0.435739  Batch: [ 1952/ 4992]\n",
      "training loss: 0.486051  Batch: [ 1984/ 4992]\n",
      "training loss: 0.481949  Batch: [ 2016/ 4992]\n",
      "training loss: 0.537494  Batch: [ 2048/ 4992]\n",
      "training loss: 0.304372  Batch: [ 2080/ 4992]\n",
      "training loss: 0.479670  Batch: [ 2112/ 4992]\n",
      "training loss: 0.630102  Batch: [ 2144/ 4992]\n",
      "training loss: 0.523119  Batch: [ 2176/ 4992]\n",
      "training loss: 0.450158  Batch: [ 2208/ 4992]\n",
      "training loss: 0.479025  Batch: [ 2240/ 4992]\n",
      "training loss: 0.401960  Batch: [ 2272/ 4992]\n",
      "training loss: 0.461854  Batch: [ 2304/ 4992]\n",
      "training loss: 0.383201  Batch: [ 2336/ 4992]\n",
      "training loss: 0.552547  Batch: [ 2368/ 4992]\n",
      "training loss: 0.410695  Batch: [ 2400/ 4992]\n",
      "training loss: 0.381683  Batch: [ 2432/ 4992]\n",
      "training loss: 0.552262  Batch: [ 2464/ 4992]\n",
      "training loss: 0.463081  Batch: [ 2496/ 4992]\n",
      "training loss: 0.479080  Batch: [ 2528/ 4992]\n",
      "training loss: 0.507605  Batch: [ 2560/ 4992]\n",
      "training loss: 0.430435  Batch: [ 2592/ 4992]\n",
      "training loss: 0.453374  Batch: [ 2624/ 4992]\n",
      "training loss: 0.345070  Batch: [ 2656/ 4992]\n",
      "training loss: 0.441638  Batch: [ 2688/ 4992]\n",
      "training loss: 0.676811  Batch: [ 2720/ 4992]\n",
      "training loss: 0.467046  Batch: [ 2752/ 4992]\n",
      "training loss: 0.469430  Batch: [ 2784/ 4992]\n",
      "training loss: 0.532351  Batch: [ 2816/ 4992]\n",
      "training loss: 0.365626  Batch: [ 2848/ 4992]\n",
      "training loss: 0.284945  Batch: [ 2880/ 4992]\n",
      "training loss: 0.326698  Batch: [ 2912/ 4992]\n",
      "training loss: 0.429584  Batch: [ 2944/ 4992]\n",
      "training loss: 0.340019  Batch: [ 2976/ 4992]\n",
      "training loss: 0.558078  Batch: [ 3008/ 4992]\n",
      "training loss: 0.443473  Batch: [ 3040/ 4992]\n",
      "training loss: 0.549591  Batch: [ 3072/ 4992]\n",
      "training loss: 0.549171  Batch: [ 3104/ 4992]\n",
      "training loss: 0.436836  Batch: [ 3136/ 4992]\n",
      "training loss: 0.436738  Batch: [ 3168/ 4992]\n",
      "training loss: 0.441656  Batch: [ 3200/ 4992]\n",
      "training loss: 0.533911  Batch: [ 3232/ 4992]\n",
      "training loss: 0.414203  Batch: [ 3264/ 4992]\n",
      "training loss: 0.456021  Batch: [ 3296/ 4992]\n",
      "training loss: 0.428789  Batch: [ 3328/ 4992]\n",
      "training loss: 0.295530  Batch: [ 3360/ 4992]\n",
      "training loss: 0.569252  Batch: [ 3392/ 4992]\n",
      "training loss: 0.499718  Batch: [ 3424/ 4992]\n",
      "training loss: 0.480136  Batch: [ 3456/ 4992]\n",
      "training loss: 0.345914  Batch: [ 3488/ 4992]\n",
      "training loss: 0.378779  Batch: [ 3520/ 4992]\n",
      "training loss: 0.358168  Batch: [ 3552/ 4992]\n",
      "training loss: 0.702293  Batch: [ 3584/ 4992]\n",
      "training loss: 0.413254  Batch: [ 3616/ 4992]\n",
      "training loss: 0.673052  Batch: [ 3648/ 4992]\n",
      "training loss: 0.388546  Batch: [ 3680/ 4992]\n",
      "training loss: 0.539541  Batch: [ 3712/ 4992]\n",
      "training loss: 0.328671  Batch: [ 3744/ 4992]\n",
      "training loss: 0.465353  Batch: [ 3776/ 4992]\n",
      "training loss: 0.363538  Batch: [ 3808/ 4992]\n",
      "training loss: 0.424646  Batch: [ 3840/ 4992]\n",
      "training loss: 0.507963  Batch: [ 3872/ 4992]\n",
      "training loss: 0.529271  Batch: [ 3904/ 4992]\n",
      "training loss: 0.632128  Batch: [ 3936/ 4992]\n",
      "training loss: 0.507021  Batch: [ 3968/ 4992]\n",
      "training loss: 0.642893  Batch: [ 4000/ 4992]\n",
      "training loss: 0.481924  Batch: [ 4032/ 4992]\n",
      "training loss: 0.397656  Batch: [ 4064/ 4992]\n",
      "training loss: 0.377526  Batch: [ 4096/ 4992]\n",
      "training loss: 0.451852  Batch: [ 4128/ 4992]\n",
      "training loss: 0.352318  Batch: [ 4160/ 4992]\n",
      "training loss: 0.479010  Batch: [ 4192/ 4992]\n",
      "training loss: 0.287648  Batch: [ 4224/ 4992]\n",
      "training loss: 0.482188  Batch: [ 4256/ 4992]\n",
      "training loss: 0.421705  Batch: [ 4288/ 4992]\n",
      "training loss: 0.530452  Batch: [ 4320/ 4992]\n",
      "training loss: 0.532739  Batch: [ 4352/ 4992]\n",
      "training loss: 0.454888  Batch: [ 4384/ 4992]\n",
      "training loss: 0.474443  Batch: [ 4416/ 4992]\n",
      "training loss: 0.395855  Batch: [ 4448/ 4992]\n",
      "training loss: 0.496450  Batch: [ 4480/ 4992]\n",
      "training loss: 0.486027  Batch: [ 4512/ 4992]\n",
      "training loss: 0.440854  Batch: [ 4544/ 4992]\n",
      "training loss: 0.454787  Batch: [ 4576/ 4992]\n",
      "training loss: 0.329036  Batch: [ 4608/ 4992]\n",
      "training loss: 0.681331  Batch: [ 4640/ 4992]\n",
      "training loss: 0.468913  Batch: [ 4672/ 4992]\n",
      "training loss: 0.393742  Batch: [ 4704/ 4992]\n",
      "training loss: 0.360633  Batch: [ 4736/ 4992]\n",
      "training loss: 0.503802  Batch: [ 4768/ 4992]\n",
      "training loss: 0.414079  Batch: [ 4800/ 4992]\n",
      "training loss: 0.459333  Batch: [ 4832/ 4992]\n",
      "training loss: 0.554223  Batch: [ 4864/ 4992]\n",
      "training loss: 0.465958  Batch: [ 4896/ 4992]\n",
      "training loss: 0.487611  Batch: [ 4928/ 4992]\n",
      "training loss: 0.380097  Batch: [ 4960/ 4992]\n",
      "training loss: 0.470244  Batch: [ 4992/ 4992]\n",
      "Epoch 14/50, Training Loss: 15.0618\n",
      "validation loss: 0.447958  Batch: [   32/ 1070]\n",
      "validation loss: 0.469823  Batch: [   64/ 1070]\n",
      "validation loss: 0.460913  Batch: [   96/ 1070]\n",
      "validation loss: 0.627470  Batch: [  128/ 1070]\n",
      "validation loss: 0.427927  Batch: [  160/ 1070]\n",
      "validation loss: 0.350272  Batch: [  192/ 1070]\n",
      "validation loss: 0.379477  Batch: [  224/ 1070]\n",
      "validation loss: 0.371504  Batch: [  256/ 1070]\n",
      "validation loss: 0.496050  Batch: [  288/ 1070]\n",
      "validation loss: 0.322957  Batch: [  320/ 1070]\n",
      "validation loss: 0.485617  Batch: [  352/ 1070]\n",
      "validation loss: 0.354772  Batch: [  384/ 1070]\n",
      "validation loss: 0.377604  Batch: [  416/ 1070]\n",
      "validation loss: 0.353413  Batch: [  448/ 1070]\n",
      "validation loss: 0.448963  Batch: [  480/ 1070]\n",
      "validation loss: 0.365261  Batch: [  512/ 1070]\n",
      "validation loss: 0.566303  Batch: [  544/ 1070]\n",
      "validation loss: 0.500870  Batch: [  576/ 1070]\n",
      "validation loss: 0.454492  Batch: [  608/ 1070]\n",
      "validation loss: 0.424255  Batch: [  640/ 1070]\n",
      "validation loss: 0.400649  Batch: [  672/ 1070]\n",
      "validation loss: 0.728975  Batch: [  704/ 1070]\n",
      "validation loss: 0.442365  Batch: [  736/ 1070]\n",
      "validation loss: 0.399795  Batch: [  768/ 1070]\n",
      "validation loss: 0.528750  Batch: [  800/ 1070]\n",
      "validation loss: 0.603149  Batch: [  832/ 1070]\n",
      "validation loss: 0.452682  Batch: [  864/ 1070]\n",
      "validation loss: 0.484194  Batch: [  896/ 1070]\n",
      "validation loss: 0.564688  Batch: [  928/ 1070]\n",
      "validation loss: 0.418502  Batch: [  960/ 1070]\n",
      "validation loss: 0.459619  Batch: [  992/ 1070]\n",
      "validation loss: 0.557489  Batch: [ 1024/ 1070]\n",
      "validation loss: 0.356829  Batch: [ 1056/ 1070]\n",
      "validation loss: 0.404849  Batch: [ 1070/ 1070]\n",
      "Epoch 14/50, Validation Loss: 0.4564, Validation Accuracy: 78.41%\n",
      "\n",
      "\n",
      "Epoch 15 \n",
      "---------------------\n",
      "training loss: 0.323693  Batch: [   32/ 4992]\n",
      "training loss: 0.392878  Batch: [   64/ 4992]\n",
      "training loss: 0.517208  Batch: [   96/ 4992]\n",
      "training loss: 0.407328  Batch: [  128/ 4992]\n",
      "training loss: 0.503624  Batch: [  160/ 4992]\n",
      "training loss: 0.364151  Batch: [  192/ 4992]\n",
      "training loss: 0.421745  Batch: [  224/ 4992]\n",
      "training loss: 0.622883  Batch: [  256/ 4992]\n",
      "training loss: 0.375550  Batch: [  288/ 4992]\n",
      "training loss: 0.366406  Batch: [  320/ 4992]\n",
      "training loss: 0.606789  Batch: [  352/ 4992]\n",
      "training loss: 0.543755  Batch: [  384/ 4992]\n",
      "training loss: 0.379541  Batch: [  416/ 4992]\n",
      "training loss: 0.443859  Batch: [  448/ 4992]\n",
      "training loss: 0.395535  Batch: [  480/ 4992]\n",
      "training loss: 0.549086  Batch: [  512/ 4992]\n",
      "training loss: 0.634797  Batch: [  544/ 4992]\n",
      "training loss: 0.399112  Batch: [  576/ 4992]\n",
      "training loss: 0.545896  Batch: [  608/ 4992]\n",
      "training loss: 0.350888  Batch: [  640/ 4992]\n",
      "training loss: 0.453670  Batch: [  672/ 4992]\n",
      "training loss: 0.486396  Batch: [  704/ 4992]\n",
      "training loss: 0.326589  Batch: [  736/ 4992]\n",
      "training loss: 0.540486  Batch: [  768/ 4992]\n",
      "training loss: 0.502821  Batch: [  800/ 4992]\n",
      "training loss: 0.549256  Batch: [  832/ 4992]\n",
      "training loss: 0.387913  Batch: [  864/ 4992]\n",
      "training loss: 0.412820  Batch: [  896/ 4992]\n",
      "training loss: 0.470084  Batch: [  928/ 4992]\n",
      "training loss: 0.389231  Batch: [  960/ 4992]\n",
      "training loss: 0.408026  Batch: [  992/ 4992]\n",
      "training loss: 0.341292  Batch: [ 1024/ 4992]\n",
      "training loss: 0.484817  Batch: [ 1056/ 4992]\n",
      "training loss: 0.392639  Batch: [ 1088/ 4992]\n",
      "training loss: 0.534685  Batch: [ 1120/ 4992]\n",
      "training loss: 0.463817  Batch: [ 1152/ 4992]\n",
      "training loss: 0.511571  Batch: [ 1184/ 4992]\n",
      "training loss: 0.548528  Batch: [ 1216/ 4992]\n",
      "training loss: 0.229045  Batch: [ 1248/ 4992]\n",
      "training loss: 0.541826  Batch: [ 1280/ 4992]\n",
      "training loss: 0.414622  Batch: [ 1312/ 4992]\n",
      "training loss: 0.445678  Batch: [ 1344/ 4992]\n",
      "training loss: 0.588633  Batch: [ 1376/ 4992]\n",
      "training loss: 0.575933  Batch: [ 1408/ 4992]\n",
      "training loss: 0.463786  Batch: [ 1440/ 4992]\n",
      "training loss: 0.549681  Batch: [ 1472/ 4992]\n",
      "training loss: 0.439909  Batch: [ 1504/ 4992]\n",
      "training loss: 0.457972  Batch: [ 1536/ 4992]\n",
      "training loss: 0.436016  Batch: [ 1568/ 4992]\n",
      "training loss: 0.346341  Batch: [ 1600/ 4992]\n",
      "training loss: 0.392693  Batch: [ 1632/ 4992]\n",
      "training loss: 0.470728  Batch: [ 1664/ 4992]\n",
      "training loss: 0.449987  Batch: [ 1696/ 4992]\n",
      "training loss: 0.468287  Batch: [ 1728/ 4992]\n",
      "training loss: 0.305652  Batch: [ 1760/ 4992]\n",
      "training loss: 0.466907  Batch: [ 1792/ 4992]\n",
      "training loss: 0.345368  Batch: [ 1824/ 4992]\n",
      "training loss: 0.358410  Batch: [ 1856/ 4992]\n",
      "training loss: 0.388475  Batch: [ 1888/ 4992]\n",
      "training loss: 0.445368  Batch: [ 1920/ 4992]\n",
      "training loss: 0.434895  Batch: [ 1952/ 4992]\n",
      "training loss: 0.427474  Batch: [ 1984/ 4992]\n",
      "training loss: 0.335083  Batch: [ 2016/ 4992]\n",
      "training loss: 0.573681  Batch: [ 2048/ 4992]\n",
      "training loss: 0.633582  Batch: [ 2080/ 4992]\n",
      "training loss: 0.705550  Batch: [ 2112/ 4992]\n",
      "training loss: 0.356881  Batch: [ 2144/ 4992]\n",
      "training loss: 0.272558  Batch: [ 2176/ 4992]\n",
      "training loss: 0.524870  Batch: [ 2208/ 4992]\n",
      "training loss: 0.409568  Batch: [ 2240/ 4992]\n",
      "training loss: 0.380991  Batch: [ 2272/ 4992]\n",
      "training loss: 0.580403  Batch: [ 2304/ 4992]\n",
      "training loss: 0.517768  Batch: [ 2336/ 4992]\n",
      "training loss: 0.498786  Batch: [ 2368/ 4992]\n",
      "training loss: 0.488329  Batch: [ 2400/ 4992]\n",
      "training loss: 0.483712  Batch: [ 2432/ 4992]\n",
      "training loss: 0.397027  Batch: [ 2464/ 4992]\n",
      "training loss: 0.607006  Batch: [ 2496/ 4992]\n",
      "training loss: 0.489382  Batch: [ 2528/ 4992]\n",
      "training loss: 0.555030  Batch: [ 2560/ 4992]\n",
      "training loss: 0.362386  Batch: [ 2592/ 4992]\n",
      "training loss: 0.339389  Batch: [ 2624/ 4992]\n",
      "training loss: 0.413493  Batch: [ 2656/ 4992]\n",
      "training loss: 0.626379  Batch: [ 2688/ 4992]\n",
      "training loss: 0.391447  Batch: [ 2720/ 4992]\n",
      "training loss: 0.370457  Batch: [ 2752/ 4992]\n",
      "training loss: 0.421458  Batch: [ 2784/ 4992]\n",
      "training loss: 0.426611  Batch: [ 2816/ 4992]\n",
      "training loss: 0.486028  Batch: [ 2848/ 4992]\n",
      "training loss: 0.452668  Batch: [ 2880/ 4992]\n",
      "training loss: 0.393602  Batch: [ 2912/ 4992]\n",
      "training loss: 0.497008  Batch: [ 2944/ 4992]\n",
      "training loss: 0.326608  Batch: [ 2976/ 4992]\n",
      "training loss: 0.524671  Batch: [ 3008/ 4992]\n",
      "training loss: 0.333784  Batch: [ 3040/ 4992]\n",
      "training loss: 0.463142  Batch: [ 3072/ 4992]\n",
      "training loss: 0.369787  Batch: [ 3104/ 4992]\n",
      "training loss: 0.276724  Batch: [ 3136/ 4992]\n",
      "training loss: 0.568549  Batch: [ 3168/ 4992]\n",
      "training loss: 0.443931  Batch: [ 3200/ 4992]\n",
      "training loss: 0.466624  Batch: [ 3232/ 4992]\n",
      "training loss: 0.426834  Batch: [ 3264/ 4992]\n",
      "training loss: 0.509084  Batch: [ 3296/ 4992]\n",
      "training loss: 0.305708  Batch: [ 3328/ 4992]\n",
      "training loss: 0.428611  Batch: [ 3360/ 4992]\n",
      "training loss: 0.302446  Batch: [ 3392/ 4992]\n",
      "training loss: 0.402355  Batch: [ 3424/ 4992]\n",
      "training loss: 0.385377  Batch: [ 3456/ 4992]\n",
      "training loss: 0.444003  Batch: [ 3488/ 4992]\n",
      "training loss: 0.322981  Batch: [ 3520/ 4992]\n",
      "training loss: 0.553470  Batch: [ 3552/ 4992]\n",
      "training loss: 0.555108  Batch: [ 3584/ 4992]\n",
      "training loss: 0.517103  Batch: [ 3616/ 4992]\n",
      "training loss: 0.432348  Batch: [ 3648/ 4992]\n",
      "training loss: 0.475571  Batch: [ 3680/ 4992]\n",
      "training loss: 0.464278  Batch: [ 3712/ 4992]\n",
      "training loss: 0.319821  Batch: [ 3744/ 4992]\n",
      "training loss: 0.515357  Batch: [ 3776/ 4992]\n",
      "training loss: 0.468742  Batch: [ 3808/ 4992]\n",
      "training loss: 0.500538  Batch: [ 3840/ 4992]\n",
      "training loss: 0.558040  Batch: [ 3872/ 4992]\n",
      "training loss: 0.381062  Batch: [ 3904/ 4992]\n",
      "training loss: 0.376316  Batch: [ 3936/ 4992]\n",
      "training loss: 0.498526  Batch: [ 3968/ 4992]\n",
      "training loss: 0.489122  Batch: [ 4000/ 4992]\n",
      "training loss: 0.448597  Batch: [ 4032/ 4992]\n",
      "training loss: 0.331257  Batch: [ 4064/ 4992]\n",
      "training loss: 0.377766  Batch: [ 4096/ 4992]\n",
      "training loss: 0.384997  Batch: [ 4128/ 4992]\n",
      "training loss: 0.429245  Batch: [ 4160/ 4992]\n",
      "training loss: 0.372099  Batch: [ 4192/ 4992]\n",
      "training loss: 0.448012  Batch: [ 4224/ 4992]\n",
      "training loss: 0.406056  Batch: [ 4256/ 4992]\n",
      "training loss: 0.403987  Batch: [ 4288/ 4992]\n",
      "training loss: 0.515136  Batch: [ 4320/ 4992]\n",
      "training loss: 0.359748  Batch: [ 4352/ 4992]\n",
      "training loss: 0.492947  Batch: [ 4384/ 4992]\n",
      "training loss: 0.406152  Batch: [ 4416/ 4992]\n",
      "training loss: 0.602534  Batch: [ 4448/ 4992]\n",
      "training loss: 0.333538  Batch: [ 4480/ 4992]\n",
      "training loss: 0.449325  Batch: [ 4512/ 4992]\n",
      "training loss: 0.509477  Batch: [ 4544/ 4992]\n",
      "training loss: 0.476397  Batch: [ 4576/ 4992]\n",
      "training loss: 0.261662  Batch: [ 4608/ 4992]\n",
      "training loss: 0.365425  Batch: [ 4640/ 4992]\n",
      "training loss: 0.376424  Batch: [ 4672/ 4992]\n",
      "training loss: 0.665166  Batch: [ 4704/ 4992]\n",
      "training loss: 0.503505  Batch: [ 4736/ 4992]\n",
      "training loss: 0.444465  Batch: [ 4768/ 4992]\n",
      "training loss: 0.374870  Batch: [ 4800/ 4992]\n",
      "training loss: 0.424608  Batch: [ 4832/ 4992]\n",
      "training loss: 0.376431  Batch: [ 4864/ 4992]\n",
      "training loss: 0.404833  Batch: [ 4896/ 4992]\n",
      "training loss: 0.527292  Batch: [ 4928/ 4992]\n",
      "training loss: 0.421287  Batch: [ 4960/ 4992]\n",
      "training loss: 0.410148  Batch: [ 4992/ 4992]\n",
      "Epoch 15/50, Training Loss: 14.2552\n",
      "validation loss: 0.630056  Batch: [   32/ 1070]\n",
      "validation loss: 0.399140  Batch: [   64/ 1070]\n",
      "validation loss: 0.342969  Batch: [   96/ 1070]\n",
      "validation loss: 0.381610  Batch: [  128/ 1070]\n",
      "validation loss: 0.266874  Batch: [  160/ 1070]\n",
      "validation loss: 0.391693  Batch: [  192/ 1070]\n",
      "validation loss: 0.461613  Batch: [  224/ 1070]\n",
      "validation loss: 0.385277  Batch: [  256/ 1070]\n",
      "validation loss: 0.423403  Batch: [  288/ 1070]\n",
      "validation loss: 0.464554  Batch: [  320/ 1070]\n",
      "validation loss: 0.424374  Batch: [  352/ 1070]\n",
      "validation loss: 0.504938  Batch: [  384/ 1070]\n",
      "validation loss: 0.596959  Batch: [  416/ 1070]\n",
      "validation loss: 0.493239  Batch: [  448/ 1070]\n",
      "validation loss: 0.590344  Batch: [  480/ 1070]\n",
      "validation loss: 0.435918  Batch: [  512/ 1070]\n",
      "validation loss: 0.285915  Batch: [  544/ 1070]\n",
      "validation loss: 0.512611  Batch: [  576/ 1070]\n",
      "validation loss: 0.336970  Batch: [  608/ 1070]\n",
      "validation loss: 0.431965  Batch: [  640/ 1070]\n",
      "validation loss: 0.414200  Batch: [  672/ 1070]\n",
      "validation loss: 0.420053  Batch: [  704/ 1070]\n",
      "validation loss: 0.408766  Batch: [  736/ 1070]\n",
      "validation loss: 0.254507  Batch: [  768/ 1070]\n",
      "validation loss: 0.433721  Batch: [  800/ 1070]\n",
      "validation loss: 0.455326  Batch: [  832/ 1070]\n",
      "validation loss: 0.425677  Batch: [  864/ 1070]\n",
      "validation loss: 0.440907  Batch: [  896/ 1070]\n",
      "validation loss: 0.330289  Batch: [  928/ 1070]\n",
      "validation loss: 0.489933  Batch: [  960/ 1070]\n",
      "validation loss: 0.273737  Batch: [  992/ 1070]\n",
      "validation loss: 0.449973  Batch: [ 1024/ 1070]\n",
      "validation loss: 0.475824  Batch: [ 1056/ 1070]\n",
      "validation loss: 0.486582  Batch: [ 1070/ 1070]\n",
      "Epoch 15/50, Validation Loss: 0.4261, Validation Accuracy: 80.00%\n",
      "\n",
      "\n",
      "Epoch 16 \n",
      "---------------------\n",
      "training loss: 0.405867  Batch: [   32/ 4992]\n",
      "training loss: 0.336730  Batch: [   64/ 4992]\n",
      "training loss: 0.601268  Batch: [   96/ 4992]\n",
      "training loss: 0.431704  Batch: [  128/ 4992]\n",
      "training loss: 0.336009  Batch: [  160/ 4992]\n",
      "training loss: 0.526108  Batch: [  192/ 4992]\n",
      "training loss: 0.443022  Batch: [  224/ 4992]\n",
      "training loss: 0.450633  Batch: [  256/ 4992]\n",
      "training loss: 0.435194  Batch: [  288/ 4992]\n",
      "training loss: 0.406782  Batch: [  320/ 4992]\n",
      "training loss: 0.668872  Batch: [  352/ 4992]\n",
      "training loss: 0.653695  Batch: [  384/ 4992]\n",
      "training loss: 0.618656  Batch: [  416/ 4992]\n",
      "training loss: 0.411237  Batch: [  448/ 4992]\n",
      "training loss: 0.509663  Batch: [  480/ 4992]\n",
      "training loss: 0.362403  Batch: [  512/ 4992]\n",
      "training loss: 0.358680  Batch: [  544/ 4992]\n",
      "training loss: 0.419172  Batch: [  576/ 4992]\n",
      "training loss: 0.542188  Batch: [  608/ 4992]\n",
      "training loss: 0.540286  Batch: [  640/ 4992]\n",
      "training loss: 0.546160  Batch: [  672/ 4992]\n",
      "training loss: 0.472434  Batch: [  704/ 4992]\n",
      "training loss: 0.373630  Batch: [  736/ 4992]\n",
      "training loss: 0.358099  Batch: [  768/ 4992]\n",
      "training loss: 0.543202  Batch: [  800/ 4992]\n",
      "training loss: 0.497639  Batch: [  832/ 4992]\n",
      "training loss: 0.409110  Batch: [  864/ 4992]\n",
      "training loss: 0.519283  Batch: [  896/ 4992]\n",
      "training loss: 0.393971  Batch: [  928/ 4992]\n",
      "training loss: 0.525878  Batch: [  960/ 4992]\n",
      "training loss: 0.434079  Batch: [  992/ 4992]\n",
      "training loss: 0.446987  Batch: [ 1024/ 4992]\n",
      "training loss: 0.422474  Batch: [ 1056/ 4992]\n",
      "training loss: 0.386169  Batch: [ 1088/ 4992]\n",
      "training loss: 0.419270  Batch: [ 1120/ 4992]\n",
      "training loss: 0.404055  Batch: [ 1152/ 4992]\n",
      "training loss: 0.481911  Batch: [ 1184/ 4992]\n",
      "training loss: 0.312656  Batch: [ 1216/ 4992]\n",
      "training loss: 0.522615  Batch: [ 1248/ 4992]\n",
      "training loss: 0.564138  Batch: [ 1280/ 4992]\n",
      "training loss: 0.366382  Batch: [ 1312/ 4992]\n",
      "training loss: 0.567747  Batch: [ 1344/ 4992]\n",
      "training loss: 0.461539  Batch: [ 1376/ 4992]\n",
      "training loss: 0.478275  Batch: [ 1408/ 4992]\n",
      "training loss: 0.289943  Batch: [ 1440/ 4992]\n",
      "training loss: 0.337062  Batch: [ 1472/ 4992]\n",
      "training loss: 0.548897  Batch: [ 1504/ 4992]\n",
      "training loss: 0.409119  Batch: [ 1536/ 4992]\n",
      "training loss: 0.340918  Batch: [ 1568/ 4992]\n",
      "training loss: 0.417305  Batch: [ 1600/ 4992]\n",
      "training loss: 0.389186  Batch: [ 1632/ 4992]\n",
      "training loss: 0.325476  Batch: [ 1664/ 4992]\n",
      "training loss: 0.437925  Batch: [ 1696/ 4992]\n",
      "training loss: 0.403501  Batch: [ 1728/ 4992]\n",
      "training loss: 0.408717  Batch: [ 1760/ 4992]\n",
      "training loss: 0.394755  Batch: [ 1792/ 4992]\n",
      "training loss: 0.426484  Batch: [ 1824/ 4992]\n",
      "training loss: 0.448388  Batch: [ 1856/ 4992]\n",
      "training loss: 0.511759  Batch: [ 1888/ 4992]\n",
      "training loss: 0.419321  Batch: [ 1920/ 4992]\n",
      "training loss: 0.401087  Batch: [ 1952/ 4992]\n",
      "training loss: 0.541128  Batch: [ 1984/ 4992]\n",
      "training loss: 0.490183  Batch: [ 2016/ 4992]\n",
      "training loss: 0.652456  Batch: [ 2048/ 4992]\n",
      "training loss: 0.344388  Batch: [ 2080/ 4992]\n",
      "training loss: 0.374609  Batch: [ 2112/ 4992]\n",
      "training loss: 0.430268  Batch: [ 2144/ 4992]\n",
      "training loss: 0.443696  Batch: [ 2176/ 4992]\n",
      "training loss: 0.418691  Batch: [ 2208/ 4992]\n",
      "training loss: 0.556827  Batch: [ 2240/ 4992]\n",
      "training loss: 0.521049  Batch: [ 2272/ 4992]\n",
      "training loss: 0.460635  Batch: [ 2304/ 4992]\n",
      "training loss: 0.456480  Batch: [ 2336/ 4992]\n",
      "training loss: 0.487422  Batch: [ 2368/ 4992]\n",
      "training loss: 0.328212  Batch: [ 2400/ 4992]\n",
      "training loss: 0.417940  Batch: [ 2432/ 4992]\n",
      "training loss: 0.445246  Batch: [ 2464/ 4992]\n",
      "training loss: 0.667757  Batch: [ 2496/ 4992]\n",
      "training loss: 0.192526  Batch: [ 2528/ 4992]\n",
      "training loss: 0.476215  Batch: [ 2560/ 4992]\n",
      "training loss: 0.575804  Batch: [ 2592/ 4992]\n",
      "training loss: 0.563859  Batch: [ 2624/ 4992]\n",
      "training loss: 0.615722  Batch: [ 2656/ 4992]\n",
      "training loss: 0.525621  Batch: [ 2688/ 4992]\n",
      "training loss: 0.364608  Batch: [ 2720/ 4992]\n",
      "training loss: 0.525107  Batch: [ 2752/ 4992]\n",
      "training loss: 0.290884  Batch: [ 2784/ 4992]\n",
      "training loss: 0.530139  Batch: [ 2816/ 4992]\n",
      "training loss: 0.474573  Batch: [ 2848/ 4992]\n",
      "training loss: 0.578788  Batch: [ 2880/ 4992]\n",
      "training loss: 0.453360  Batch: [ 2912/ 4992]\n",
      "training loss: 0.435864  Batch: [ 2944/ 4992]\n",
      "training loss: 0.622615  Batch: [ 2976/ 4992]\n",
      "training loss: 0.536932  Batch: [ 3008/ 4992]\n",
      "training loss: 0.585728  Batch: [ 3040/ 4992]\n",
      "training loss: 0.402860  Batch: [ 3072/ 4992]\n",
      "training loss: 0.401912  Batch: [ 3104/ 4992]\n",
      "training loss: 0.424213  Batch: [ 3136/ 4992]\n",
      "training loss: 0.574184  Batch: [ 3168/ 4992]\n",
      "training loss: 0.683628  Batch: [ 3200/ 4992]\n",
      "training loss: 0.478691  Batch: [ 3232/ 4992]\n",
      "training loss: 0.381019  Batch: [ 3264/ 4992]\n",
      "training loss: 0.422191  Batch: [ 3296/ 4992]\n",
      "training loss: 0.472498  Batch: [ 3328/ 4992]\n",
      "training loss: 0.568554  Batch: [ 3360/ 4992]\n",
      "training loss: 0.500758  Batch: [ 3392/ 4992]\n",
      "training loss: 0.343434  Batch: [ 3424/ 4992]\n",
      "training loss: 0.525217  Batch: [ 3456/ 4992]\n",
      "training loss: 0.543019  Batch: [ 3488/ 4992]\n",
      "training loss: 0.299759  Batch: [ 3520/ 4992]\n",
      "training loss: 0.472710  Batch: [ 3552/ 4992]\n",
      "training loss: 0.574495  Batch: [ 3584/ 4992]\n",
      "training loss: 0.410004  Batch: [ 3616/ 4992]\n",
      "training loss: 0.551479  Batch: [ 3648/ 4992]\n",
      "training loss: 0.500440  Batch: [ 3680/ 4992]\n",
      "training loss: 0.583868  Batch: [ 3712/ 4992]\n",
      "training loss: 0.553232  Batch: [ 3744/ 4992]\n",
      "training loss: 0.493311  Batch: [ 3776/ 4992]\n",
      "training loss: 0.436905  Batch: [ 3808/ 4992]\n",
      "training loss: 0.398389  Batch: [ 3840/ 4992]\n",
      "training loss: 0.380309  Batch: [ 3872/ 4992]\n",
      "training loss: 0.361275  Batch: [ 3904/ 4992]\n",
      "training loss: 0.511868  Batch: [ 3936/ 4992]\n",
      "training loss: 0.610118  Batch: [ 3968/ 4992]\n",
      "training loss: 0.633867  Batch: [ 4000/ 4992]\n",
      "training loss: 0.417182  Batch: [ 4032/ 4992]\n",
      "training loss: 0.366244  Batch: [ 4064/ 4992]\n",
      "training loss: 0.303061  Batch: [ 4096/ 4992]\n",
      "training loss: 0.409103  Batch: [ 4128/ 4992]\n",
      "training loss: 0.426858  Batch: [ 4160/ 4992]\n",
      "training loss: 0.530315  Batch: [ 4192/ 4992]\n",
      "training loss: 0.529655  Batch: [ 4224/ 4992]\n",
      "training loss: 0.322168  Batch: [ 4256/ 4992]\n",
      "training loss: 0.414919  Batch: [ 4288/ 4992]\n",
      "training loss: 0.538836  Batch: [ 4320/ 4992]\n",
      "training loss: 0.460373  Batch: [ 4352/ 4992]\n",
      "training loss: 0.440624  Batch: [ 4384/ 4992]\n",
      "training loss: 0.423444  Batch: [ 4416/ 4992]\n",
      "training loss: 0.626260  Batch: [ 4448/ 4992]\n",
      "training loss: 0.469319  Batch: [ 4480/ 4992]\n",
      "training loss: 0.486207  Batch: [ 4512/ 4992]\n",
      "training loss: 0.586352  Batch: [ 4544/ 4992]\n",
      "training loss: 0.389086  Batch: [ 4576/ 4992]\n",
      "training loss: 0.444459  Batch: [ 4608/ 4992]\n",
      "training loss: 0.512153  Batch: [ 4640/ 4992]\n",
      "training loss: 0.481412  Batch: [ 4672/ 4992]\n",
      "training loss: 0.676354  Batch: [ 4704/ 4992]\n",
      "training loss: 0.218298  Batch: [ 4736/ 4992]\n",
      "training loss: 0.519171  Batch: [ 4768/ 4992]\n",
      "training loss: 0.356245  Batch: [ 4800/ 4992]\n",
      "training loss: 0.515302  Batch: [ 4832/ 4992]\n",
      "training loss: 0.391779  Batch: [ 4864/ 4992]\n",
      "training loss: 0.428456  Batch: [ 4896/ 4992]\n",
      "training loss: 0.553713  Batch: [ 4928/ 4992]\n",
      "training loss: 0.448301  Batch: [ 4960/ 4992]\n",
      "training loss: 0.539069  Batch: [ 4992/ 4992]\n",
      "Epoch 16/50, Training Loss: 14.8472\n",
      "validation loss: 0.422557  Batch: [   32/ 1070]\n",
      "validation loss: 0.471904  Batch: [   64/ 1070]\n",
      "validation loss: 0.397518  Batch: [   96/ 1070]\n",
      "validation loss: 0.538922  Batch: [  128/ 1070]\n",
      "validation loss: 0.400575  Batch: [  160/ 1070]\n",
      "validation loss: 0.417172  Batch: [  192/ 1070]\n",
      "validation loss: 0.444756  Batch: [  224/ 1070]\n",
      "validation loss: 0.515719  Batch: [  256/ 1070]\n",
      "validation loss: 0.476551  Batch: [  288/ 1070]\n",
      "validation loss: 0.430691  Batch: [  320/ 1070]\n",
      "validation loss: 0.465608  Batch: [  352/ 1070]\n",
      "validation loss: 0.520743  Batch: [  384/ 1070]\n",
      "validation loss: 0.432561  Batch: [  416/ 1070]\n",
      "validation loss: 0.491819  Batch: [  448/ 1070]\n",
      "validation loss: 0.487219  Batch: [  480/ 1070]\n",
      "validation loss: 0.521701  Batch: [  512/ 1070]\n",
      "validation loss: 0.541851  Batch: [  544/ 1070]\n",
      "validation loss: 0.514363  Batch: [  576/ 1070]\n",
      "validation loss: 0.332813  Batch: [  608/ 1070]\n",
      "validation loss: 0.441442  Batch: [  640/ 1070]\n",
      "validation loss: 0.570656  Batch: [  672/ 1070]\n",
      "validation loss: 0.388999  Batch: [  704/ 1070]\n",
      "validation loss: 0.596367  Batch: [  736/ 1070]\n",
      "validation loss: 0.509355  Batch: [  768/ 1070]\n",
      "validation loss: 0.497549  Batch: [  800/ 1070]\n",
      "validation loss: 0.408934  Batch: [  832/ 1070]\n",
      "validation loss: 0.456306  Batch: [  864/ 1070]\n",
      "validation loss: 0.532267  Batch: [  896/ 1070]\n",
      "validation loss: 0.543145  Batch: [  928/ 1070]\n",
      "validation loss: 0.522349  Batch: [  960/ 1070]\n",
      "validation loss: 0.464931  Batch: [  992/ 1070]\n",
      "validation loss: 0.348477  Batch: [ 1024/ 1070]\n",
      "validation loss: 0.465820  Batch: [ 1056/ 1070]\n",
      "validation loss: 0.561571  Batch: [ 1070/ 1070]\n",
      "Epoch 16/50, Validation Loss: 0.4730, Validation Accuracy: 76.82%\n",
      "\n",
      "\n",
      "Epoch 17 \n",
      "---------------------\n",
      "training loss: 0.494278  Batch: [   32/ 4992]\n",
      "training loss: 0.592804  Batch: [   64/ 4992]\n",
      "training loss: 0.594467  Batch: [   96/ 4992]\n",
      "training loss: 0.394887  Batch: [  128/ 4992]\n",
      "training loss: 0.448461  Batch: [  160/ 4992]\n",
      "training loss: 0.675721  Batch: [  192/ 4992]\n",
      "training loss: 0.667568  Batch: [  224/ 4992]\n",
      "training loss: 0.446948  Batch: [  256/ 4992]\n",
      "training loss: 0.361843  Batch: [  288/ 4992]\n",
      "training loss: 0.372133  Batch: [  320/ 4992]\n",
      "training loss: 0.446989  Batch: [  352/ 4992]\n",
      "training loss: 0.546443  Batch: [  384/ 4992]\n",
      "training loss: 0.402526  Batch: [  416/ 4992]\n",
      "training loss: 0.421787  Batch: [  448/ 4992]\n",
      "training loss: 0.403063  Batch: [  480/ 4992]\n",
      "training loss: 0.464839  Batch: [  512/ 4992]\n",
      "training loss: 0.582123  Batch: [  544/ 4992]\n",
      "training loss: 0.470999  Batch: [  576/ 4992]\n",
      "training loss: 0.456187  Batch: [  608/ 4992]\n",
      "training loss: 0.413871  Batch: [  640/ 4992]\n",
      "training loss: 0.482044  Batch: [  672/ 4992]\n",
      "training loss: 0.506302  Batch: [  704/ 4992]\n",
      "training loss: 0.471459  Batch: [  736/ 4992]\n",
      "training loss: 0.354043  Batch: [  768/ 4992]\n",
      "training loss: 0.450641  Batch: [  800/ 4992]\n",
      "training loss: 0.385487  Batch: [  832/ 4992]\n",
      "training loss: 0.472541  Batch: [  864/ 4992]\n",
      "training loss: 0.509709  Batch: [  896/ 4992]\n",
      "training loss: 0.471125  Batch: [  928/ 4992]\n",
      "training loss: 0.527266  Batch: [  960/ 4992]\n",
      "training loss: 0.389637  Batch: [  992/ 4992]\n",
      "training loss: 0.560256  Batch: [ 1024/ 4992]\n",
      "training loss: 0.517484  Batch: [ 1056/ 4992]\n",
      "training loss: 0.576692  Batch: [ 1088/ 4992]\n",
      "training loss: 0.426315  Batch: [ 1120/ 4992]\n",
      "training loss: 0.596773  Batch: [ 1152/ 4992]\n",
      "training loss: 0.407851  Batch: [ 1184/ 4992]\n",
      "training loss: 0.445240  Batch: [ 1216/ 4992]\n",
      "training loss: 0.503979  Batch: [ 1248/ 4992]\n",
      "training loss: 0.484231  Batch: [ 1280/ 4992]\n",
      "training loss: 0.453340  Batch: [ 1312/ 4992]\n",
      "training loss: 0.514478  Batch: [ 1344/ 4992]\n",
      "training loss: 0.339463  Batch: [ 1376/ 4992]\n",
      "training loss: 0.398869  Batch: [ 1408/ 4992]\n",
      "training loss: 0.333108  Batch: [ 1440/ 4992]\n",
      "training loss: 0.578187  Batch: [ 1472/ 4992]\n",
      "training loss: 0.302456  Batch: [ 1504/ 4992]\n",
      "training loss: 0.471294  Batch: [ 1536/ 4992]\n",
      "training loss: 0.494055  Batch: [ 1568/ 4992]\n",
      "training loss: 0.433789  Batch: [ 1600/ 4992]\n",
      "training loss: 0.458096  Batch: [ 1632/ 4992]\n",
      "training loss: 0.436612  Batch: [ 1664/ 4992]\n",
      "training loss: 0.524258  Batch: [ 1696/ 4992]\n",
      "training loss: 0.520468  Batch: [ 1728/ 4992]\n",
      "training loss: 0.576038  Batch: [ 1760/ 4992]\n",
      "training loss: 0.507792  Batch: [ 1792/ 4992]\n",
      "training loss: 0.446449  Batch: [ 1824/ 4992]\n",
      "training loss: 0.355866  Batch: [ 1856/ 4992]\n",
      "training loss: 0.369006  Batch: [ 1888/ 4992]\n",
      "training loss: 0.359994  Batch: [ 1920/ 4992]\n",
      "training loss: 0.589198  Batch: [ 1952/ 4992]\n",
      "training loss: 0.476119  Batch: [ 1984/ 4992]\n",
      "training loss: 0.543264  Batch: [ 2016/ 4992]\n",
      "training loss: 0.463716  Batch: [ 2048/ 4992]\n",
      "training loss: 0.399567  Batch: [ 2080/ 4992]\n",
      "training loss: 0.440972  Batch: [ 2112/ 4992]\n",
      "training loss: 0.407252  Batch: [ 2144/ 4992]\n",
      "training loss: 0.302972  Batch: [ 2176/ 4992]\n",
      "training loss: 0.441769  Batch: [ 2208/ 4992]\n",
      "training loss: 0.665000  Batch: [ 2240/ 4992]\n",
      "training loss: 0.699189  Batch: [ 2272/ 4992]\n",
      "training loss: 0.499843  Batch: [ 2304/ 4992]\n",
      "training loss: 0.376214  Batch: [ 2336/ 4992]\n",
      "training loss: 0.594409  Batch: [ 2368/ 4992]\n",
      "training loss: 0.523842  Batch: [ 2400/ 4992]\n",
      "training loss: 0.714216  Batch: [ 2432/ 4992]\n",
      "training loss: 0.599845  Batch: [ 2464/ 4992]\n",
      "training loss: 0.648723  Batch: [ 2496/ 4992]\n",
      "training loss: 0.634035  Batch: [ 2528/ 4992]\n",
      "training loss: 0.526370  Batch: [ 2560/ 4992]\n",
      "training loss: 0.515076  Batch: [ 2592/ 4992]\n",
      "training loss: 0.599384  Batch: [ 2624/ 4992]\n",
      "training loss: 0.482040  Batch: [ 2656/ 4992]\n",
      "training loss: 0.441110  Batch: [ 2688/ 4992]\n",
      "training loss: 0.470019  Batch: [ 2720/ 4992]\n",
      "training loss: 0.556753  Batch: [ 2752/ 4992]\n",
      "training loss: 0.467959  Batch: [ 2784/ 4992]\n",
      "training loss: 0.562557  Batch: [ 2816/ 4992]\n",
      "training loss: 0.432539  Batch: [ 2848/ 4992]\n",
      "training loss: 0.458109  Batch: [ 2880/ 4992]\n",
      "training loss: 0.405772  Batch: [ 2912/ 4992]\n",
      "training loss: 0.435531  Batch: [ 2944/ 4992]\n",
      "training loss: 0.451270  Batch: [ 2976/ 4992]\n",
      "training loss: 0.455115  Batch: [ 3008/ 4992]\n",
      "training loss: 0.406107  Batch: [ 3040/ 4992]\n",
      "training loss: 0.380939  Batch: [ 3072/ 4992]\n",
      "training loss: 0.628403  Batch: [ 3104/ 4992]\n",
      "training loss: 0.421353  Batch: [ 3136/ 4992]\n",
      "training loss: 0.367000  Batch: [ 3168/ 4992]\n",
      "training loss: 0.517722  Batch: [ 3200/ 4992]\n",
      "training loss: 0.591988  Batch: [ 3232/ 4992]\n",
      "training loss: 0.403431  Batch: [ 3264/ 4992]\n",
      "training loss: 0.302267  Batch: [ 3296/ 4992]\n",
      "training loss: 0.312239  Batch: [ 3328/ 4992]\n",
      "training loss: 0.510605  Batch: [ 3360/ 4992]\n",
      "training loss: 0.421035  Batch: [ 3392/ 4992]\n",
      "training loss: 0.451907  Batch: [ 3424/ 4992]\n",
      "training loss: 0.537100  Batch: [ 3456/ 4992]\n",
      "training loss: 0.424878  Batch: [ 3488/ 4992]\n",
      "training loss: 0.529674  Batch: [ 3520/ 4992]\n",
      "training loss: 0.369790  Batch: [ 3552/ 4992]\n",
      "training loss: 0.405445  Batch: [ 3584/ 4992]\n",
      "training loss: 0.383407  Batch: [ 3616/ 4992]\n",
      "training loss: 0.390117  Batch: [ 3648/ 4992]\n",
      "training loss: 0.379943  Batch: [ 3680/ 4992]\n",
      "training loss: 0.417523  Batch: [ 3712/ 4992]\n",
      "training loss: 0.532210  Batch: [ 3744/ 4992]\n",
      "training loss: 0.454724  Batch: [ 3776/ 4992]\n",
      "training loss: 0.462349  Batch: [ 3808/ 4992]\n",
      "training loss: 0.533550  Batch: [ 3840/ 4992]\n",
      "training loss: 0.301127  Batch: [ 3872/ 4992]\n",
      "training loss: 0.414381  Batch: [ 3904/ 4992]\n",
      "training loss: 0.595457  Batch: [ 3936/ 4992]\n",
      "training loss: 0.350507  Batch: [ 3968/ 4992]\n",
      "training loss: 0.580717  Batch: [ 4000/ 4992]\n",
      "training loss: 0.555311  Batch: [ 4032/ 4992]\n",
      "training loss: 0.341221  Batch: [ 4064/ 4992]\n",
      "training loss: 0.372258  Batch: [ 4096/ 4992]\n",
      "training loss: 0.591113  Batch: [ 4128/ 4992]\n",
      "training loss: 0.651313  Batch: [ 4160/ 4992]\n",
      "training loss: 0.429538  Batch: [ 4192/ 4992]\n",
      "training loss: 0.472474  Batch: [ 4224/ 4992]\n",
      "training loss: 0.524162  Batch: [ 4256/ 4992]\n",
      "training loss: 0.468899  Batch: [ 4288/ 4992]\n",
      "training loss: 0.409378  Batch: [ 4320/ 4992]\n",
      "training loss: 0.415014  Batch: [ 4352/ 4992]\n",
      "training loss: 0.551901  Batch: [ 4384/ 4992]\n",
      "training loss: 0.354421  Batch: [ 4416/ 4992]\n",
      "training loss: 0.513394  Batch: [ 4448/ 4992]\n",
      "training loss: 0.384900  Batch: [ 4480/ 4992]\n",
      "training loss: 0.419150  Batch: [ 4512/ 4992]\n",
      "training loss: 0.473292  Batch: [ 4544/ 4992]\n",
      "training loss: 0.551344  Batch: [ 4576/ 4992]\n",
      "training loss: 0.482451  Batch: [ 4608/ 4992]\n",
      "training loss: 0.591258  Batch: [ 4640/ 4992]\n",
      "training loss: 0.486342  Batch: [ 4672/ 4992]\n",
      "training loss: 0.471701  Batch: [ 4704/ 4992]\n",
      "training loss: 0.479710  Batch: [ 4736/ 4992]\n",
      "training loss: 0.484952  Batch: [ 4768/ 4992]\n",
      "training loss: 0.566493  Batch: [ 4800/ 4992]\n",
      "training loss: 0.394175  Batch: [ 4832/ 4992]\n",
      "training loss: 0.470463  Batch: [ 4864/ 4992]\n",
      "training loss: 0.478707  Batch: [ 4896/ 4992]\n",
      "training loss: 0.538444  Batch: [ 4928/ 4992]\n",
      "training loss: 0.398660  Batch: [ 4960/ 4992]\n",
      "training loss: 0.494496  Batch: [ 4992/ 4992]\n",
      "Epoch 17/50, Training Loss: 15.1682\n",
      "validation loss: 0.528953  Batch: [   32/ 1070]\n",
      "validation loss: 0.394202  Batch: [   64/ 1070]\n",
      "validation loss: 0.669706  Batch: [   96/ 1070]\n",
      "validation loss: 0.475877  Batch: [  128/ 1070]\n",
      "validation loss: 0.552606  Batch: [  160/ 1070]\n",
      "validation loss: 0.506871  Batch: [  192/ 1070]\n",
      "validation loss: 0.660263  Batch: [  224/ 1070]\n",
      "validation loss: 0.501974  Batch: [  256/ 1070]\n",
      "validation loss: 0.598767  Batch: [  288/ 1070]\n",
      "validation loss: 0.525543  Batch: [  320/ 1070]\n",
      "validation loss: 0.475594  Batch: [  352/ 1070]\n",
      "validation loss: 0.463883  Batch: [  384/ 1070]\n",
      "validation loss: 0.438309  Batch: [  416/ 1070]\n",
      "validation loss: 0.471226  Batch: [  448/ 1070]\n",
      "validation loss: 0.510600  Batch: [  480/ 1070]\n",
      "validation loss: 0.460058  Batch: [  512/ 1070]\n",
      "validation loss: 0.349916  Batch: [  544/ 1070]\n",
      "validation loss: 0.357743  Batch: [  576/ 1070]\n",
      "validation loss: 0.435570  Batch: [  608/ 1070]\n",
      "validation loss: 0.450591  Batch: [  640/ 1070]\n",
      "validation loss: 0.367504  Batch: [  672/ 1070]\n",
      "validation loss: 0.434404  Batch: [  704/ 1070]\n",
      "validation loss: 0.514468  Batch: [  736/ 1070]\n",
      "validation loss: 0.548798  Batch: [  768/ 1070]\n",
      "validation loss: 0.510258  Batch: [  800/ 1070]\n",
      "validation loss: 0.398346  Batch: [  832/ 1070]\n",
      "validation loss: 0.593690  Batch: [  864/ 1070]\n",
      "validation loss: 0.333476  Batch: [  896/ 1070]\n",
      "validation loss: 0.487027  Batch: [  928/ 1070]\n",
      "validation loss: 0.705554  Batch: [  960/ 1070]\n",
      "validation loss: 0.480340  Batch: [  992/ 1070]\n",
      "validation loss: 0.558018  Batch: [ 1024/ 1070]\n",
      "validation loss: 0.285358  Batch: [ 1056/ 1070]\n",
      "validation loss: 0.544429  Batch: [ 1070/ 1070]\n",
      "Epoch 17/50, Validation Loss: 0.4870, Validation Accuracy: 75.51%\n",
      "\n",
      "\n",
      "Epoch 18 \n",
      "---------------------\n",
      "training loss: 0.514093  Batch: [   32/ 4992]\n",
      "training loss: 0.374585  Batch: [   64/ 4992]\n",
      "training loss: 0.385135  Batch: [   96/ 4992]\n",
      "training loss: 0.501236  Batch: [  128/ 4992]\n",
      "training loss: 0.556487  Batch: [  160/ 4992]\n",
      "training loss: 0.395599  Batch: [  192/ 4992]\n",
      "training loss: 0.432215  Batch: [  224/ 4992]\n",
      "training loss: 0.716243  Batch: [  256/ 4992]\n",
      "training loss: 0.407454  Batch: [  288/ 4992]\n",
      "training loss: 0.483770  Batch: [  320/ 4992]\n",
      "training loss: 0.479769  Batch: [  352/ 4992]\n",
      "training loss: 0.370106  Batch: [  384/ 4992]\n",
      "training loss: 0.449160  Batch: [  416/ 4992]\n",
      "training loss: 0.461830  Batch: [  448/ 4992]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-98981d54aea8>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Iterate over training batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Move inputs and labels to the specified device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-777157b7feb6>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m52\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mlatn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlonw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m87\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m91\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;31m# lat = (latn+lats)/2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# lon = (lone+lonw)/2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1563\u001b[0m         \u001b[0mtup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_tuple_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexingError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple_same_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    965\u001b[0m                 \u001b[0;31m# We don't need to check for tuples here because those are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m                 \u001b[0;31m#  caught by the _is_nested_tuple_indexer check above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m                 \u001b[0msection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m                 \u001b[0;31m# We should never have a scalar section here, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1625\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1627\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3714\u001b[0m         \u001b[0;31m# irow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3716\u001b[0;31m             \u001b[0mnew_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_xs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3718\u001b[0m             \u001b[0;31m# if we are a copy, mark as such\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mfast_xs\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m   1112\u001b[0m                 \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mimmutable_ea\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdtype\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m             )\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_wrapped_if_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36mensure_wrapped_if_datetimelike\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \"\"\"\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"M\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatetimeArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#verification Model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "model = VerificationModel(51).to(device)\n",
    "\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Define the number of epochs for training\n",
    "num_epochs = 50\n",
    "\n",
    "# Get the total number of training samples\n",
    "training_size = len(train_data_loader.dataset)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch\", epoch+1, \"\\n---------------------\")\n",
    "\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    training_batch = 0\n",
    "\n",
    "    # Iterate over training batches\n",
    "    for inputs, labels in train_data_loader:\n",
    "\n",
    "        # Move inputs and labels to the specified device\n",
    "        inputs = inputs.float()\n",
    "        # print(inputs.shape)\n",
    "        # print(labels.shape)\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass to get outputs\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate the loss\n",
    "        outputs = outputs.view(-1)\n",
    "\n",
    "        labels = labels.float()\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "\n",
    "        # Backward pass to calculate gradients\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # Update running loss and batch count\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        training_batch += len(inputs)\n",
    "        print(f\"training loss: {loss:>7f}  Batch: [{training_batch:>5d}/{training_size:>5d}]\")\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_data_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    validation_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    validation_size = len(validation_data_loader.dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        validation_batch = 0\n",
    "        for inputs, labels in validation_data_loader:\n",
    "            # Move inputs and labels to the specified device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1)\n",
    "\n",
    "            labels = labels.float()\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            # Update validation loss and count\n",
    "            validation_loss += loss.item() * inputs.size(0)\n",
    "            # Calculate accuracy\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            validation_batch += len(inputs)\n",
    "            print(f\"validation loss: {loss:>7f}  Batch: [{validation_batch:>5d}/{validation_size:>5d}]\")\n",
    "\n",
    "    # Calculate average validation loss and accuracy for the epoch\n",
    "    epoch_validation_loss = validation_loss / len(validation_data_loader.dataset)\n",
    "    validation_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {epoch_validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.2f}%\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uO-USyEX1yvz",
    "outputId": "35c6fdb7-92be-4973-ca1b-92e05f2c23dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.3\n",
      "Threshold: 0.325\n",
      "Threshold: 0.35000000000000003\n",
      "Threshold: 0.37500000000000006\n",
      "Threshold: 0.4000000000000001\n",
      "Threshold: 0.4250000000000001\n",
      "Threshold: 0.4500000000000001\n",
      "Threshold: 0.47500000000000014\n",
      "Threshold: 0.5000000000000002\n",
      "Threshold: 0.5250000000000001\n",
      "Threshold: 0.5500000000000003\n",
      "Threshold: 0.5750000000000002\n",
      "Threshold: 0.6000000000000003\n",
      "Threshold: 0.6250000000000002\n",
      "Threshold: 0.6500000000000004\n",
      "Threshold: 0.6750000000000003\n",
      "Threshold: 0.7000000000000004\n",
      "Threshold: 0.7250000000000003\n",
      "Threshold: 0.7500000000000004\n",
      "Threshold: 0.7750000000000004\n",
      "Threshold: 0.8000000000000005\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr4AAAK9CAYAAADCE2/bAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACx3ElEQVR4nOzdd1xV9ePH8ddlgzIc4MS9R+5ZbpxpmjtNTcuysmFZpqYtzcpvadvS0tTcWmqZOXLnXjly742igMi+5/fH/QneRAUFzr3wfj4ePjp8zh1v4EJvzv2cz7EYhmEgIiIiIpLFuZgdQEREREQkM6j4ioiIiEi2oOIrIiIiItmCiq+IiIiIZAsqviIiIiKSLaj4ioiIiEi2oOIrIiIiItmCiq+IiIiIZAsqviIiIiKSLaj4imSSYsWK8dRTT5kdI9tp3LgxjRs3NjvGPb377rtYLBYuX75sdhSHY7FYePfdd9PlsU6cOIHFYmHKlCnp8ngAW7ZswcPDg5MnT6bbY6a37t2707VrV7NjiJhOxVeyhClTpmCxWJL+ubm5UahQIZ566inOnj1rdjyHFhUVxQcffMBDDz2Ej48P/v7+NGjQgKlTp+IsVzTfv38/7777LidOnDA7ym0SExOZPHkyjRs3Jnfu3Hh6elKsWDH69u3Ltm3bzI6XLmbMmMH48ePNjmEnMzMNHz6cJ554gqJFiyaNNW7c2O53kre3Nw899BDjx4/HarWm+DhXrlzhjTfeoGzZsnh5eZE7d25atmzJb7/9dsfnjoiI4L333qNKlSrkzJkTb29vKlWqxJAhQzh37lzS7YYMGcL8+fPZvXt3qj+v7PDalezHYjjL/9lE7mLKlCn07duX999/n+LFixMTE8OmTZuYMmUKxYoVY+/evXh5eZmaMTY2FhcXF9zd3U3NcauLFy/SrFkz/v33X7p3706jRo2IiYlh/vz5rF27lm7duvHzzz/j6upqdtS7mjdvHl26dGHVqlW3Hd2Ni4sDwMPDI9NzRUdH07FjR5YuXUrDhg1p164duXPn5sSJE8yZM4dDhw5x6tQpChcuzLvvvst7771HaGgoefPmzfSsD6Jt27bs3bs3w/7wiImJwc3NDTc3twfOZBgGsbGxuLu7p8vreteuXVSrVo2///6bevXqJY03btyYo0ePMmbMGAAuX77MjBkz2Lp1K8OGDWP06NF2j3Pw4EGaNWtGaGgoffv2pWbNmly7do2ff/6ZXbt2MXjwYMaOHWt3n2PHjhESEsKpU6fo0qULjzzyCB4eHvzzzz/MnDmT3Llzc+jQoaTb16lTh7JlyzJ16tR7fl5pee2KOBVDJAuYPHmyARhbt261Gx8yZIgBGLNnzzYpmbmio6ONxMTEO+5v2bKl4eLiYixcuPC2fYMHDzYA46OPPsrIiCm6fv16mm4/d+5cAzBWrVqVMYHu04svvmgAxrhx427bl5CQYIwdO9Y4ffq0YRiG8c477xiAERoammF5rFarcePGjXR/3EcffdQoWrRouj5mYmKiER0dfd/3z4hMKXn55ZeNIkWKGFar1W68UaNGRsWKFe3GoqOjjaJFixq+vr5GQkJC0nhcXJxRqVIlw8fHx9i0aZPdfRISEoxu3boZgDFr1qyk8fj4eKNKlSqGj4+PsW7duttyhYeHG8OGDbMb+9///mfkyJHDiIyMvOfnlZbX7oN40O+zSFqp+EqWcKfi+9tvvxmA8eGHH9qN//vvv0anTp2MXLlyGZ6enkaNGjVSLH9Xr141Xn31VaNo0aKGh4eHUahQIaNXr1525SQmJsYYOXKkUbJkScPDw8MoXLiw8cYbbxgxMTF2j1W0aFGjT58+hmEYxtatWw3AmDJlym3PuXTpUgMwFi9enDR25swZo2/fvkZQUJDh4eFhVKhQwfjhhx/s7rdq1SoDMGbOnGkMHz7cKFiwoGGxWIyrV6+m+DXbuHGjARj9+vVLcX98fLxRunRpI1euXEll6fjx4wZgjB071vjss8+MIkWKGF5eXkbDhg2NPXv23PYYqfk63/zerV692nj++eeNwMBAIyAgwDAMwzhx4oTx/PPPG2XKlDG8vLyM3LlzG507dzaOHz9+2/3/++9mCW7UqJHRqFGj275Os2fPNkaNGmUUKlTI8PT0NJo2bWocPnz4ts/hq6++MooXL254eXkZtWrVMtauXXvbY6bk9OnThpubm9G8efO73u6mm8X38OHDRp8+fQx/f3/Dz8/PeOqpp4yoqCi72/74449GkyZNjMDAQMPDw8MoX7688c0339z2mEWLFjUeffRRY+nSpUaNGjUMT0/PpCKT2scwDMNYsmSJ0bBhQyNnzpyGr6+vUbNmTePnn382DMP29f3v1/7Wwpnanw/AePHFF43p06cbFSpUMNzc3Ixffvklad8777yTdNuIiAjjlVdeSfq5DAwMNEJCQozt27ffM9PN1/DkyZPtnv/ff/81unTpYuTNm9fw8vIyypQpc1txTEmRIkWMp5566rbxlIqvYRhG586dDcA4d+5c0tjMmTMNwHj//fdTfI5r164ZAQEBRrly5ZLGZs2aZQDG6NGj75nxpt27dxuAsWDBgrveLq2v3T59+qT4R8bN1/StUvo+z5kzx8iVK1eKX8fw8HDD09PTeP3115PGUvuaEklJ6t83EnFCN9/mzJUrV9LYvn37ePjhhylUqBBvvfUWOXLkYM6cOXTo0IH58+fz+OOPA3D9+nUaNGjAv//+S79+/ahevTqXL19m0aJFnDlzhrx582K1WnnsscdYv349zz77LOXLl2fPnj2MGzeOQ4cO8euvv6aYq2bNmpQoUYI5c+bQp08fu32zZ88mV65ctGzZErBNR6hbty4Wi4WBAwcSGBjIH3/8wdNPP01ERASvvvqq3f0/+OADPDw8GDx4MLGxsXd8i3/x4sUA9O7dO8X9bm5u9OjRg/fee48NGzYQEhKStG/q1KlERkby4osvEhMTw+eff07Tpk3Zs2cP+fLlS9PX+aYXXniBwMBARo4cSVRUFABbt27l77//pnv37hQuXJgTJ07w7bff0rhxY/bv34+Pjw8NGzbk5Zdf5osvvmDYsGGUL18eIOm/d/LRRx/h4uLC4MGDCQ8P55NPPqFnz55s3rw56TbffvstAwcOpEGDBgwaNIgTJ07QoUMHcuXKdc+3eP/44w8SEhLo1avXXW/3X127dqV48eKMGTOGHTt2MGnSJIKCgvj444/tclWsWJHHHnsMNzc3Fi9ezAsvvIDVauXFF1+0e7yDBw/yxBNP8Nxzz9G/f3/Kli2bpseYMmUK/fr1o2LFigwdOpSAgAB27tzJ0qVL6dGjB8OHDyc8PJwzZ84wbtw4AHLmzAmQ5p+Pv/76izlz5jBw4EDy5s1LsWLFUvwaDRgwgHnz5jFw4EAqVKjAlStXWL9+Pf/++y/Vq1e/a6aU/PPPPzRo0AB3d3eeffZZihUrxtGjR1m8ePFtUxJudfbsWU6dOkX16tXveJv/unlyXUBAQNLYvX4W/f39ad++PT/99BNHjhyhVKlSLFq0CCBNr68KFSrg7e3Nhg0bbvv5u9X9vnZT67/f59KlS/P444+zYMECvvvuO7vfWb/++iuxsbF0794dSPtrSuQ2ZjdvkfRw86jfihUrjNDQUOP06dPGvHnzjMDAQMPT09PuLblmzZoZlStXtjs6YLVajfr16xulS5dOGhs5cuQdj47cfFtz2rRphouLy21vNU6YMMEAjA0bNiSN3XrE1zAMY+jQoYa7u7sRFhaWNBYbG2sEBATYHYV9+umnjQIFChiXL1+2e47u3bsb/v7+SUdjbx7JLFGiRKrezu7QoYMB3PGIsGEYxoIFCwzA+OKLLwzDSD5a5u3tbZw5cybpdps3bzYAY9CgQUljqf063/zePfLII3Zv/xqGkeLncfNI9dSpU5PG7jbV4U5HfMuXL2/ExsYmjX/++ecGkHTkOjY21siTJ49Rq1YtIz4+Pul2U6ZMMYB7HvEdNGiQARg7d+686+1uunl07L9H4B9//HEjT548dmMpfV1atmxplChRwm6saNGiBmAsXbr0ttun5jGuXbtm+Pr6GnXq1Lnt7ehb39q/07SCtPx8AIaLi4uxb9++2x6H/xzx9ff3N1588cXbbnerO2VK6Yhvw4YNDV9fX+PkyZN3/BxTsmLFitvenbmpUaNGRrly5YzQ0FAjNDTUOHDggPHGG28YgPHoo4/a3bZq1aqGv7//XZ/rs88+MwBj0aJFhmEYRrVq1e55n5SUKVPGaN269V1vk9bXblqP+Kb0ff7zzz9T/Fq2adPG7jWZlteUSEq0qoNkKSEhIQQGBhIcHEznzp3JkSMHixYtSjo6FxYWxl9//UXXrl2JjIzk8uXLXL58mStXrtCyZUsOHz6ctArE/PnzqVKlSopHRiwWCwBz586lfPnylCtXLumxLl++TNOmTQFYtWrVHbN269aN+Ph4FixYkDS2bNkyrl27Rrdu3QDbiTjz58+nXbt2GIZh9xwtW7YkPDycHTt22D1unz598Pb2vufXKjIyEgBfX9873ubmvoiICLvxDh06UKhQoaSPa9euTZ06dViyZAmQtq/zTf3797/tZKNbP4/4+HiuXLlCqVKlCAgIuO3zTqu+ffvaHVlq0KABYDthCGDbtm1cuXKF/v37251U1bNnT7t3EO7k5tfsbl/flAwYMMDu4wYNGnDlyhW778GtX5fw8HAuX75Mo0aNOHbsGOHh4Xb3L168eNK7B7dKzWMsX76cyMhI3nrrrdtODr35M3A3af35aNSoERUqVLjn4wYEBLB582a7VQvuV2hoKGvXrqVfv34UKVLEbt+9PscrV64A3PH1cODAAQIDAwkMDKRcuXKMHTuWxx577Lal1CIjI+/5Ovnvz2JERESaX1s3s95rybz7fe2mVkrf56ZNm5I3b15mz56dNHb16lWWL1+e9PsQHux3rgiApjpIlvL1119TpkwZwsPD+fHHH1m7di2enp5J+48cOYJhGIwYMYIRI0ak+BiXLl2iUKFCHD16lE6dOt31+Q4fPsy///5LYGDgHR/rTqpUqUK5cuWYPXs2Tz/9NGCb5pA3b96kX+KhoaFcu3aN77//nu+//z5Vz1G8ePG7Zr7p5v/UIiMj7d52vdWdynHp0qVvu22ZMmWYM2cOkLav891yR0dHM2bMGCZPnszZs2ftllf7b8FLq/+WnJvl5erVqwBJa7KWKlXK7nZubm53fAv+Vn5+fkDy1zA9ct18zA0bNvDOO++wceNGbty4YXf78PBw/P39kz6+0+shNY9x9OhRACpVqpSmz+GmtP58pPa1+8knn9CnTx+Cg4OpUaMGbdq0oXfv3pQoUSLNGW/+oXO/nyNwx2X/ihUrxsSJE7FarRw9epTRo0cTGhp62x8Rvr6+9yyj//1Z9PPzS8qe1qz3KvT3+9pNrZS+z25ubnTq1IkZM2YQGxuLp6cnCxYsID4+3q74PsjvXBFQ8ZUspnbt2tSsWROwHZV85JFH6NGjBwcPHiRnzpxJ62cOHjw4xaNgcHvRuRur1UrlypX57LPPUtwfHBx81/t369aN0aNHc/nyZXx9fVm0aBFPPPFE0hHGm3mffPLJ2+YC3/TQQw/ZfZyao71gmwP766+/8s8//9CwYcMUb/PPP/8ApOoo3K3u5+ucUu6XXnqJyZMn8+qrr1KvXj38/f2xWCx07979jmuhptadlrK6U4lJq3LlygGwZ88eqlatmur73SvX0aNHadasGeXKleOzzz4jODgYDw8PlixZwrhx4277uqT0dU3rY9yvtP58pPa127VrVxo0aMAvv/zCsmXLGDt2LB9//DELFiygdevWD5w7tfLkyQMk/7H0Xzly5LCbG//www9TvXp1hg0bxhdffJE0Xr58eXbt2sWpU6du+8Pnpv/+LJYrV46dO3dy+vTpe/6eudXVq1dT/MP1Vml97d6pSCcmJqY4fqfvc/fu3fnuu+/4448/6NChA3PmzKFcuXJUqVIl6TYP+jtXRMVXsixXV1fGjBlDkyZN+Oqrr3jrrbeSjgi5u7vb/Q8pJSVLlmTv3r33vM3u3btp1qxZqt76/a9u3brx3nvvMX/+fPLly0dERETSSRwAgYGB+Pr6kpiYeM+8adW2bVvGjBnD1KlTUyy+iYmJzJgxg1y5cvHwww/b7Tt8+PBttz906FDSkdC0fJ3vZt68efTp04dPP/00aSwmJoZr167Z3e5+vvb3cvNiBEeOHKFJkyZJ4wkJCZw4ceK2Pzj+q3Xr1ri6ujJ9+vR0PUlo8eLFxMbGsmjRIruSlJa3eFP7GCVLlgRg7969d/2D8E5f/wf9+bibAgUK8MILL/DCCy9w6dIlqlevzujRo5OKb2qf7+Zr9V4/6ym5WRCPHz+eqts/9NBDPPnkk3z33XcMHjw46Wvftm1bZs6cydSpU3n77bdvu19ERAQLFy6kXLlySd+Hdu3aMXPmTKZPn87QoUNT9fwJCQmcPn2axx577K63S+trN1euXLf9TAJpvpJdw4YNKVCgALNnz+aRRx7hr7/+Yvjw4Xa3ycjXlGQPmuMrWVrjxo2pXbs248ePJyYmhqCgIBo3bsx3333H+fPnb7t9aGho0nanTp3YvXs3v/zyy223u3n0rWvXrpw9e5aJEyfedpvo6Oik1QnupHz58lSuXJnZs2cze/ZsChQoYFdCXV1d6dSpE/Pnz0/xf8y35k2r+vXrExISwuTJk1O8MtTw4cM5dOgQb7755m1HaH799Ve7Obpbtmxh8+bNSaUjLV/nu3F1db3tCOyXX35525GkHDlyAKT4P9/7VbNmTfLkycPEiRNJSEhIGv/555/veITvVsHBwfTv359ly5bx5Zdf3rbfarXy6aefcubMmTTlunlE+L/TPiZPnpzuj9GiRQt8fX0ZM2YMMTExdvtuvW+OHDlSnHryoD8fKUlMTLztuYKCgihYsCCxsbH3zPRfgYGBNGzYkB9//JFTp07Z7bvX0f9ChQoRHBycpquYvfnmm8THx9sdsezcuTMVKlTgo48+uu2xrFYrzz//PFevXuWdd96xu0/lypUZPXo0GzduvO15IiMjbyuN+/fvJyYmhvr16981Y1pfuyVLliQ8PDzpqDTA+fPnU/zdeTcuLi507tyZxYsXM23aNBISEuymOUDGvKYke9ERX8ny3njjDbp06cKUKVMYMGAAX3/9NY888giVK1emf//+lChRgosXL7Jx40bOnDmTdEnPN954I+mKYP369aNGjRqEhYWxaNEiJkyYQJUqVejVqxdz5sxhwIABrFq1iocffpjExEQOHDjAnDlz+PPPP5OmXtxJt27dGDlyJF5eXjz99NO4uNj/PfrRRx+xatUq6tSpQ//+/alQoQJhYWHs2LGDFStWEBYWdt9fm6lTp9KsWTPat29Pjx49aNCgAbGxsSxYsIDVq1fTrVs33njjjdvuV6pUKR555BGef/55YmNjGT9+PHny5OHNN99Muk1qv85307ZtW6ZNm4a/vz8VKlRg48aNrFixIukt5puqVq2Kq6srH3/8MeHh4Xh6etK0aVOCgoLu+2vj4eHBu+++y0svvUTTpk3p2rUrJ06cYMqUKZQsWTJVR5s+/fRTjh49yssvv8yCBQto27YtuXLl4tSpU8ydO5cDBw7YHeFPjRYtWuDh4UG7du147rnnuH79OhMnTiQoKCjFPzIe5DH8/PwYN24czzzzDLVq1aJHjx7kypWL3bt3c+PGDX766ScAatSowezZs3nttdeoVasWOXPmpF27duny8/FfkZGRFC5cmM6dOyddpnfFihVs3brV7p2BO2VKyRdffMEjjzxC9erVefbZZylevDgnTpzg999/Z9euXXfN0759e3755ZdUzZ0F21SFNm3aMGnSJEaMGEGePHnw8PBg3rx5NGvWjEceecTuym0zZsxgx44dvP7663avFXd3dxYsWEBISAgNGzaka9euPPzww7i7u7Nv376kd2tuXY5t+fLl+Pj40Lx583vmTMtrt3v37gwZMoTHH3+cl19+mRs3bvDtt99SpkyZNJ+E2q1bN7788kveeecdKleufNuyhBnxmpJsJvMXkhBJf3e6gIVh2K4MVLJkSaNkyZJJy2UdPXrU6N27t5E/f37D3d3dKFSokNG2bVtj3rx5dve9cuWKMXDgQKNQoUJJC6X36dPHbmmxuLg44+OPPzYqVqxoeHp6Grly5TJq1KhhvPfee0Z4eHjS7f67nNlNhw8fTlpkf/369Sl+fhcvXjRefPFFIzg42HB3dzfy589vNGvWzPj++++TbnNzma65c+em6WsXGRlpvPvuu0bFihUNb29vw9fX13j44YeNKVOm3Lac060XsPj000+N4OBgw9PT02jQoIGxe/fu2x47NV/nu33vrl69avTt29fImzevkTNnTqNly5bGgQMHUvxaTpw40ShRooTh6uqaqgtY/PfrdKcLG3zxxRdG0aJFDU9PT6N27drGhg0bjBo1ahitWrVKxVfXdpWrSZMmGQ0aNDD8/f0Nd3d3o2jRokbfvn3tlou605Xbbn59br1ox6JFi4yHHnrI8PLyMooVK2Z8/PHHxo8//njb7W5ewCIlqX2Mm7etX7++4e3tbfj5+Rm1a9c2Zs6cmbT/+vXrRo8ePYyAgIDbLmCR2p8P/v/CBinhluXMYmNjjTfeeMOoUqWK4evra+TIkcOoUqXKbRffuFOmO32f9+7dazz++ONGQECA4eXlZZQtW9YYMWJEinlutWPHDgO4bXmtO13AwjAMY/Xq1bct0WYYhnHp0iXjtddeM0qVKmV4enoaAQEBRkhISNISZim5evWqMXLkSKNy5cqGj4+P4eXlZVSqVMkYOnSocf78ebvb1qlTx3jyySfv+TndlNrXrmEYxrJly4xKlSoZHh4eRtmyZY3p06ff9QIWd2K1Wo3g4GADMEaNGpXibVL7mhJJicUw0ulMDhHJ8k6cOEHx4sUZO3YsgwcPNjuOKaxWK4GBgXTs2DHFt1sl+2nWrBkFCxZk2rRpZke5o127dlG9enV27NiRppMtRbIazfEVEbmDmJiY2+Z5Tp06lbCwMBo3bmxOKHE4H374IbNnz07zyVyZ6aOPPqJz584qvZLtaY6viMgdbNq0iUGDBtGlSxfy5MnDjh07+OGHH6hUqRJdunQxO544iDp16hAXF2d2jLuaNWuW2RFEHIKKr4jIHRQrVozg4GC++OILwsLCyJ07N7179+ajjz6yu+qbiIg4B83xFREREZFsQXN8RURERCRbUPEVERERkWwh283xtVqtnDt3Dl9fX13uUERERMQBGYZBZGQkBQsWvO3CTg8i2xXfc+fOERwcbHYMEREREbmH06dPU7hw4XR7vGxXfH19fQHbF9LPz8/kNCIiIiLyXxEREQQHByf1tvSS7YrvzekNfn5+Kr4iIiIiDiy9p6Xq5DYRERERyRZUfEVEREQkW1DxFREREZFsQcVXRERERLIFFV8RERERyRZUfEVEREQkW1DxFREREZFsQcVXRERERLIFFV8RERERyRZUfEVEREQkW1DxFREREZFsQcVXRERERLIFFV8RERERyRZUfEVEREQkW1DxFREREZFsQcVXRERERLIFFV8RERERyRZUfEVEREQkW1DxFREREZFsQcVXRERERLIFFV8RERERyRZUfEVEREQkWzC1+K5du5Z27dpRsGBBLBYLv/766z3vs3r1aqpXr46npyelSpViypQpGZ5TRERERJyfqcU3KiqKKlWq8PXXX6fq9sePH+fRRx+lSZMm7Nq1i1dffZVnnnmGP//8M4OTioiIiIizczPzyVu3bk3r1q1TffsJEyZQvHhxPv30UwDKly/P+vXrGTduHC1btsyomCIiIiKSma4dz5CHdao5vhs3biQkJMRurGXLlmzcuPGO94mNjSUiIsLun4iIiIg4oLBDsPQpmF49Qx7eqYrvhQsXyJcvn91Yvnz5iIiIIDo6OsX7jBkzBn9//6R/wcHBmRFVRERERFLryn74vSdMKQ/7fgLDmiFP41TF934MHTqU8PDwpH+nT582O5KIiIiIAIT+A4u7wpRKJOyblVx4Pfwz5Omcqvjmz5+fixcv2o1dvHgRPz8/vL29U7yPp6cnfn5+dv9ERERExEQXt8OvHWBqFTg0lxk7KlF9/HOEJhSGRz6Ep/ZmyNOaenJbWtWrV48lS5bYjS1fvpx69eqZlEhEREREUu3cJtj0ARxP7nPfb6rBgPltMQwLLWcNY80L/cCIzZCnN/WI7/Xr19m1axe7du0CbMuV7dq1i1OnTgG2aQq9e/dOuv2AAQM4duwYb775JgcOHOCbb75hzpw5DBo0yIz4IiIiIpIaZ9bBvBYws55d6f10Y0uem9cOw7AAULtOMDlyeGRYDFOP+G7bto0mTZokffzaa68B0KdPH6ZMmcL58+eTSjBA8eLF+f333xk0aBCff/45hQsXZtKkSVrKTERERMTRGAacXmU7wnt6tf2unEV4d8dA3p9/I2ls8OB6fPJJcywWS4ZFshiGYWTYozugiIgI/P39CQ8P13xfERERkfRmGHByGWx8H879bb/PvwRGrbd4fXIhxn2+NWn4gw+aMHx4g6TSm1F9zanm+IqIiIiIgzIMOPY7bHofLmy135erNNR5m8Qy3RnwwlImTUreP25cS159tW6mRFTxFREREZH7Z1jhyK+waRRc2mm/L08FqPM2lO1KgtVCr16/MGuWbcUGiwUmTmzH009nzMUqUqLiKyIiIiJpZ02EQ/Ng8yi4/J/lxwIfgrojoHRHsNjWUnC1GOTI4Q6Am5sL06c/TrdulTI1soqviIiIiKSeNQEOzILNoyHsgP2+fDVshbdku6TCe5PFYuG779qSmGjQqVN52rYtk4mhbVR8RUREROTeEuPh3+m2wnvtqP2+AnVthbd4a9schjtwdXVh8uT2GRz0zpzqym0iIiIikskSYmH3d/BjGfizn33pLdQAOi+HJ/6GEm3sSu+lS1E0bfoT//xzMYUHNYeO+IqIiIjI7RJiYM8k2PIxXD9jv69IM9sR3uBGKd719OlwQkKmcejQFZo3n8a6dX0pUyZPJoS+OxVfEREREUkWfwP++Q62fgJRF+z3FWtlK7yF6t/x7keOhBESMpWTJ8MB8PR0zci0aaLiKyIiIiIQFwm7voVt/4PoUPt9JdpB3behQO27PsTevZdo3nwaFy5cB6BUqdysWNGLokUDMih02qj4ioiIiGRnseGw80vYPg5iwuz3le5oW4c3X7V7Psy2bedo2XI6YWHRAFSqFMTy5b3Inz9nRqS+Lyq+IiIiItlRdBjs+Bx2fm4rv0ksULYb1B0OeVO3zu7atSdp23YGkZFxANSqVZClS58kd27vDAh+/1R8RURERLKTG5dh+2ew6yvb9IabLC5QvifUHgZ5yqX64ZYuPcLjj88mJiYBgIYNi7J48RP4+Xmmd/IHpuIrIiIikh1EXYBtn8KubyDhRvK4ixtU6A21h0KuUml+2KNHw5JKb6tWpZg/vys+Pu7plTpdqfiKiIiIZGWRZ2HbWNtKDQkxyeMu7lCpH9R+C/yL3ffDv/hibSIiYtm+/TwzZnTCw8NxVnH4LxVfERERkawo4hRs+Qj2/gCJccnjrp5QuT/UehP8gtPlqYYObYDVauDicuertjkCFV8RERGRrOTaMdgyBvb9BNb45HE3b6jyPNQcDDkL3PfDf/zxekqXzkPHjuXtxh299IKKr4iIiEjWEHYItnwI+6eDkZg87p4Dqg6Emq+BT9B9P7xhGAwf/hdjxqzH3d2FxYufoGXLtM8JNpOKr4iIiIgzu7IfNo2Gg7PAsCaPe/hB9Zeh+qvg/WCXC7ZaDV555Q+++morAPHxVvbuvaTiKyIiIiKZ4NJu2DwKDs0HjORxr1y2slvtZfAKeOCnSUiw8swzi/jpp91JY19/3YYXXqj1wI+d2VR8RURERJzJxe2w8QM4utB+3Dsv1Hgdqr4Ann7p8lRxcYn06DGf+fP/BWzzeCdPbk/v3lXS5fEzm4qviIiIiDM4twk2fQDHl9iP++SDWm9AlQG2+bzp5MaNeDp1msPSpUcAcHd3Ydaszred1OZMVHxFREREHNmZdbbCe3K5/XjOglBriG1pMvf0vTRwREQs7drNZO3akwB4e7vxyy/dnG5O73+p+IqIiIg4GsOA06tg4/twZo39Pt8itotOVOoLbl4Z8vR7915i8+YzAPj5efLbb0/QoEHRDHmuzKTiKyIiIuIoDANOLrMV3nN/2+/zL2G7rHDF3uDqkaEx6tcPZu7cLjz77G/89tsT1KhRMEOfL7Oo+IqIiIiYzTDg2G+2KQ0Xttrvy1UG6gyH8j3AJfOqW7t2ZTlypDg5cmRsyc5MKr4iIiIiZjGscORX2DQKLu2035enAtR5G8p2BRfXDI1x8OBlli49wiuv1LUbz0qlF1R8RURERDKfNREOzbOtw3t5r/2+wIeg7ggo3REsLhkeZffuC7RoMZ1Ll6KwWg0GDaqX4c9pFhVfERERkcxiTYADs2DzaAg7YL8vXw1b4S3ZLlMKL8CmTWdo3fpnrl2LAWDq1H944YVaeHpmzYqYNT8rEREREUeSGA/7p8GWD+HaUft9BepCvZFQrBVYLJkW6a+/jvPYYzOJiooHoF69wvz+e48sW3pBxVdEREQk4yTEwr4psOUjiDhhv69QA1vhLdIsUwsvwG+/HaJz5znExiYC0LRpcRYu7E7OnFlrTu9/qfiKiIiIpLeEGNgzCbZ8DNfP2O8r0sw2pSG4kSnRZs/ey5NP/kJCghWAdu3KMGdOF7y8sn4tzPqfoYiIiEhmib8B/3wHWz+BqAv2+4q1shXeQvXNyQZMmrSDZ59djGHYPu7evRJTp3bA3T1jV41wFCq+IiIiIg8qLhJ2fQvb/gfRofb7SrSDeiMgfy1zsv2/yMhY3nlndVLpfeaZakyY0BZX18w5kc4RqPiKiIiI3K/YcNj5JWwfBzFh9vtKd7Stw5uvmjnZ/sPX15Nly56kUaMp9O5dhU8/bYElk+cWm03FV0RERCStosNgx+ew83Nb+U1igbLdoO5wyFvJtHh3UrFiELt3D6BgQd9sV3pBxVdEREQk9W5chu2fwa6vbNMbbrK4QPmeUHsY5ClnXr5bJCZa+eGHnfTrVw03t+TpDIUK+ZmYylwqviIiIiL3EnUBtn0Ku76BhBvJ4y5uUKE31B4KuUqZl+8/4uMTeeqphcyYsYcNG04zeXJ7XFyy3xHe/1LxFREREbmTyLO2FRr2fG9bouwmF3eo1A9qvwX+xUyLl5KYmAS6d5/HwoUHAfj5538YOLAWtWoVMjmZ+VR8RURERP4r4pTtohN7f4DEuORxV0946Fmo9Sb4FjYv3x1ERcXRocNsVqw4BoCnpytz53ZR6f1/Kr4iIiIiN107BlvGwL6fwBqfPO7mDVWeh5qDIWcB8/LdxbVrMTz66Az+/vs0ADlyuLNwYXeaNSthcjLHoeIrIiIiEnYItnwI+6eDkZg87p4Tqr4INV8DnyDz8t1DaGgULVpMZ9cu20Uz/P09+eOPntSrF2xyMsei4isiIiLZ1+V9sHk0HJwNhjV53MMPqr8M1V8F7zymxUuNs2cjCAmZxoEDlwEIDPRh2bJeVK2a3+RkjkfFV0RERLKfS7th8yg4NB8wkse9ckH1QVDtJfAKMCtdmrz22rKk0lu4sB/Ll/eiXLm8JqdyTCq+IiIikn1c3A4bP4CjC+3HvfNCjdeh6gvg6Vzr3H777aPs3x/KjRvxrFzZm2LFAsyO5LBUfEVERCTrO7cJNn0Ax5fYj/vkg1pvQJUB4J7DnGwPKHdub5Yv74XValCwoK/ZcRyaiq+IiIhkXWfWwcb34dQK+/GcBaHWEKjcH9y9zcl2nzZvPkPp0nnInTs5d/78OU1M5DxUfEVERCRrMQw4vcpWeM+ssd/nW8R20YlKfcHNy5x8D2DZsqN06DCLhx7Kx/LlvfD19TQ7klNxufdNRERERJyAYcDxpTDrEZjbzL70+peAFpPg6cNQ9XmnLL2//PIv7drNJDo6gc2bz/LRR+vNjuR0dMRXREREnJthwLHfbHN4L2y135erDNQZDuV7gIvz1p7p0//hqad+JTHRtgLF44+XY+TIRiancj7O+woQERGR7M2wwpFfYdMouLTTfl+eClDnbSjbFVxcTYmXXiZM2MYLL/yO8f+rrvXq9RA//tgeNze9cZ9WKr4iIiLiXKyJcGiebR3ey3vt9wVWgbpvQ+mOYHH+Yjh27AbefDP5xLwXXqjJl1+2wcXFYmIq56XiKyIiIs7BmgAHZtmO8F49aL8vXw2oOxJKtgOL85dCwzAYOXIVo0atSxobMuRhxoxphiULfH5mUfEVERERx5YYD/unwZYP4dpR+30F6kK9kVCsVZYovDfNmLHHrvR++GFThg5tYGKirEHFV0RERBxTQizsmwJbPoKIE/b7CjeEuiOgSLMsVXhv6tatEvPm/cuvvx7giy9a8dJLdcyOlCWo+IqIiIhjSYiBPZNgy8dw/Yz9viLNbIU3OGuvaODm5sKsWZ3466/jtG5d2uw4WYaKr4iIiDiG+Bvwz3ew9ROIumC/r1grW+EtVN+cbBksOjqec+ciKVkyd9KYp6ebSm86U/EVERERc8VFwq5vYNunEB1qv69EO6g3AvLXMidbJoiMjOWxx2Zx6NAV1q3rS4kSucyOlGWp+IqIiIg5YsNh55ewfRzEhNnvK93JtixZUFVTomWWsLBoWrf+mS1bzgLQseNsdux4TsuVZRAVXxEREclc0WGw43PY+bmt/CaxQNluUHc45K1kWrzMcuHCdVq0mMaePZcAyJ3bm4kT26n0ZiAVXxEREckcN0JtR3d3fWWb3nCTxdV2SeHawyBPOfPyZaJTp8IJCZnK4cO2I9358uVgxYreVKoUZHKyrE3FV0RERDJW1AXY+j/Y/S0k3Eged3GDCn2gzlAIKGlevkx2+PAVmjWbyunTEQAUKeLPihW9KF06j8nJsj4VXxEREckYkWdtKzTs+d62RNlNLu5Q+WmoNQT8i5kWzwz//HORFi2mcfFiFAClS+dmxYreFCnib3Ky7EHFV0RERNJXxEnbGrx7f4DEuORxV0946Fmo9Sb4FjYvn0kuXLhO48ZTuHrV9kdA5cpBLF/ei3z5cpqcLPtwMTuAiIiIZBHXjsKfz8APpWzTGm6WXjcfqPEaPHMcmn6RLUsvQP78OXnppdoA1KlTiNWrn1LpzWQ64isiIiIPJuwQbB4N//4MRmLyuHtOqPoi1HwNfHTSFsC77zamQAFfevasjK+vp9lxsh0VXxEREbk/l/fZCu/B2WBYk8c9/KD6y1D9VfDO3idsXboURVBQjqSPLRYLAwbUNDFR9qapDiIiIpI2l3bD4i7wU2U4MDO59HrlgvrvQ/+T8PAH2b70Tp68kxIlPmfVquNmR5H/pyO+IiIikjoXt8PGD+DoQvtx77xQ43Wo+gJ4+pmTzcF88cVmXnllKQDt2s1k587ntFyZA1DxFRERkbs7twk2fQDHl9iP++SDWm9AlQHgniPl+2YzhmHw4YfrePvtVUljTz9djZIlc5uYSm5S8RUREZGUnVlrO8J7aoX9eM5CtjV4Kz8D7t7mZHNAhmEwdOhKPv54Q9LYiBENee+9xlgsugyxI1DxFRERkWSGAadXwcb34cwa+32+RWxXWavYF9y0IsGtrFaDgQOX8O2325LGPvkkhDfeeNjEVPJfKr4iIiJiK7wn/rRNaTj3t/0+/xJQZxhU6AWuHubkc2AJCVb69VvItGn/AGCxwDffPKrVGxyQiq+IiEh2Zhhw7Ddb4b2w1X5frjJQZziU7wEuqgx30qvXL8yatRcAV1cLU6Z04MknHzI5laREr2IREZHsyLDCkV9tc3hDd9nvy1MB6o6AMl3AxdWMdE6lc+fyzJmzDzc3F2bP7kyHDuXMjiR3oOIrIiKSnVgT4dA82DwKLu+13xdYBeq+DaU7gkVL/adWp04VmDKlPfnz56R585Jmx5G7UPEVERHJDqwJtotNbBoNVw/a78tX03aEt2Q72wRVuauYmAS8vOwrVK9eVUxKI2mhP+dERESyssR42PMjTC4Hf/S2L70F6kHHJdBzC5R6TKU3Fc6di6RGje/55put976xOBwd8RUREcmKEmJh3xTY8hFEnLDfV7gh1B0JRZqq7KbB8eNXCQmZxrFjV3nxxSXkzu1N9+6VzI4laaDiKyIikpXER8OeSbD1E7h+xn5fkWa2KQ3BjczJ5sQOHLhMSMhUzp6NBKB48QBq1y5kcipJKxVfERGRrCA+CnZ/B9vGQtQF+33FW9sKb8F65mRzcjt3nqdFi+lcvnwDgPLl87J8eS8KFfIzOZmklYqviIiIM4uLhF3fwLZPITrUfl/Jx2yrNOSvZU62LODvv0/Tps3PhIfHAlCtWn7+/PNJAgNzmJxM7oeKr4iIiDOKDYedX8L2cRATZr+vdCdb4Q2qakq0rGLFimO0bz+LGzfiAahfP5jff+9BQICXycnkfqn4ioiIOJPoMNgxHnZ+YSu/SSxQthvUHQ55dcLVg/r990N07DiHuLhEAEJCSvDrr93IkUOXbHZmKr4iIiLO4EYobP8Mdn4F8deTxy2utksK1x4GeXTFsPRSuLAfPj7uxMUl0r59WWbN6nzb2r3ifPQdFBERcWRRF2Dr/2D3t5BwI3ncxQ0q9IE6QyFAVwtLb1Wq5OePP3ry4487+frrNri769LNWYGKr4iIiCOKPGtbkmzP95AQkzzu6gGV+kGtIeBfzLR4WZFhGFhuWde4bt3C1K1b2MREkt5UfEVERBxJxEnY8jHs/QES45LH3bygcn+o9Sb4qoylJ8MweO+9NZw9G8H337ezK7+Staj4ioiIOIJrR2HzGNj/E1gTksfdfKDKAKg5GHIWMC9fFmUYBq+/voxx4zYB4OvryWeftTQ5lWQUFV8REREzhR2EzR/Cvz+DkZg87p4Tqg2EGq+BT6B5+bKwxEQrAwb8xqRJO5PGihTxNzGRZDQVXxERETNc3gebR8PB2WBYk8c9/KD6K7Z/3nnMy5fFxccn0qvXL8yevQ8AiwUmTmzH009XNzmZZCQVXxERkcx0aTdsHgWH5gNG8rhXLqg+CKq9BF4BZqXLFmJiEujSZS6//XYIADc3F6ZPf5xu3bT+cVan4isiIpIZLmyDTR/A0UX24955ocbrUPUF8PQzJ1s2cv16HO3bz+Kvv44D4Onpyvz5XXn00TImJ5PMoOIrIiKSkc5ttBXe43/Yj/vks63QUOU5cM9hTrZs5urVaNq0mcGmTWcAyJnTg0WLutOkSXGTk0lmUfEVERHJCGfWwsYP4NQK+/GchWxr8FZ+Bty9zcmWTSUkWLl2zbYmckCAF0uX9qROHS0Nl52o+IqIiKQXw4BTf9mO8J5ZY7/Pt4jtKmsV+4Kbpzn5srnAwBwsX96LLl3m8t13bXnooXxmR5JMpuIrIiLyoAwDTvxpK7zn/rbf518C6gyDCr1sV10TUxUu7Mfff/fTRSqyKRezA4iIiDgtw4Cji2FGHVjQ2r705ioDradCv4NQ+WmVXhPs3XuJLl3mEhUVZzeu0pt96YiviIhIWhlWOPwLbBoFobvs9+WpAHVHQJku4OJqSjyBbdvO0bLldMLCoomIiGXRou54eqr2ZHd6BYiIiKSWNREOzbUV3iv77PcFVrEV3tKPg0VvqJpp7dqTtG07g8hI25Heq1ejuXEjXsVXVHxFRETuyZoAB2bCptFw9aD9vnw1bYW3ZDvb5b/EVEuXHqFjx9lERycA0LBhURYvfgI/P51QKCq+IiIid5YYD/unwZYP4dpR+30F6kG9EVCslQqvg5g/fz9PPDGf+HjbJaBbtSrF/Pld8fFxNzmZOAoVXxERkf9KiIV9k2HLRxBx0n5f4YZQdyQUaarC60B++mkX/fotwmq1XQa6c+cK/PxzRzw8NM9akqn4ioiI3BQfDXsmwdaP4fpZ+31FmtmmNAQ3Mieb3NHXX29h4MDkK+M99VRVJk5sh5ub5lqLPRVfERGR+CjY/R1sGwtRF+z3FW9tK7wF65mTTe7KajVYsuRI0scvvVSb8eNb4eKio/FyOxVfERHJvuIiYdc3sO1TiA6131fyMaj7NuSvZU42SRUXFwvz5nWhTZsZ1K9fmFGjmmqdXrkjFV8REcl+YsNh55ewfRzEhNnvK93JVniDqpoSTdLO29udP/98UvN55Z5Mn/zy9ddfU6xYMby8vKhTpw5btmy56+3Hjx9P2bJl8fb2Jjg4mEGDBhETE5NJaUVExKlFh8GGkTCxKGwYcUvptUDZ7tBnDzw2T6XXgSUkWBkyZDmnToXbjav0SmqYesR39uzZvPbaa0yYMIE6deowfvx4WrZsycGDBwkKCrrt9jNmzOCtt97ixx9/pH79+hw6dIinnnoKi8XCZ599ZsJnICIiTuFGKGz/DHZ+BfHXk8ctrlC+J9QZBrnLmpdPUiUuLpGePRcwb95+fvnlAOvW9SVfvpxmxxInYjEMwzDryevUqUOtWrX46quvALBarQQHB/PSSy/x1ltv3Xb7gQMH8u+//7Jy5cqksddff53Nmzezfv36VD1nREQE/v7+hIeH4+fnlz6fiIiIOKaoC7D1f7D7W0i4kTzu4gYV+kCdoRBQ0rx8kmo3bsTTqdMcli61ncjm7u7C4sVP0LJlKZOTSUbIqL5m2lSHuLg4tm/fTkhISHIYFxdCQkLYuHFjivepX78+27dvT5oOcezYMZYsWUKbNm3u+DyxsbFERETY/RMRkSwu8iz89QpMKg7bP00uva4eUGUA9DsMLSep9DqJiIhYWrWanlR6vb3dVHrlvpg21eHy5cskJiaSL18+u/F8+fJx4MCBFO/To0cPLl++zCOPPIJhGCQkJDBgwACGDRt2x+cZM2YM7733XrpmFxERBxVx0nbRib0/QmJc8ribF1TuD7XeBN/C5uWTNLty5QatWv3Mtm3nAPDz8+S3356gQYOiJicTZ2T6yW1psXr1aj788EO++eYbduzYwYIFC/j999/54IMP7nifoUOHEh4envTv9OnTmZhYREQyxbWj8Ocz8EMp2D0hufS6+UCN1+GZ49D0C5VeJ3P+fCSNGk1JKr158njz11+9VXrlvpl2xDdv3ry4urpy8eJFu/GLFy+SP3/+FO8zYsQIevXqxTPPPANA5cqViYqK4tlnn2X48OG4uNze4z09PfH09Ez/T0BERMwXdhA2fwj//gxGYvK4e06oNhBqvAY+geblk/t24sQ1QkKmcvToVQAKFMjJ8uW9qFjx9pPfRVLLtCO+Hh4e1KhRw+5ENavVysqVK6lXL+Wr49y4ceO2cuvqalu+xMRz9EREJLNd3ge/94ApFWD/1OTS6+lvu8pa/xPQYIxKrxObOXNPUuktWtSfdev6qvTKAzN1ObPXXnuNPn36ULNmTWrXrs348eOJioqib9++APTu3ZtChQoxZswYANq1a8dnn31GtWrVqFOnDkeOHGHEiBG0a9cuqQCLiEgWdmk3bB4Fh+bZj3vlguqDoNpL4BVgSjRJX2+99QinToWzatUJVqzoTeHCWolJHpypxbdbt26EhoYycuRILly4QNWqVVm6dGnSCW+nTp2yO8L79ttvY7FYePvttzl79iyBgYG0a9eO0aNHm/UpiIhIZriwDTZ9AEcX2Y9754Wag6HqC+Dha042yRAWi4Wvv36U8PAYcuXyNjuOZBGmruNrBq3jKyLiRM5ttBXe43/Yj/vks63QUOU5cM9hTjZJV3/9dRwPD1ceeaSI2VHEAWRUXzP1iK+IiEiKzqyFjR/AqRX24zkLQa0hUPkZcNdRwKzit98O0bnzHDw93Vi1qg/VqxcwO5JkUSq+IiLiGAwDTv1lO8J7Zo39Pt8itqusVewLblqpJyuZPXsvTz75CwkJVmJjE/nii81MmdLB7FiSRan4ioiIuQwDTvwJG9+H8/+5cqd/CagzDCr0sl11TbKUH37YQf/+i7k56bJ790pMnNjO3FCSpan4ioiIOQwDji62HeG9uM1+X64yUPdtKPcEuOh/VVnR+PGbGDToz6SPn3mmGhMmtMXV1amurSVORr9NREQkcxlWOPwLbBoFobvs9+WpYFuHt0wXcNEylVmRYRiMGrWWkSNXJ4299lpd/ve/FlgsFvOCSbag4isiIpnDmgiH5toK75V99vsCq9gKb+nHwaIjflmVYRi8+eZy/ve/5Ckt777biJEjG6n0SqZQ8RURkYxlTYADM2HTaLh60H5fvpq2wluyHaj4ZHlbt57j00+TS++nn7bgtddSvlqrSEZQ8RURkYyRGA/7p8GWD+HaUft9BepBvZFQrKUKbzZSu3YhJkxoy/PP/8633z7Ks8/WMDuSZDMqviIikr4SYmHfZNjyEUSctN9XuCHUHQlFmqrwZlPPPluDRo2KUrZsXrOjSDak4isiIukjPhr2TIKtH8P1s/b7ioRAvRG24ivZRlRUHGvWnKRNm9J24yq9YhYVXxEReTDxUbD7O9g2FqIu2O8r3to2h7eg5nFmN9euxfDoozPYuPE0M2d2olu3SmZHElHxFRGR+xQXCbu+gW2fQnSo/b6Sj9kKb/6a5mQTU4WGRtGixXR27bL9IfTSS3/Qpk1pfH111T0xl4qviIikTcw12Pkl7BgPMWH2+0p3sl14IqiqCcHEEZw5E0Hz5tM4cOAyAIGBPixb1kulVxyCiq+IiKROdJit7O78AmLDb9lhgXLdoc5wyFvRrHTiAI4du0qzZlM5ceIaAIUK+bJiRW/KldOcXnEMKr4iInJ3N0Jh+2ew8yuIv548bnGF8j2hzjDIXda8fOIQ9u8PJSRkKufP214jJUrkYuXK3hQrFmBuMJFbqPiKiEjKoi7A1v/B7m8h4UbyuIsbVOgDdYZCQEnz8onD2LHjPC1aTOPKlWgAKlQIZPnyXhQs6GtyMhF7Kr4iImIv8ixs/QT2fA8JMcnjrh5QqR/Ufgv8ipqXTxxKbGwCHTrMSiq9NWoUYOnSJ8mb18fkZCK3U/EVERGbiJO2i07s/RES45LH3byg8rNQ6w3wLWxePnFInp5uTJ/ekVatplOjRkF+++0J/P29zI4lkiIVXxGR7O7aUdg8Bvb/BNaE5HE3H6jyPNQaDDnym5dPHF7DhkX5668+VK4cRI4cHmbHEbkjFV8Rkewq7CBs/hD+/RmMxORx95xQbSDUeA18As3LJw5ry5az1KpVEMstl52uW1fvBojjczE7gIiIZLLL++D3HjC5POyfmlx6Pf1tF53ofxIajFHplRRNmLCNunUn8fbbf5kdRSTNdMRXRCS7uLQLNo2Cw/Ptx71yQfVBUO0l8AowI5k4ibFjN/DmmysA+PDD9TRsWJSWLUuZnEok9VR8RUSyugvbYNMHcHSR/bh3Xqg5GKq+AB5adkruzDAMRo5cxahR65LGhgx5mBYttJydOBcVXxGRrOrcRlvhPf6H/bhPPqj1JlR5DtxzmJNNnIbVajBo0FK++GJL0tjo0U0ZNqyBialE7o+Kr4hIVnNmLWz8AE6tsB/PWQhqDYHKz4C7tznZxKkkJlrp338xkyfvShr74otWvPRSHfNCiTwAFV8RkazAMODUX7DpfVvxvZVfUdtFJyr2BTdPc/KJ04mLS+TJJxcwd+5+AFxcLPzww2M89VRVc4OJPAAVXxERZ2YYcOJP2Pg+nN9ov8+/BNQZDhV6gau7OfnEaQ0atDSp9Lq7uzBjRic6d65gciqRB6PiKyLijAwDji62zeG9uM1+X64yUPdtKPcEuOjXvNyfN998mEWLDnH58g0WLOhK69alzY4k8sD0G1FExJkYVjj8i21ZstBd9vvyVLQV3jJdwMXVlHiSdRQtGsCKFb24eDGKhg2Lmh1HJF2o+IqIOANrIhyaayu8V/bZ7wusYrvwROnHwaLrEsn9uXQpCj8/T7y8kqtB2bJ5KVs2r4mpRNKXiq+IiCOzJsCBmbBpNFw9aL8vX02oNxJKtIVbLh0rklanToUTEjKVsmXzsmBBV9zd9Y6BZE0qviIijigxDvZPgy1j4NpR+30F6tkKb7GWKrzywA4fvkKzZlM5fTqCw4fDePPN5Ywb18rsWCIZQsVXRMSRJMTCvsmw5SOIOGm/r3Aj25SGIk1VeCVd/PPPRVq0mMbFi1EAlC6dm0GD6pmcSiTjqPiKiDiC+GjYMwm2fgzXz9rvKxIC9UZA4YbmZJMsacuWs7RqNZ2rV2MAqFw5iOXLe5EvX06Tk4lkHBVfEREzxUfB7u9g21iIumC/r3hr2xHegjoCJ+lr9eoTtGs3k+vX4wCoU6cQS5b0JHduXdFPsjYVXxERM8RFwq5vYNv/IPqy/b6Sj9kKb/6a5mSTLG3JksN06jSHmJgEAJo0KcbChd3x9dVV/STrU/EVEclMMddg55ewYzzEhNnvK9MZ6rwNQVXMSCbZwKpVx+nQYRbx8VYAHn20NHPndsHbW1f2k+xBxVdEJDNEh9nK7s4vIDb8lh0WKNfddmnhvBXNSifZRO3ahahVqxB//32arl0rMm3a43h4aOkyyT5UfEVEMtKNUNj+Gez8CuKvJ49bXKF8T6gzDHKXNS+fZCs5cnjw++89+OqrLQwd+giurrrgiWQvKr4iIhkh6gJs/R/s/hYSbiSPu7hBhT5QZygElDQvn2QLhmFw/Xqc3fzdgAAv3n5bK4RI9qTiKyKSniLPwNZPYM9ESIhJHnf1gEr9oPZb4FfUvHySbRiGwdChK1m8+BBr1jxF3rw+ZkcSMZ2Kr4hIeog4abvoxN4fbVddu8nNCyo/C7XeAN/C5uWTbMVqNRg4cAnffrsNgFatprNx49O6FLFkeyq+IiIP4tpR2DwG9v8E1oTkcTcfqPI81BoMOfKbl0+ynYQEK/36LWTatH8A20X++vevrtIrgoqviMj9CTsImz+Ef38GIzF53D0nVBsINV4Dn0Dz8km2FBubwBNPzOeXXw4A4Opq4aefOtCz50MmJxNxDCq+IiJpcXkfbBoFB2cDRvK4pz9Uexmqvwreuc1KJ9lYVFQcHTvOYdmyowB4eLgyZ05n2rcvZ3IyEceh4isikhqXdtkK7+H59uNeuaHGIKg6ELwCzEgmQnh4DG3bzmT9+lMA+Pi48+uv3WjeXCuHiNxKxVdE5G4ubINNH8DRRfbj3oFQ83Wo+gJ4+JqTTQSIiIilWbOpbN9+HgA/P0+WLOnBww8XMTmZiONR8RURScm5jbbCe/wP+/Ec+aHmG1DlOXDPYU42kVvkzOlBlSr52L79PHnz+vDnn09SvXoBs2OJOCQVXxGRW51ZCxvfh1Mr7cdzFoJaQ6DyM+DubU42kRS4uFj4/vt2+Pi48/zztahQQSdVityJiq+IiGHAqb9g0/u24nsrv6JQeyhUfArcPFO8u0hmS0y02l1u2NXVhS+/bGNiIhHnoIt0i0j2ZRi2qQwzH4Z5Ifal178EtPgB+h22TWtQ6RUHsXPneSpW/IY9ey6aHUXE6eiIr4hkP4YBRxfb5vBe3Ga/L1dZqDscyj0BLvoVKY7l779P06bNz4SHx9K8+TTWr+9HqVJaPk8ktfRbXUSyD8MKh3+xLUsWust+X56KUPdtKNMFXHSFK3E8K1Yco337Wdy4EQ9AyZK5yZvXx+RUIs5FxVdEsj5rIhyaayu8V/bZ7wusAnVHQOnHwaLZX+KYFi06SJcuc4mLs10lMCSkBL/+2o0cOTxMTibiXFR8RSTrsibAgZmwaTRcPWi/L38tW+Et0RYsFnPyiaTCzJl76NXrFxITbVcKbN++LLNmdcbLS/8LF0kr/dSISNaTGAf7p8GWMXDtqP2+AvWg3kgo1lKFVxze999vZ8CA3zD+/+rYPXpUZsqU9ri7azqOyP1Q8RWRrCMhFvZNhi0fQcRJ+32FG9mO8BZpqsIrTuGzzzby+uvLkj5+7rkafPPNo7i46PUrcr9UfEXE+cVHw56JsPUTuH7Wfl+REKg3Ago3NCebyH1yd0+ecz54cD0++aQ5Fv3RJvJAVHxFxHnFR8HuCbB1LNz4z5qmxVvbjvAWrGdONpEH9NJLdYiMjCMx0crbbzdU6RVJByq+IuJ84iJh59ew/VOIvmy/r+RjtsKbv6Y52UTS0bBhDcyOIJKlqPiKiPOIuQY7v4Qd4yEm7JYdFijTCeq8DUFVTAoncv/i4xPp128RHTuW4/HHy5sdRyTLUvEVEccXHWYruzs+h7iIW3ZYoFx3qDMc8lY0K53IA4mJSaBr17ksXnyIOXP2sXjxE7RoUdLsWCJZkoqviDiuG6Gw/TPY+RXEX08et7hC+Z5QZxjkLmtePpEHdP16HO3bz+Kvv44DtgVH4uMTTU4lknWp+IqI47l+Hrb9z3biWsKN5HEXN6jQB+oMhQAdERPndvVqNG3azGDTpjMA5MzpwaJF3WnSpLjJyUSyLhVfEXEckWdsS5LtmQgJMcnjrh5Q6WmoPQT8ipqXTySdXLoURYsW09i927YaSUCAF0uX9qROncImJxPJ2lR8RcR8ESdtF53Y+6Ptqms3uXlB5Weh1pvgW8i8fCLp6PTpcEJCpnHo0BUAgoJysHx5Lx56KJ/JyUSyPhVfETHPtaOw+UPYPxWsCcnjbj5Q5XmoNRhy5Dcvn0g6O3IkjJCQqZw8GQ5AcLAfK1b0pkyZPCYnE8keVHxFJPOFHYTNo+HfGWDcciKPe06o9hLUGAQ+geblE8kgp06Fc/687UTNUqVys2JFL4oWDTA3lEg2ouIrIpnn8j7YNAoOzgaM5HFPf6j2ClR/BbxzmxZPJKM1bVqcuXO78M47q1mypAcFCviaHUkkW7EYhmHc+2ZZR0REBP7+/oSHh+Pn52d2HJHs4dIuW+E9PN9+3Cu37ehu1YHgFWBGMhFTJCZacXV1MTuGiMPKqL6mI74iknEubINNH8DRRfbj3oFQ83Wo+gJ46IiXZF1Llx5h587zDB1qf+lhlV4Rc6j4ikj6O7fRVniP/2E/niM/1HwDqjwH7jnMySaSSebP388TT8wnPt6Kl5cbgwbVMzuSSLan4isi6ef0GlvhPbXSfjxnIag1BCo/A+7e5mQTyUQ//bSLfv0WYbXaZhP+/fcZXn3VwGKxmJxMJHtT8RWRB2MYcOov2PQ+nFlrv8+vKNQeChWfAjdPU+KJZLavv97CwIHJ73b06VOFSZMeU+kVcQAqviJyfwwDTiyFjR/A+Y32+wJKQu1hUKEXuLqbk0/EBB99tJ6hQ5Pf8Rg4sBaff94aFxeVXhFHoOIrImljGHB0sW1Kw8Vt9vtylYW6w6HcE+CiXy+SfRiGwfDhfzFmzPqksWHDHmHUqKY60iviQPR/JhFJHcMKhxfYliUL3W2/L09FqDsCynQGF1dz8omYxGo1ePnlP/j6661JY2PGNOOttx4xMZWIpETFV0TuzpoIB+fYrrR2ZZ/9vsAqUG8klOoAFi3PJNnTxYvXWbDg36SPv/66DS+8UMvERCJyJyq+IpIya4LtksKbP4SrB+335a9lO8Jboi3obVzJ5goU8GX58l40azaVTz5pTu/eVcyOJCJ3oOIrIvYS42D/NFvhDT9mv69gfVvhLdZShVfkFhUrBnH48Ev4+mr1EhFHpuIrIjYJsbBvMmweA5Gn7PcVbmSb0hDcRIVXsr2IiFjGj9/EsGENcHNLnuKj0ivi+FR8RbK7+GjYMxG2fgLXz9rvKxIC9UZA4YbmZBNxMFeu3KBVq5/Ztu0cx45d5ccf22upMhEnouIrkl3FR8HuCbB1LNy4aL+veBuo+zYU1CVWRW46fz6S5s2nsW9fKAC//XaIkyevUbx4LpOTiUhqqfiKZDdxkbDza9j+KURftt9Xsr2t8OavaU42EQd14sQ1QkKmcvToVQAKFMjJ8uW9VHpFnIyKr0h2EXMNdn4JO8ZDTNgtOyxQphPUeRuCdDa6yH8dPHiZkJBpnDkTAUDRov6sXNmbkiVzm5xMRNJKxVckq4u+Yiu7O76AuIjkcYsLlO0GdYZD3oqmxRNxZLt3X6BFi+lcuhQFQNmyeVixojeFC/uZnExE7oeKr0hWdSMUtn0Ku76G+OvJ4xZXqPAk1B4GucuYl0/EwW3adIbWrX/m2rUYAKpUyceyZb0ICsphcjIRuV8qviJZzfXzsO1/thPXEm4kj7u4QcWnoPZQCChhWjwRZ2AYBsOGrUwqvfXqFeb333uQK5e3yclE5EGo+IpkFZFnbEuS/fM9JMYmj7t6QKWnofYQ8CtqXj4RJ2KxWJg7twuNG/9Evnw5+PXX7uTM6WF2LBF5QCq+Is4u4iRs+Qj2/mi76tpNbl5Q+Vmo9Sb4FjIvn4iTypPHh5Ure+Pn54mXl/53KZIV6CdZxFldO2q7rPD+qWBNSB5384GqL0DN1yFHfvPyiTiZefP206xZcbvpDJrPK5K1qPiKOJuwg7B5NPw7A4zE5HH3nFDtJagxCHwCzcsn4oTGj9/EoEF/UqdOIZYv76XLD4tkUSq+Is7i8j7YNAoOzgaM5HFPf6j2ClR/Bby1rqhIWhiGwahRaxk5cjUAmzef5eef9zBggC7iIpIVqfiKOLpLu2yF9/B8+3Gv3Laju9VespVfEUkTwzB4883l/O9/G5PG3n23Ec89V8PEVCKSkR6o+MbExODl5ZVeWUTkVhe2wsYP4Nhi+3HvQNv83aovgIevOdlEnFxiopUXX1zCd99tTxr79NMWvPZaPRNTiUhGc0nrHaxWKx988AGFChUiZ86cHDt2DIARI0bwww8/pHtAkWzn7N8wvzX8XNu+9ObID40/g/7HbUuTqfSK3Jf4+ER69/41qfRaLPD9921VekWygTQX31GjRjFlyhQ++eQTPDyS1zSsVKkSkyZNStdwItnK6TUwNwRmPQwnliaP5ywETb+Ep4/Zpja46yxzkfsVE5NAly5zmTFjDwBubi78/HNH+vfX9AaR7CDNUx2mTp3K999/T7NmzRgwYEDSeJUqVThw4EC6hhPJ8gwDTq2ETR/AmbX2+/yK2q6yVvEpcNMZ5iLp4csvN7Nw4UEAPD1dmTu3C+3alTU5lYhkljQX37Nnz1KqVKnbxq1WK/Hx8ekSSiTLMwzbUd2N78P5Tfb7AkpC7WFQoRe4upuTTySLevXVumzYcJoVK46xcGF3mjXT5btFspM0F98KFSqwbt06iha1v/TpvHnzqFatWroFE8mSDAOOLrYd4b24zX5frrJQ920o1x1ctOCKSEZwd3dl1qzOHDx4mSpVdIEXkewmzf93HTlyJH369OHs2bNYrVYWLFjAwYMHmTp1Kr/99ltGZBRxfoYVDi+wLUsWutt+X56KUHcElOkMLq7m5BPJos6ejSAyMo5y5fImjXl5uan0imRTFsMwjHvfzN66det4//332b17N9evX6d69eqMHDmSFi1aZETGdBUREYG/vz/h4eH4+fmZHUeyOmsiHJxju9LalX32+wKrQr0RUKoDWNJ8nqmI3MOxY1dp1mwq8fGJrFvXl+LFc5kdSURSKaP62n0VX2em4iuZwppgu6Tw5tFw9ZD9vvy1bEd4S7S1raMkIulu//5QQkKmcv78dQCaNy/BsmW9TE4lIqmVUX0tzYeZSpQowZUrV24bv3btGiVK6CQByeYS42DPD/BjWVjax770FqwPnZZCj81Qsp1Kr0gG2bHjPA0bTk4qvRUqBDJlSgdzQ4mIQ0jzHN8TJ06QmJh423hsbCxnz55Nl1AiTichFvb+CFs+gshT9vsKN4J6IyG4icquSAZbv/4Ujz46g4iIWABq1CjA0qVPkjevj8nJRMQRpLr4Llq0KGn7zz//xN/fP+njxMREVq5cSbFixdI1nIjDi4+GPRNh6ydw/T9/+BVtbpvSULiBOdlEspnly4/Svv0soqMTAHjkkSL89tsT+Pt7mZxMRBxFqotvhw4dALBYLPTp08dun7u7O8WKFePTTz9N13AiDis+CnZPgK1j4cZF+33F29gKb8G65mQTyYZ+/fUA3brNIy7O9o5kixYl+eWXbvj4aC1sEUmW6jm+VqsVq9VKkSJFuHTpUtLHVquV2NhYDh48SNu2bdMc4Ouvv6ZYsWJ4eXlRp04dtmzZctfbX7t2jRdffJECBQrg6elJmTJlWLJkSZqfV+S+xEXC5o9gYjFYM9i+9JZsDz23QsffVXpFMtG//4bSufOcpNLbsWN5Fi3qrtIrIrdJ8xzf48ePp9uTz549m9dee40JEyZQp04dxo8fT8uWLTl48CBBQUG33T4uLo7mzZsTFBTEvHnzKFSoECdPniQgICDdMomkKOYa7PwSdoyDmKu37LBAmU5Q520IqmJWOpFsrXz5QIYNa8AHH6ylV6+H+PHH9ri5aYlAEbndfS1nFhUVxZo1azh16hRxcXF2+15++eVUP06dOnWoVasWX331FWA7qhwcHMxLL73EW2+9ddvtJ0yYwNixYzlw4ADu7vf3l7yWM5M0ib4CO8bDji8gLiJ53OICZbtD3eGQp4Jp8UTExjAMFiz4l8cfL4+Li04iFXF2DrOO786dO2nTpg03btwgKiqK3Llzc/nyZXx8fAgKCuLYsWOpepy4uDh8fHyYN29e0vxhgD59+nDt2jUWLlx4233atGlD7ty58fHxYeHChQQGBtKjRw+GDBmCq2vKV7yKjY0lNjY26eOIiAiCg4NVfOXublyCbZ/Brq8h/nryuMUVKjwJtYdB7jLm5RPJxgzD4NChK5Qtm/feNxYRp+Qw6/gOGjSIdu3acfXqVby9vdm0aRMnT56kRo0a/O9//0v141y+fJnExETy5ctnN54vXz4uXLiQ4n2OHTvGvHnzSExMZMmSJYwYMYJPP/2UUaNG3fF5xowZg7+/f9K/4ODgVGeUbOj6eVj9um0O79aPk0uvixtUfgb6HYJWU1R6RUxitRoMGvQnVat+x+rVJ8yOIyJOJs3Fd9euXbz++uu4uLjg6upKbGwswcHBfPLJJwwbNiwjMiaxWq0EBQXx/fffU6NGDbp168bw4cOZMGHCHe8zdOhQwsPDk/6dPn06QzOKk4o8A3+9DJOKw/bPICHaNu7qAVWeh6ePQIuJEKCLtIiYJTHRSv/+i/j8883ExCTQvv0sQkOjzI4lIk4kzSe3ubu74+Ji68tBQUGcOnWK8uXL4+/vn6ZSmTdvXlxdXbl40X4pqIsXL5I/f/4U71OgQAHc3d3tpjWUL1+eCxcuEBcXh4eHx2338fT0xNPTM9W5JJsJP2G76MS+ybarrt3k5gWVn4Vab4JvIdPiiYhNXFwivXr9wpw5+wBwcbHw+eetCAzMYXIyEXEmaT7iW61aNbZu3QpAo0aNGDlyJD///DOvvvoqlSpVSvXjeHh4UKNGDVauXJk0ZrVaWblyJfXq1UvxPg8//DBHjhzBarUmjR06dIgCBQqkWHpF7ujqEfjzafixNPzzXXLpdfOBmoPhmePQ9HOVXhEHEB0dT8eOs5NKr7u7C7Nnd+app6qaG0xEnE6ai++HH35IgQIFABg9ejS5cuXi+eefJzQ0lO+++y5Nj/Xaa68xceJEfvrpJ/7991+ef/55oqKi6Nu3LwC9e/dm6NChSbd//vnnCQsL45VXXuHQoUP8/vvvfPjhh7z44otp/TQkuwo7CH/0hsllbZcYttqu8ISHL9QeCv1PQKOxkCPldx1EJHNFRsbSps0Mfv/9MABeXm4sXNidzp21moqIpF2apzrUrFkzaTsoKIilS5fe95N369aN0NBQRo4cyYULF6hatSpLly5NOuHt1KlTSdMqAIKDg/nzzz8ZNGgQDz30EIUKFeKVV15hyJAh951BsonLe2HTaDg4G7hlIRNPf6j2ClR/BbxzmxZPRG4XFhZN69Y/s2WL7XLgvr4eLF78BI0aFTM3mIg4rftaxzclO3bsYOTIkfz222/p8XAZRuv4ZjOXdsGmD+DwAvtxr9xQYxBUe8lWfkXEoSQmWqlb9we2bTsHQK5cXvz555PUqqXpRyLZgUMsZ/bnn38yePBghg0blrRe74EDB+jQoQO1atWym3srYqoLW+GXx2BaNfvS6x0IDT62TWmo+7ZKr4iDcnV1YciQh3FxsZAvXw7WrHlKpVdEHliqpzr88MMP9O/fn9y5c3P16lUmTZrEZ599xksvvUS3bt3Yu3cv5cuXz8isIvd29m/bEd4T/5mCkyO/bYWGh54Fd50FLuIMOneuwM8/d6RGjQKULp3H7DgikgWkeqrDQw89RK9evXjjjTeYP38+Xbp0oW7dusyZM4fChQtndM50o6kOWdTpNbbCe2ql/XjOwlB7CFR6Gty9zckmIqly5coN8uTxMTuGiDgA06c6HD16lC5dugDQsWNH3NzcGDt2rFOVXsliDANOroDZjWBOY/vS61cUQibYLjxRbaBKr4iD27LlLGXKfMU332w1O4qIZGGpnuoQHR2Nj4/tL3GLxYKnp2fSsmYimcowbFMZNr4P5zfZ7wsoCXWGQ/knwdXdnHwikiarV5+gXbuZXL8ex4svLiE42I927cqaHUtEsqA0LWc2adIkcubMCUBCQgJTpkwhb968drd5+eWX0y+dyK0MA44ugk2j4OI2+325ytpOVivXHVzSvEqfiJhkyZLDdOo0h5gY25rajRsXo3HjYuaGEpEsK9VzfIsVK4bFYrn7g1ksSas9OCrN8XVChtW2MsOmURC6235f3kpQ520o0xlcXFO+v4g4pLlz99GjxwISEmwrArVpU5p587rg7a13a0Syu4zqa6k+NHbixIl0e1KRVLEmwsE5sHk0XNlnvy+wKtQbAaU6gCXNFyAUEZNNnryTZ55ZjNVqO/bSpUsFpk/viIeH/oAVkYyj94TF8VgT4N8ZtsJ79ZD9vvy1oO4IKNEW7vEOhIg4pi++2MwrryQvOdivX1W+/74drq76I1ZEMpaKrziOxDjYPw02fwjh/5kyU7A+1BsJRVuo8Io4sfHjNzFo0J9JH7/8cm3GjWuFi4t+rkUk46n4ivkSYmHvj7DlI4g8Zb8vuLHtCG9wExVekSygSZNiBAR4ce1aDCNGNOS99xrf8/wREZH0ouIr5omPhj0TYesncP2s/b6izW2Ft3ADc7KJSIaoUiU/S5b0YNOmMwwaVM/sOCKSzaj4SuaLj4LdE2DrWLhx0X5f8Ta2wluwrjnZRCRdJSRYsViwm79br14w9eoFm5hKRLKr+yq+R48eZfLkyRw9epTPP/+coKAg/vjjD4oUKULFihXTO6NkFXGRsPNr2P4pRF+231eyvW2Vhnw1zMkmIukuNjaB7t3nExjow3fftdWUBhExXZpPoV2zZg2VK1dm8+bNLFiwgOvXrwOwe/du3nnnnXQPKFlAzDXbVdYmFoX1Q28pvRbb+ru9dkGHX1V6RbKQqKg42rWbya+/HmDixB0MHbry3ncSEclgaT7i+9ZbbzFq1Chee+01fH19k8abNm3KV199la7hxMlFX4Ed42HHFxAXkTxucYGy3aHucMhTwbR4IpIxwsNjePTRGWzYcBoAHx93mjUrbnIqEZH7KL579uxhxowZt40HBQVx+fLlFO4h2c6NS7DtM9j1NcRfTx63uEKFJ6H2MMhdxrx8IpJhLl++QcuW09mx4zwAfn6eLFnSg4cfLmJyMhGR+yi+AQEBnD9/nuLF7f9637lzJ4UKFUq3YOKErp+Hbf+D3d9CQnTyuIsbVHwKag+FgBKmxRORjHXuXCTNm09j//5QAPLm9eHPP5+kevUCJicTEbFJc/Ht3r07Q4YMYe7cuVgsFqxWKxs2bGDw4MH07t07IzKKo4s8A1s+ti1NlhibPO7qAZWehtpDwK+oeflEJMMdP36VkJBpHDt2FYCCBX1ZsaIX5csHmpxMRCRZmovvhx9+yIsvvkhwcDCJiYlUqFCBxMREevTowdtvv50RGcVRxUXC2iGw9wfbVdducvOCh56Dmm+Ar94FEMnqDh26QtOmP3H2bCQAxYsHsGJFb0qUyGVyMhERe2kuvh4eHkycOJERI0awd+9erl+/TrVq1ShdunRG5BNHtuVj27SGm9x8oOoLUPN1yJHfvFwikqkCArzImdMDgPLl87J8eS8KFfIzOZWIyO3SXHzXr1/PI488QpEiRShSRCcrZGvH/0jerj0UagwCH72tKZLdBAXlYMWK3rz88h98911bAgNzmB1JRCRFFsMwjLTcwcPDg0KFCvHEE0/w5JNPUqGCcy1HFRERgb+/P+Hh4fj56YjEfYsOg2/yAgYEVoXeO81OJCKZyDAMXZBCRDJMRvW1NF/A4ty5c7z++uusWbOGSpUqUbVqVcaOHcuZM2fSLZQ4gTNrgP//m6lIU1OjiEjmWrToIC1bTufGjXizo4iIpEmai2/evHkZOHAgGzZs4OjRo3Tp0oWffvqJYsWK0bSpClC2ceqv5G0VX5FsY+bMPXTsOJvly4/x+OOziY1NMDuSiEiqpbn43qp48eK89dZbfPTRR1SuXJk1a9akVy5xdKf/v/haXKFQA3OziEim+P777fTsuYDERNu7PYGBPri4aLqDiDiP+y6+GzZs4IUXXqBAgQL06NGDSpUq8fvvv6dnNnFUURfgyn7bdv5a4Km50iJZ3aef/s1zz/3GzbNCnnuuBlOnPo67u6u5wURE0iDNqzoMHTqUWbNmce7cOZo3b87nn39O+/bt8fHxyYh84ohOrUre1jQHkSzNMAzee28N772X/I7e4MH1+OST5jq5TUScTpqL79q1a3njjTfo2rUrefPmzYhM4uhO3zK/N1jFVySrMgyD119fxrhxm5LG3n+/MW+/3VClV0ScUpqL74YNGzIihziTmye2uXpAwfrmZhGRDJGYaGXAgN+YNCl5qcJx41ry6qt1TUwlIvJgUlV8Fy1aROvWrXF3d2fRokV3ve1jjz2WLsHEQYWfgPBjtu2C9cHd29Q4IpIxEhONpEsQWywwcWI7nn66usmpREQeTKqKb4cOHbhw4QJBQUF06NDhjrezWCwkJiamVzZxRKdvmd+raQ4iWZaHhyvz53flscdm8cwz1ejWrZLZkUREHliqiq/Vak1xW7Ihrd8rkm14e7uzbNmTms8rIllGmpczmzp1KrGxsbeNx8XFMXXq1HQJJQ7KMJJPbHPPYVvKTESyhKtXo+nadS6nT4fbjav0ikhWkubi27dvX8LDw28bj4yMpG/fvukSShzU1UNw/Zxtu1AD28ltIuL0Ll68TuPGPzF37n5CQqZx8eJ1syOJiGSINBdfwzBSPAJw5swZ/P390yWUOChNcxDJck6fDqdhwyn8889FAK5diyE09IbJqUREMkaqlzOrVq0aFosFi8VCs2bNcHNLvmtiYiLHjx+nVatWGRJSHMRpFV+RrOTIkTBCQqZy8qTtXbzgYD9WrOhNmTJ5TE4mIpIxUl18b67msGvXLlq2bEnOnDmT9nl4eFCsWDE6deqU7gHFQRjW5Cu2eQZAYFUz04jIA9q79xLNm0/jwgXbtIZSpXKzYkUvihYNMDeYiEgGSnXxfeeddwAoVqwY3bp1w8vLK8NCiQMK3QMxV2zbwY3BxdXUOCJy/7ZtO0fLltMJC4sGoFKlIJYte5ICBXxNTiYikrHSfOW2Pn36ZEQOcXS6TLFIlrBu3UkefXQGkZFxANSqVZA//uhJnjw+JicTEcl4qSq+uXPn5tChQ+TNm5dcuXLddXmbsLCwdAsnDkQntolkCX/+eTSp9DZsWJTFi5/Az8/T5FQiIpkjVcV33Lhx+Pr6Jm1rXcdsxpoAZ9bYtn2CIE8Fc/OIyH374IMmXLlygxMnwpk/vys+Pu5mRxIRyTQWwzAMs0NkpoiICPz9/QkPD8fPz8/sOM7h/GaYUde2XbY7tJ1pbh4ReSBWq0FCghUPD83VFxHHlFF9Lc3r+O7YsYM9e/Ykfbxw4UI6dOjAsGHDiIuLS7dg4kA0zUHEaU2YsI0NG07Zjbm4WFR6RSRbSnPxfe655zh06BAAx44do1u3bvj4+DB37lzefPPNdA8oDkDFV8QpffTRep5//ncefXQGO3acNzuOiIjp0lx8Dx06RNWqVQGYO3cujRo1YsaMGUyZMoX58+endz4xW0IsnFtv2/YtAv4lzM0jIvdkGAbDhq1k6NCVAISHx7Js2VGTU4mImC/Ny5kZhoHVagVgxYoVtG3bFoDg4GAuX76cvunEfOc3QUKMbbtIU9CJjSIOzWo1eOWVP/jqq61JYx991IwhQx4xMZWIiGNIc/GtWbMmo0aNIiQkhDVr1vDtt98CcPz4cfLly5fuAcVkmuYg4jQSEqz077+YKVN2JY19/XUbXnihlnmhREQcSJqL7/jx4+nZsye//vorw4cPp1SpUgDMmzeP+vXrp3tAMZndhSuamJdDRO4qLi6Rnj0XMG/efsB2AtuUKe3p1auKyclERBxHui1nFhMTg6urK+7ujr0mpJYzS4P4KPgqwLaOb64y0O+g2YlEJAU3bsTTqdMcli49AoC7uwuzZnWmY8fyJicTEbk/GdXX0nzE96bt27fz77//AlChQgWqV6+ebqHEQZxdbyu9oGkOIg5s7dqT/PmnrfR6e7vxyy/daNmylMmpREQcT5qL76VLl+jWrRtr1qwhICAAgGvXrtGkSRNmzZpFYGBgemcUs9w6vzdYxVfEUbVqVYpvv32UN99cwW+/PUGDBkXNjiQi4pDSvJzZSy+9xPXr19m3bx9hYWGEhYWxd+9eIiIiePnllzMio5jFrvg2Ni2GiNzbc8/V5PDhl1R6RUTuIs3Fd+nSpXzzzTeUL588d6xChQp8/fXX/PHHH+kaTkwUcxUu7bBtBz4EPjqSL+IoTp68xuzZe28bDwrKYUIaERHnkeapDlarNcUT2Nzd3ZPW95Us4MxaMP7/+6lpDiIO4+DBy4SETOPcuUgsFgtdu1Y0O5KIiNNI8xHfpk2b8sorr3Du3LmksbNnzzJo0CCaNWuWruHERFq/V8Th7N59gQYNJnPmTARWq8EHH6wlIUEHHEREUivNxferr74iIiKCYsWKUbJkSUqWLEnx4sWJiIjgyy+/zIiMYoab6/daXKBwQ3OziAibNp2hceOfCA29AUCVKvlYubI3bm5p/jUuIpJtpXmqQ3BwMDt27GDlypVJy5mVL1+ekJCQdA8nJom6CJf/f/5gvprg6W9uHpFs7q+/jvPYYzOJiooHoG7dwixZ0oNcubxNTiYi4lzSVHxnz57NokWLiIuLo1mzZrz00ksZlUvMdHp18ramOYiY6rffDtG58xxiYxMBaNq0OAsXdidnTg+Tk4mIOJ9UF99vv/2WF198kdKlS+Pt7c2CBQs4evQoY8eOzch8YobTWr9XxBHMnr2XJ5/8JWkeb7t2ZZgzpwteXvd97SERkWwt1ZPDvvrqK9555x0OHjzIrl27+Omnn/jmm28yMpuY5dRK239dPaDQw+ZmEcmmrl6NZsCA35NKb/fulZg/v6tKr4jIA0h18T127Bh9+vRJ+rhHjx4kJCRw/vz5DAkmJok4CdeO2rYL1AN3H3PziGRTuXJ5s3Bhd7y83HjmmWpMn/447u6uZscSEXFqqT50EBsbS44cyYuju7i44OHhQXR0dIYEE5OcWpW8rfm9IqZq2LAo27c/S/nyebFYLGbHERFxeml6z2zEiBH4+CQfAYyLi2P06NH4+yef9f/ZZ5+lXzrJfJrfK2IKwzBYuPAg7duXtSu5FSroqokiIukl1cW3YcOGHDx40G6sfv36HDt2LOljHZFwcoaRfOEKNx8oUNvcPCLZRGKilRde+J3vv9/B8OENGDVKf3SKiGSEVBff1atXZ2AMcQhXD8P1s7btwg1sJ7eJSIaKj0/kqacWMmPGHgA+/HAdXbpUoEqV/CYnExHJenR6sCTTNAeRTBUTk0D37vNYuND2bpqbmwtTp3ZQ6RURySAqvpLs1C3FVye2iWSoqKg4OnSYzYoVtulinp6uzJ3bhXbtypqcTEQk61LxFRvDCqf/f0UHT38IqmZuHpEs7Nq1GB59dAZ//30agBw53Fm4sDvNmpUwOZmISNam4is2l/dC9GXbduHG4KL1QkUyQmhoFC1aTGfXrgsABAR4sWRJD+rVCzY5mYhI1qfiKzaa5iCSKfr1W5RUegMDfVi2rBdVq2pOr4hIZkj1ldtutW7dOp588knq1avH2bO2VQCmTZvG+vXr0zWcZCIVX5FM8eWXrSlc2I/Chf1Yu7avSq+ISCZKc/GdP38+LVu2xNvbm507dxIbGwtAeHg4H374YboHlExgTYAza2zb3oGQp6K5eUSysGLFAlixohfr1vWlXLm8ZscREclW0lx8R40axYQJE5g4cSLu7u5J4w8//DA7duxI13CSSS7ugLgI23aRpqALkYikm/37Q4mJSbAbK1s2L8WKBZgTSEQkG0tz8T148CANGza8bdzf359r166lRybJbJrmIJIhNmw4Rb16P9C161zi4xPNjiMiku2lufjmz5+fI0eO3Da+fv16SpTQUjxOSReuEEl3y5cfpUWL6URExLJ48SE++WSD2ZFERLK9NBff/v3788orr7B582YsFgvnzp3j559/ZvDgwTz//PMZkVEyUkIsnP3/kxJ9gyGgpLl5RLKAX375l7ZtZ3LjRjwALVqUZNCgeianEhGRNC9n9tZbb2G1WmnWrBk3btygYcOGeHp6MnjwYF566aWMyCgZ6cJmSIi2bWt+r8gDmz79H5566lcSEw0AHn+8HDNndsLTU6tHioiYLc2/iS0WC8OHD+eNN97gyJEjXL9+nQoVKpAzZ86MyCcZ7ZSmOYiklwkTtvHCC79j2DovvXo9xI8/tsfN7b5WjhQRkXR234cgPDw8qFChQnpmETPYFd8m5uUQcXJjx27gzTdXJH38/PM1+eqrNri46F0UERFHkebi26RJEyx3eTv8r7/+uuM+cTDxUXB+k207V2nw0yVTRe7HTz/tsiu9Q4Y8zJgxze76u1JERDJfmotv1apV7T6Oj49n165d7N27lz59+qRXLskMZzeA1XbyjaY5iNy/Tp0q8N1329m48QyjRzdl2LAGZkcSEZEUpLn4jhs3LsXxd999l+vXrz9wIMlEWr9XJF3kzOnBkiU9+f33Q/Ts+ZDZcURE5A7S7YyLJ598kh9//DG9Hk4yg936vY1NiyHibOLiEgkNjbIbCwjwUukVEXFw6VZ8N27ciJeXV3o9nGS0mGtwcbttO29l8AkyNY6Is4iOjqdDh1k0bvwTly/fMDuOiIikQZqnOnTs2NHuY8MwOH/+PNu2bWPEiBHpFkwy2Jm1YFht25rmIJIqkZGxtGs3kzVrTgLQseNs1qx5SiexiYg4iTQXX39/f7uPXVxcKFu2LO+//z4tWrRIt2CSwXSZYpE0CQuLpnXrn9my5Sxgm9f7wQd3X+VGREQcS5qKb2JiIn379qVy5crkypUrozJJZrh5YpvFBQo3NDeLiIO7cOE6LVpMY8+eSwDkyuXF0qVPUrt2IZOTiYhIWqRpjq+rqystWrTg2rVrGRRHMsWNS3B5j207Xw3wCjA1jogjO3UqnIYNJyeV3nz5crBmzVMqvSIiTijNJ7dVqlSJY8eOZUQWySynVydva5qDyB0dPnyFRx75kcOHwwAoUsSfdev6UrlyPpOTiYjI/Uhz8R01ahSDBw/mt99+4/z580RERNj9Eyeg9XtF7unMmQgaNJjM6dO232ulS+dm3bq+lC6dx+RkIiJyv1JdfN9//32ioqJo06YNu3fv5rHHHqNw4cLkypWLXLlyERAQoHm/zuLmiW0u7lDoYXOziDioggV9ad26NACVKwexbl1fihTxv8e9RETEkaX65Lb33nuPAQMGsGrVqozMIxkt4jRcPWzbLlAX3HOYm0fEQbm4WJg0qR1Fivjxyit1yZ3b2+xIIiLygFJdfA3DAKBRo0YZFkYywelb/nDRNAcRO1FRceTI4ZH0saurC++918TERCIikp7SNMdX61VmAac1v1ckJXPn7qNUqS/Zs+ei2VFERCSDpGkd3zJlytyz/IaFhT1QIMlAhpF8YpubN+SvY24eEQcxefJOnnlmMVarQfPm09i6tT/BwZrPKyKS1aSp+L733nu3XblNnMi1oxB52rZd6BFw8zQ3j4gD+PLLzbz88tKkjx99tDQFC/qamEhERDJKmopv9+7dCQoKyqgsktF0mWKRJIZhMGbMeoYPT/65eOWVOnz2WUtcXDStS0QkK0r1HF/N780CtH6vCGArvUOHrrQrvSNGNGTcOJVeEZGsLM2rOoiTunV+r4cf5Ktubh4Rk1itBgMHLuHbb7cljX3ySQhvvKE1rUVEsrpUF1+r1ZqROSSjXdkH0aG27cKNwCVNs1xEsgTDMOjbdyFTp+4GwGKBb755lAEDapqcTEREMkOaL1ksTkrTHESwWCzUqFEAAFdXC9OmPa7SKyKSjeiwX3ah4isCwMsv1yEqKo7y5QPp0KGc2XFERCQTqfhmB9ZEOLPatu2dF/JWMjWOSGayWo3bTlgbOrSBSWlERMRMmuqQHVzaCbHhtu3gJmDRt12yh8uXb1C//g8sXHjA7CgiIuIA1ICyg1Mrk7c1zUGyiXPnImnUaAqbN5+la9d5rFhxzOxIIiJiMocovl9//TXFihXDy8uLOnXqsGXLllTdb9asWVgsFjp06JCxAZ3dKV24QrKX48ev0qDBZPbvt61kkjevj67GJiIi5hff2bNn89prr/HOO++wY8cOqlSpQsuWLbl06dJd73fixAkGDx5Mgwaaq3dXiXFwdp1tO2dhyFXa3DwiGezAgcs0aDCZY8euAlC8eADr1vWlQoVAk5OJiIjZTC++n332Gf3796dv375UqFCBCRMm4OPjw48//njH+yQmJtKzZ0/ee+89SpQokYlpndD5zZAQbdsu0tS2cKlIFrVz53kaNJjM2bORAJQrl5d16/pSokQuk5OJiIgjMLX4xsXFsX37dkJCQpLGXFxcCAkJYePGjXe83/vvv09QUBBPP/30PZ8jNjaWiIgIu3/ZipYxk2zi779P06TJT1y+fAOAatXys3btUxQq5GdyMhERcRSmFt/Lly+TmJhIvnz57Mbz5cvHhQsXUrzP+vXr+eGHH5g4cWKqnmPMmDH4+/sn/QsODn7g3E7l9K3ze5uYl0MkA61YcYzmzacRHh4LQP36wfz1Vx8CA3OYnExERByJ6VMd0iIyMpJevXoxceJE8ubNm6r7DB06lPDw8KR/p0+fzuCUDiT+Bpz7/yPnAaXAr4i5eUQyUEKC7bLqISElWLbsSQICvExOJCIijsbUC1jkzZsXV1dXLl68aDd+8eJF8ufPf9vtjx49yokTJ2jXrl3SmNVq+5+dm5sbBw8epGTJknb38fT0xNPTMwPSO4GzG8Aab9vWNAfJwkJCSjBnTmemT9/DtGmP4+Wla/OIiMjtTD3i6+HhQY0aNVi5MnmdWavVysqVK6lXr95tty9Xrhx79uxh165dSf8ee+wxmjRpwq5du7LfNIZ7Oa1lzCT7aN++HHPndlHpFRGROzL9/xCvvfYaffr0oWbNmtSuXZvx48cTFRVF3759AejduzeFChVizJgxeHl5UamS/eV2AwICAG4bF/6zfm9j02KIpLfPPttIVFQcI0Y0MjuKiIg4EdOLb7du3QgNDWXkyJFcuHCBqlWrsnTp0qQT3k6dOoWLi1NNRXYMseFwcZttO28lyJHv7rcXcQKGYfDee2t47701APj5efLKK3VNTiUiIs7C9OILMHDgQAYOHJjivtWrV9/1vlOmTEn/QFnBmbVg2OY/a5qDZAWGYfD668sYN25T0lhkZJyJiURExNk4RPGVDKD1eyULSUy0MmDAb0yatDNpbPz4ljraKyIiaaLim1XdPLHN4gKFNQ9SnFd8fCK9e//KrFl7AdvFBydNeox+/aqZnExERJyNim9WdCMUQv+xbQdVB68AU+OI3K+YmAS6dp3L4sWHAHBzc2H69Mfp1k0ns4qISNqp+GZFp1cnb2uagzip69fjaN9+Fn/9dRwALy835s3rwqOPljE5mYiIOCsV36zotOb3ivMLDY1i//5QAHLm9GDx4ido3LiYuaFERMSpaZ2wrOjmiW0ublDoEXOziNyn4sVzsXx5L0qVys2KFb1UekVE5IHpiG9WE3kGrtrmQ1KgLrjnMDePyAOoVCmIf/99ETc3/Y0uIiIPTv83yWpOr0re1vq94kSOHAnjlVf+ICHBajeu0isiIulFR3yzGq3fK05o795LNG8+jQsXrhMeHsuPP7bHxcVidiwREclidCglKzGM5OLr5mWb6iDi4LZtO0ejRlO4cOE6ANu3nyc8PMbkVCIikhWp+GYl4ccg8pRtu+Aj4OZpbh6Re1i79iRNm/5EWFg0ALVqFWT16j7kyuVtcjIREcmKVHyzEk1zECeydOkRWrWaTmRkHAANGxZlxYre5MnjY3IyERHJqlR8sxIVX3ES8+fv57HHZhIdnQBAq1al+OOPnvj56V0KERHJOCq+WYVhJF+4wsMX8tUwN4/IHfz00y66dp1HfLxt9YZOncqzcGF3fHzcTU4mIiJZnYpvVnFlP9y4ZNsu3Mh28QoRB5OQYOWbb7ZhtRoAPPVUVWbN6oyHh6vJyUREJDtQ8c0qNM1BnICbmwtLlvSgYsVABg6sxQ8/PKZ1ekVEJNPosGBWcfqW4qsLV4gDy5PHhw0b+uHn54nForV6RUQk8+hQS1ZgTYTTq23bXnkgsLKZaUSSWK0Gn3yygatXo+3G/f29VHpFRCTTqfhmBaG7IPaabbtIE7Do2yrmS0iw0q/fQoYMWUGbNjO4fj3O7EgiIpLNqSFlBac0zUEcS1xcIt27z+Onn3YDsGXLWdavP2VyKhERye40xzcr0Ilt4kBu3IinU6c5LF16BAB3dxdmzepMq1alTE4mIiLZnYqvs0uMg7PrbNs5C0KuMubmkWwtIiKWdu1msnbtSQC8vd345ZdutGyp0isiIuZT8XV2F7ZCfJRtO7gp6IQhMcmVKzdo1epntm07B4Cfnye//fYEDRoUNTmZiIiIjYqvs9M0B3EA589H0rz5NPbtCwUgTx5v/vzzSWrUKGhyMhERkWQqvs7utIqvmG/cuE1JpbdAgZwsX96LihWDTE4lIiJiT8XXmcVHw7m/bdv+JcBPbymLOUaPbsrBg1fYvfsCK1f2pmTJ3GZHEhERuY2KrzM797ft5DbQ0V4xlbu7K7NndyYsLJqCBX3NjiMiIpIirePrzHSZYjHJ5s1nOHDgst2Yl5ebSq+IiDg0FV9nZndiWxPzcki2smrVcZo1m0pIyFSOH79qdhwREZFUU/F1VrERtqXMAPJUgBz5zc0j2cLvvx+ideufiYqK5+zZSD74YK3ZkURERFJNxddZnV0HRqJtW9McJBPMnr2XDh1mExtre921a1eGb7551ORUIiIiqafi66y0fq9koh9+2METT8wnIcEKQPfulZg/vyteXjo/VkREnIeKr7M6tfL/NyxQuJGpUSRrGz9+E888sxjDsH38zDPVmD79cdzdXc0NJiIikkYqvs7oxmUI3W3bDqoG3lozVdKfYRh88MEaBg36M2ls0KC6fP99O1xd9atDREScj/7v5YzOrE7e1jQHySBr1pxk5MjVSR+/804jPv20BRaLxbxQIiIiD0DF1xlpfq9kgsaNi/H22w0A+N//mvPuu41VekVExKnpzBRndLP4urhBoUfMzSJZ2vvvN6F169LUrx9sdhQREZEHpiO+zibyLFw9aNvOXxs8dKUsSR8xMQls3nzGbsxisaj0iohIlqHi62xOr0reLtLMvBySpVy/HkfbtjNo3Pgn1qw5YXYcERGRDKHi62w0v1fS2bVrMbRsOZ2VK48TE5NA9+7ziY6ONzuWiIhIutMcX2diGMnr97p5QYG65uYRpxcaGkWLFtPZtesCAP7+nsyf3xVvb3eTk4mIiKQ/FV9nEn4cIk/Ztgs+bCu/IvfpzJkImjefxoEDlwEIDPRh2bJeVK2a3+RkIiIiGUPF15lomoOkk6NHwwgJmcaJE9cAKFTIlxUrelOuXF5zg4mIiGQgFV9ncvqW4hus4iv3Z//+UEJCpnL+/HUASpTIxcqVvSlWLMDcYCIiIhlMxddZGEbyEV8PX8hf09w84pRu3Ii3K70VKgSyfHkvChbUsngiIpL1aVUHZxH2L9y4aNsu3NB28QqRNPLxcefzz1vh4mKhRo0CrFnzlEqviIhkG2pPzuKUpjlI+ujSpSLe3u40aFAEf3+dICkiItmHjvg6C53YJvfpyJGw28bati2j0isiItmOiq8zsCbCmdW2ba/cEPiQqXHEeUyf/g/lyn3FhAnbzI4iIiJiOhVfZxC6G2Ku2raDm4BF3za5twkTttG79y8kJhq88MLvrFt30uxIIiIiplKDcgaa5iBpNHbsBp5//ncMw/bxgAE1efjhIuaGEhERMZmKrzPQ+r2SSoZhMGLEX7z55oqksSFDHubrr9vg4mIxMZmIiIj5tKqDo0uMhzNrbds5CkDusubmEYdltRoMGrSUL77YkjT24YdNGTq0gYmpREREHIeKr6O7sBXio2zbRZqCRUft5HaJiVaefXYxP/64K2nsyy9bM3BgbfNCiYiIOBgVX0enaQ6SCi+99EdS6XVxsfDDD4/x1FNVTc0kIiLiaDTH19HpxDZJhf79q+Pv74m7uwuzZ3dW6RUREUmBjvg6svhoOPe3bdu/OPgXMzWOOK5q1QqwZElPwsNjaN26tNlxREREHJKKryM7vxESY23bmuYgtwgPjyFnTg9cXZPftKlfP9jERCIiIo5PUx0cmaY5SAouXLhOgwaTefHFJRg3F+oVERGRe9IRX0d2a/ENbmJeDnEYp06FExIylcOHw9iz5xIFC/oycmQjs2OJiIg4BRVfRxUXCRf+fz3W3OUhZwFz84jpDh++QrNmUzl9OgKAIkX8eeKJSianEhERcR4qvo7qzDowEm3bmuaQ7e3Zc5Hmzadx8aJtTefSpXOzYkVvihTxNzmZiIiI81DxdVSa3yv/b8uWs7RqNZ2rV2MAqFw5iOXLe5EvX06Tk4mIiDgXndzmqJIuXGGBwprDmV2tXn2CZs2mJpXeOnUKsXr1Uyq9IiIi90FHfB1R9BW4tMu2HVQVvPOYmUZMsmrVcdq0mUFMTAIAjRsXY9Gi7vj6epqcTERExDnpiK8jOrMG+P9lqrR+b7ZVsWIQRYva5vC2aVOaJUt6qPSKiIg8ABVfR6T5vQIEBeVg+fJevPJKHX75pRve3u5mRxIREXFqmurgiG4WX4srFG5gbhbJVPHxibi7uyZ9HBzsz/jxrUxMJCIiknXoiK+juX4ewv61beevDR6+5uaRTGEYBh9+uI5mzaZy40a82XFERESyJBVfR3N6VfK2pjlkC4Zh8NZbKxg+/C/WrTtFp05zSEy0mh1LREQky9FUB0ej+b3ZitVqMHDgEr79dlvSWNOmxXB11d+kIiIi6U3F19HcXL/X1RMK1DM3i2SohAQr/fotZNq0fwCwWODbbx/luedqmpxMREQka1LxdSThx23/AArWB3dvc/NIhomNTeCJJ+bzyy8HAHB1tfDTTx3o2fMhk5OJiIhkXSq+juSU5vdmB1FRcXTsOIdly44C4OHhyuzZnenQoZzJyURERLI2FV9HcvqW+b26cEWWFBERy6OPzmD9+lMA+Pi48+uv3WjevKTJyURERLI+FV9HYRjJJ7a554D8tczNIxnC09OVnDk9APDz82TJkh48/HARk1OJiIhkDzp13FGEHYSo87btwg3BVVfpyoo8Pd2YP78rHTuWZ9WqPiq9IiIimUhHfB3FqZXJ25rmkKUYhoHFYkn62MfHnfnzu5qYSEREJHvSEV9HcVrr92ZFBw5cpkGDyZw+HW52FBERkWxPxdcRGNbkK7Z55YLAKubmkXSxc+d5GjaczIYNpwkJmcbFi9fNjiQiIpKtaaqDI7i0G2Ku2rYLNwYXV1PjyIPbuPE0rVv/THh4LAA5crjj4mK5x71EREQkI+mIryPQNIcsZeXKYzRvPi2p9NavH8xff/UhMDCHyclERESyNxVfR3BKxTerWLToIG3azCAqKh6AkJASLFv2JAEBXiYnExERERVfsyXGw5m1tm2ffJC7vLl55L7NnLmHjh1nExeXCED79mVZvPgJcuTwMDmZiIiIgIqv+S5ug/j/P+mpSFOwaB6oM/r+++307LmAxEQDgB49KjN3bhe8vDSNXkRExFGo+JrtlC5TnBUcORKGYeu8PPdcDaZNexx3d52kKCIi4kh0OMpst57YVrSZeTnkgXz8cQjh4TH4+noydmxzuwtWiIiIiGNQ8TVTQgyc3WDb9isG/sVNjSP3z2Kx8O23bbFYUOkVERFxUJrqYKZzGyHRtuSVVnNwHomJVl5++Q/+/vu03biLi0WlV0RExIGp+JpJ6/c6nfj4RHr2XMCXX26hTZuf2bXrgtmRREREJJVUfM1kd2JbE/NySKpER8fTseMcZs/eB0BUVDxHj4aZnEpERERSS3N8zRIXCRe22LZzl4OcBc3NI3d1/Xocjz02k1WrTgDg6enK/PldefTRMuYGExERkVRT8TXL2fVgTbBtaxkzh3b1ajRt2sxg06YzAOTI4c7ixU/QpIlORhQREXEmKr5m0WWKncLFi9dp0WI6//xzEYCAAC/++KMndesWNjmZiIiIpJWKr1ns5vc2Ni2G3Nnp0+GEhEzj0KErAAQF5WD58l489FA+k5OJiIjI/VDxNcP/tXfncVFV/R/APzMDwyKLK5uCuKG5K2645IbikrlDam5pVriU1JNmJfr4M80nt3zMFddQlHJLCRcSRdNMFC0XTEUBBdxBZBlgzu+PeRibBGVgmAvM5/168XqdOXPuvd/hSH05fO+5mY+Ae+c17RotAatqkoZDBfvttzv46y9N0uvqaocjR8bAw4NzRUREVF5xVwcpJB4D8L/n27LMocwaNqwxvvuuPxo0qIqoqPFMeomIiMo5rvhKgfW95cb777fB2LEtYGVlLnUoREREVEJc8ZVC/oMrZAqgZhdpYyGt48dvY/PmmBf6mfQSERFVDFzxNbZnycDDy5q2U1vAwk7aeAgAEB5+HUOG7EB2dh6srMzh69tE6pCIiIjIwLjia2zxR5+3WeZQJvz442W8+eZ2ZGbmQq0WCA7+A0IIqcMiIiIiA2Pia2wJf9/GjImv1DZvjoGv7w/IyVEDAIYOfQ2hocMhk8kkjoyIiIgMjYmvseXf2KZQAi4dpY3FxH333e8YN24v1GrN6u64cS0REjIMSqVC4siIiIioNDDxNabUW0DqTU3bpSNgbiVpOKZs4cITmDw5TPt66tR2CAp6E2Zm/JEgIiKqqPh/eWNK+Ft9L8scJCGEwKxZEfjsswht3+efd8Hy5X0gl7O8gYiIqCLjrg7GxP17JXf7dipWrDijfb1wYU/MmNFZwoiIiIjIWMrEiu/KlSvh7u4OS0tLtG/fHmfOnCl07Lp169ClSxdUqVIFVapUgbe390vHlxlCPL+xzbySZiszMjp398rYv38ErK3NsXJlPya9REREJkTyxHfHjh0ICAhAYGAgzp07hxYtWsDHxwf37t0rcHxkZCRGjBiBo0eP4tSpU3B1dUXv3r1x584dI0eup8fXgPS7mnbNLpqb20gSXbu648aNafD35y8fREREpkTyxHfJkiV49913MX78eDRu3BirV6+GtbU1NmzYUOD44OBg+Pv7o2XLlmjUqBHWr18PtVqNiIiIAseXGSxzkERGRg7Wro1+YV9eJycbiSIiIiIiqUia+KpUKkRHR8Pb21vbJ5fL4e3tjVOnThXpHBkZGcjJyUHVqlULfD87OxtpaWk6X5JIYOJrbGlp2ejT53u8995+zJ599NUHEBERUYUmaeL74MED5OXlwdHRUaff0dERycnJRTrHjBkz4OLiopM8/92CBQtgb2+v/XJ1dS1x3HoT6udPbLOoDNRoafwYTMzDhxno2XMLoqLiAQDLl/+GxESJfukhIiKiMkHyUoeSWLhwIUJCQrB7925YWloWOOazzz5Damqq9ishIcHIUQK4/weQ9VDTdu0GyPmAhNKUlPQUXbtuwtmzmprqatWscPToWNSqZSdxZERERCQlSbczq169OhQKBVJSUnT6U1JS4OTk9NJjv/nmGyxcuBBHjhxB8+bNCx1nYWEBCwsLg8RbbHxMsdHcvv0EPXtuwY0bjwEAzs42OHx4NJo0cZA4MiIiIpKapCu+SqUSnp6eOjem5d+o5uXlVehxixYtwrx58xAeHo42bdoYI9SS4Y1tRhEb+wCdO2/UJr21a9sjKmo8k14iIiICUAYeYBEQEICxY8eiTZs2aNeuHZYtW4Znz55h/PjxAIAxY8agZs2aWLBgAQDg66+/xuzZs7Ft2za4u7tra4FtbGxgY1MG79RX5wKJxzRtawegWmNp46mgLlxIRq9eW3H/fgYAoGHDajhyZAzLG4iIiEhL8sTXz88P9+/fx+zZs5GcnIyWLVsiPDxce8NbfHw85PLnC9OrVq2CSqXCsGHDdM4TGBiIOXPmGDP0okmJBlRPNW3XHoCMj8U1NCEEJk78SZv0tmjhiEOHRsPBoZLEkREREVFZIhP/3OC0gktLS4O9vT1SU1NhZ2eE1cDfFgAnZmnavdYCzd8t/WuaoLi4x+jSZSNcXe0RFjYSVapYSR0SERERFVNp5WuSr/hWeKzvNYo6darg2LFxcHS0gY0Nn4pHRERELyrX25mVebnZwN0TmratG2BfV9p4KpCIiJvIysrV6atXryqTXiIiIioUE9/SlHQayM3StN1Y32soQUHn0KvXVvj5/YCcnDypwyEiIqJygolvaWKZg8EtW3YaEyf+BCGAfftisXXrRalDIiIionKCiW9pin++PzFcu0sXRwUghMC8eccwffpBbV9AQAeMH99SuqCIiIioXOHNbaVFlQ4k/6ZpV/EAbGtJG085JoTAp58exjffnNL2zZnTFbNnd4WM5SNERERUREx8S8udE5qHVwAscygBtVrA3/8A1qyJ1vYtXtwbAQGFP9mPiIiIqCBMfEvL3+t7XZn4FkdOTh7Gj9+L4OA/AGjuDVyz5g28+66nxJERERFRecTEt7Qk/D3x7SZZGOXZwoUntEmvmZkcW7YMwogRzSSOioiIiMor3txWGrIeAynnNO0azQHrGtLGU05Nn+6FDh1qwcJCgV27fJn0EhERUYlwxbc0JBwD8L8nQbPModhsbJQICxuJS5fuo3NnN6nDISIionKOK76lIYH79xbH/fvPkJT0VKevShUrJr1ERERkEEx8S0P+jW0yOVDrdWljKSfu3EnD669vQq9eW/HgQYbU4RAREVEFxMTX0J6lAA8vadqObQALe2njKQdu3nyMLl024urVB7h06T4mTtwndUhERERUAbHG19ASjj5vs8zhlS5fvg9v7y1ISkoHANStWwXLlvWROCoiIiKqiJj4Ghr37y2yc+eS4OPzvba0oXHjGjh8eDRcXGwljoyIiIgqIia+hpZ/Y5tCCdTsJG0sZdiJE/Ho338b0tKyAQCens4ID38b1atbSxwZERERVVSs8TWktNvAkxuatrMXYM4kriCHD99A795btUlv585uiIgYw6SXiIiIShVXfA0pnvW9rxITk4w33tgOlSoPANC7dz3s2uWLSpWUEkdGREREFR1XfA0pgfW9r9K8uSNGjtQ8gW3w4EbYt+8tJr1ERERkFFzxNRQhnt/YZmYNOLeTNp4ySi6XYd26AWjb1gWTJnnCzIy/exEREZFxMOswlMd/Ael3NO1aXTQ3txEAIDk5Xee1mZkc/v5tmfQSERGRUTHzMBSWObxACIEvv/wFTZp8hz//vCd1OERERGTimPgayt/37+WNbVCrBT76KBz/939RePQoE716bcWTJ1lSh0VEREQmjDW+hiDUz5/YZmEPOLSSNh6J5eWpMWnST9iwIUbbN2tWZ1SubCldUERERGTymPgawoM/gcwHmnatboBcIWk4UlKp8vD227sQGnoZgOZmtqCgNzFuXEtpAyMiIiKTx8TXEFjmAADIzMzB0KE78fPP1wEA5uZybNs2FMOGNZY4MiIiIiImvobBxBdPn2ZjwIDtOHbsNgDA0tIMu3b5om/fBhJHRkRERKTBxLek1LlA4jFN26oGUK2JtPFIICcnD716bcVvv2m2c7O1VeKnn0aga1d3aQMjIiIi+hvu6lBSKecAVZqm7dYDkMmkjUcC5uYKvPVWUwBAlSqWiIgYw6SXiIiIyhyu+JYUyxwAAB991AFCCHh710WzZo5Sh0NERET0Aia+JWWiD67IzMyBlZW5Tt/06V4SRUNERET0aix1KIncbODOCU3b1hWoXE/aeIzk4sUUNGiwAnv3XpU6FCIiIqIiY+JbEsm/AbmZmraJ1PeeOXMH3bptwp07T+Hr+wOOH78tdUhERERERcLEtyTiTavM4dixW+jZcwseP9Y8erhlSyc0beogcVRERERERcPEtyR0Et/u0sVhBGFhf6FPn2Ckp6sAAN26uePIkdGoWtVK4siIiIiIioaJb3HlPAOSTmvaVRoAdq7SxlOKQkMvYdCgEGRl5QIA+vdvgLCwkbC1tZA4MiIiIqKiY+JbXHdOAuocTbsClzls3Hgeb731I3Jy1AAAX98m2LXL74UdHYiIiIjKOia+xWUC+/euWvU73nlnH9RqAQB4552W2LZtCJRKhcSREREREemPiW9x6ezf202yMEpTgwbVtEnuhx+2x7p1b0Kh4D8ZIiIiKp/4AIviyHoCpERr2tWbAdYVc2cDb++62LlzGM6dS8KcOd0gM4Ht2oiIiKjiYuJbHInHAaGpea1IZQ5qtYBMBp0Ed+DARhg4sJGEUREREREZBv9uXRwV8DHFublqjBu3B199FSV1KERERESlgiu+xZF/Y5tMDtR6XdpYDCA7OxcjRvyI3bs1jyC2tbXAtGntJY6KiIiIyLCY+Oor4x7w4A9N29ETsKwsaTgl9eyZCoMH78DhwzcBAEqlArVr20scFREREZHhMfHVV0Lk83Y5L3NITc1C//7bcPJkAgDA2toce/b4oVevehJHRkRERGR4THz1VUH2733wIAM+Pt/j3LkkAICdnQXCwkaiUyc3iSMjIiIiKh1MfPWVf2Ob3Byo2UnaWIrp7t2n6NVrKy5fvg8AqF7dGgcPvo3WrZ0ljoyIiIio9DDx1UdaAvD4L03buQNgXknaeIohLu4xvL234ubNxwAAFxdbHD48Go0b15A4MiIiIqLSxcRXHwnlv8whL08gIyMHAFCnTmUcOTIGdetWkTgqIiIiotLHfXz1UQHqe+vXr4rDh0ejc2c3REWNZ9JLREREJoMrvkUlxPPE18wKcCq/+9w2beqA48fH8RHEREREZFK44ltUT64D6Ymads3OgJmFtPEUUUTETYwbtwe5uWqdfia9REREZGq44ltU8eXvMcX79sVi+PBQqFR5kMlkCAp6E3I5E14iIiIyTVzxLapyVt+7ffsfGDJkB1SqPADA48eZL6z6EhEREZkSJr5FIdRAwlFNW2kHOLaWNp5XWLcuGqNG7UJengAAjBzZDKGhw6FUKiSOjIiIiEg6THyL4sElIFPzsAfU6grIy26FyJIlpzBp0n4ITc6L997zxNatg2FuzqSXiIiITBsT36IoB/v3CiEwZ04kPv74kLbvk0+8sGpVf9b1EhEREYE3txVNGa/vFULgk08OYcmS09q+efO64/PPu3D3BiIiIqL/YeL7KupcICFS07aqDlRvKmU0BcrIyMGxY7e1r5ct88GHH3aQMCIiIiKisoelDq9y7zygStO0XbsDsrL3LatUSYnw8LfRrJkDgoLeZNJLREREVACu+L5KGS9zyFe9ujWioyfxJjYiIiKiQpS95cuypgw+uCI9XYUpU8Lw5EmWTj+TXiIiIqLCccX3ZfJUwJ0oTdumJlClgbTxQPMgin79tuH06UScO5eEQ4dGw8ZGKXVYRERERGUeV3xfJuk3IDdT03brCUi8Q8K9e8/QvftmnD6dCAC4cuUBbt58LGlMREREROUFV3xfpgzV9yYkpMLbeyuuXXsIAHBwqITDh0ejeXNHSeMiIiIiKi+Y+L7M3x9c4dpdsjCuX38Eb+8tuH07VROKqx2OHBkDD49qksVEREREVN4w8S1MTgZw95SmXbk+YOcmSRh//nkPvXptRXJyOgCgXr0qiIgYg9q1K0sSDxEREVF5xcS3MHdOAuocTVuiMoezZ+/Cx+d7PHqkqTNu2tQBhw69DWdnW0niISIiIirPmPgWJkH6bczWro3WJr1t2rggPHwUqlWzliQWIiIiovKOiW9hdPbv7SZJCCtX9kNycjpSU7Px008jYGdnIUkcRERERBUBE9+CZKcCKWc17epNgUrS7Jxgbq7Azp3DoVYLWFubSxIDERERUUXBfXwLkngcEGpN24hlDtu3/4GrVx/o9FlamjHpJSIiIjIAJr4FkWD/3pUrz2DkyF3o1Wsrbt16YpRrEhEREZkSJr4Fyb+xTSYHanUt9cstXHgCU6b8DABITExDcPDFUr8mERERkalhje8/ZdwH7v8v8XRoDVhWLrVLCSHw+ee/YMGCE9q+WbM6Y9asLqV2TSIiIiJTxcT3nxIin7dLscxBrRaYNu1nrFz5u7Zv4cKemDGjc6ldk4iIiMiUMfH9p4TSr+/NzVVjwoR92LLlgrZv5cp+8PdvWyrXIyIiIiImvi/Kv7FNbgbUNPzqa3Z2LkaO3IVdu65oLiOXYePGgRgzpoXBr0VERFSWCSGQm5uLvLw8qUMhCZibm0OhUBj1mkx8/+5pIvD4mqbt3AEwr2TwS+zdG6tNes3N5QgJGYYhQ14z+HWIiIjKMpVKhaSkJGRkZEgdCklEJpOhVq1asLGxMdo1mfj+XcLR5+1S2r/X17cJLl5MwZIlp7B7tx98fOqXynWIiIjKKrVajbi4OCgUCri4uECpVEImk0kdFhmREAL3799HYmIiGjRoYLSVXya+f2ek/XvnzeuOceNaon79qqV2DSIiorJKpVJBrVbD1dUV1tbWUodDEqlRowZu3bqFnJwcoyW+3Mc3nxDPE18zS02pgwEkJT3F0aNxOn0ymYxJLxERmTy5nGmIKZNilZ//4vKl3gSexmvaLp0BM4sSn/L27Sfo0mUj+vXbhmPHbpX4fERERERUfEx88xm4zCE29gE6d96IGzceIysrF1On/gy1WpT4vERERERUPKzxzWfAxPfChWT07v097t17BgBo2LAawsJGQS5n4T4RERGRVLjiC2jqe/MfXKG0BRw9i32q06cT0a3bZm3S26KFI44fH49atewMESkRERFJ7NSpU1AoFOjfv/8L70VGRkImk+HJkycvvOfu7o5ly5bp9B09ehT9+vVDtWrVYG1tjcaNG+Pjjz/GnTt3Sil6ICsrC5MnT0a1atVgY2ODoUOHIiUl5aXHyGSyAr/+85//aMfMnz8fHTt2hLW1NSpXrlxq8ZcEE18AeHgZyLinadfqqnl4RTH88kscvL234MmTLACAl1ctHD06Fg4Oht8PmIiIiKQRFBSEqVOn4vjx47h7926xz7NmzRp4e3vDyckJP/74Iy5fvozVq1cjNTUVixcvNmDEuqZPn46ffvoJoaGhOHbsGO7evYshQ4a89JikpCSdrw0bNkAmk2Ho0KHaMSqVCsOHD8cHH3xQarGXFEsdAIOUOezffw3Dhu1Edrbm6TM9etTB3r1vwcZGaYgIiYiIqAxIT0/Hjh07cPbsWSQnJ2PTpk2YNWuW3udJTEzEtGnTMG3aNCxdulTb7+7ujtdff73AFWNDSE1NRVBQELZt24YePTQ5z8aNG/Haa6/h9OnT6NCh4F2tnJycdF7v3bsX3bt3R926dbV9c+fOBQBs2rSpVGI3BCa+wPMyB6BYD664d+8Z/Px+0Ca9AwZ4YOfO4bC05LeXiIioSL5vAzxLNv51KzkBb58t8vCdO3eiUaNGaNiwId5++2189NFH+Oyzz/Temis0NBQqlQqffvppge+/rFSgb9++iIqKKvT92rVr49KlSwW+Fx0djZycHHh7e2v7GjVqBDc3N5w6darQxPfvUlJScODAAWzevPmVY8saZmbqPCAhUtO2rAbUaKb3KRwcKmHjxoEYMeJH+Po2wZYtg2BubtxnTxMREZVrz5KB9NKrazWUoKAgvP322wCAPn36IDU1FceOHUO3bt30Os9ff/0FOzs7ODs76x3D+vXrkZmZWej75ubmhb6XnJwMpVL5QmLt6OiI5OSi/eKxefNm2NravrI8oixi4ns/Bsh+omm7dQdkxSt79vVtAhcXW3h51YJCwdJpIiIivVRyevUYia8bGxuLM2fOYPfu3QAAMzMz+Pn5ISgoSO/EVwhR7Ac41KxZs1jHGcqGDRswatQoWFpaShpHcTDxjde/zEEIgdOnE+Hl5arT37mzmyEjIyIiMh16lBtIJSgoCLm5uXBxcdH2CSFgYWGB//73v7C3t4ednWYXp9TU1BdWVZ88eQJ7e3sAgIeHB1JTU5GUlKT3qm9JSh2cnJygUqnw5MkTnfhSUlJeqOMtSFRUFGJjY7Fjxw69Yi4ruDSp541tQgh8+ulhdOy4AWvWlP0fUiIiIiq53NxcbNmyBYsXL0ZMTIz268KFC3BxccH27dsBAA0aNIBcLkd0dLTO8Tdv3kRqaio8PDwAAMOGDYNSqcSiRYsKvN7Lbm5bv369Tgz//AoLCyv0WE9PT5ibmyMiIkLbFxsbi/j4eHh5eb3y+xAUFARPT0+0aNHilWPLItNe8c1TAYnHNW0bF6CKx8uH56nh738Aa9eeAwD4+4eha1d3NGpUvbQjJSIiIgnt378fjx8/xoQJE7SrtvmGDh2KoKAgvP/++7C1tcXEiRPx8ccfw8zMDM2aNUNCQgJmzJiBDh06oGPHjgAAV1dXLF26FFOmTEFaWhrGjBkDd3d3JCYmYsuWLbCxsSl0S7OSlDrY29tjwoQJCAgIQNWqVWFnZ4epU6fCy8tL58a2Ro0aYcGCBRg8eLC2Ly0tDaGhoYXGFR8fj0ePHiE+Ph55eXmIiYkBANSvXx82NjbFjtmQTHvFN+kMkJuhabv2AF5Sa5OTk4cxY/Zok16ZDFi9uj+TXiIiIhMQFBQEb2/vF5JeQJP4nj17FhcvXgQALF++HGPHjsWMGTPQpEkTjBs3Ds2bN8dPP/2kU9fr7++PQ4cO4c6dOxg8eDAaNWqEiRMnws7ODp988kmpfZalS5fijTfewNChQ/H666/DyckJu3bt0hkTGxuL1NRUnb6QkBAIITBixIgCzzt79my0atUKgYGBSE9PR6tWrdCqVSucPVt2/kIuE0IIqYMwprS0NNjb2yM1NRV2l5YBvwZq3vDZADQdX+AxWVm58PP7Afv2xQIAzMzk2LJlEEaM0H8HCCIiIlOXlZWFuLg41KlTp1zeIEWG8bJ/Bzr5mp3hnn5r2qUORajvTU9XYdCgEERExAEALCwUCA0djgEDGhojQiIiIiIyENNNfHMygKRTmrZ9XcCu9gtDnjzJQv/+2/DrrwkAgEqVzLF371vo2bPuC2OJiIiIqGwz3cQ36TfNzW1Aoau9fn4/aJNee3sL/PzzqBe2MCMiIiKi8sF0b27L380BKHT/3oULe8Le3gI1algjMnIck14iIiKicsx0V3wTjz1vu3UvcEirVs74+edRqFLFirs3EBERGZiJ3V9P/yDF/Jvuiu89zbZkqNZY+7jC+PhUqNW6k+Dl5cqkl4iIyIDMzc0BABkZGRJHQlJSqTQlpwqFwmjXNN0V3/zfMv5X5nDuXBJ6996KYcMaY9Wq/sV+fjYRERG9nEKhQOXKlXHv3j0AgLW1Nf+/a2LUajXu378Pa2trmJkZLx013cQ3n1sPnDgRj/79tyEtLRtr1kSjceMamDatvdSRERERVVhOTpq/tuYnv2R65HI53NzcjPpLj4knvjIcjq2Dgb5bkZmZCwDo3NkNY8eWz+dPExERlRcymQzOzs5wcHBATk6O1OGQBJRKJeRy41bdmnTiuyehL/xm7YdKlQcA6N27Hnbv9oO1tbnEkREREZkGhUJh1BpPMm1l4ua2lStXwt3dHZaWlmjfvj3OnDnz0vGhoaFo1KgRLC0t0axZM4SFhel9zR0xTTBsRVtt0jt4cCPs2/cWk14iIiKiCkryxHfHjh0ICAhAYGAgzp07hxYtWsDHx6fQmp9ff/0VI0aMwIQJE3D+/HkMGjQIgwYNwp9//qnXdSf9MAB5ak1NyejRzbFz53BYWJj0AjgRERFRhSYTEm+i1759e7Rt2xb//e9/AWju8nN1dcXUqVMxc+bMF8b7+fnh2bNn2L9/v7avQ4cOaNmyJVavXv3K66WlpcHe3h7ATACW8PdvgxUr+kEu592kRERERGVBfr6WmpoKOzs7g51X0iVOlUqF6OhofPbZZ9o+uVwOb29vnDp1qsBjTp06hYCAAJ0+Hx8f7Nmzp8Dx2dnZyM7O1r5OTU3NfwcffdQBc+Z0Rnr60xJ9DiIiIiIynLS0NACGf8iFpInvgwcPkJeXB0dHR51+R0dHXL16tcBjkpOTCxyfnJxc4PgFCxZg7ty5BbyzFMuWLcWyZcWJnIiIiIhK28OHD//3l3rDqPBFrZ999pnOCvGTJ09Qu3ZtxMfHG/QbSWVTWloaXF1dkZCQYNA/lVDZxPk2LZxv08L5Ni2pqalwc3ND1apVDXpeSRPf6tWrQ6FQICUlRac/JSVFu7H1Pzk5Oek13sLCAhYWFi/029vb8wfHhNjZ2XG+TQjn27Rwvk0L59u0GHqfX0l3dVAqlfD09ERERIS2T61WIyIiAl5eXgUe4+XlpTMeAA4fPlzoeCIiIiIioAyUOgQEBGDs2LFo06YN2rVrh2XLluHZs2cYP348AGDMmDGoWbMmFixYAAD48MMP0bVrVyxevBj9+/dHSEgIzp49i7Vr10r5MYiIiIiojJM88fXz88P9+/cxe/ZsJCcno2XLlggPD9fewBYfH6+zzN2xY0ds27YNX3zxBWbNmoUGDRpgz549aNq0aZGuZ2FhgcDAwALLH6ji4XybFs63aeF8mxbOt2kprfmWfB9fIiIiIiJjkPzJbURERERExsDEl4iIiIhMAhNfIiIiIjIJTHyJiIiIyCRUyMR35cqVcHd3h6WlJdq3b48zZ868dHxoaCgaNWoES0tLNGvWDGFhYUaKlAxBn/let24dunTpgipVqqBKlSrw9vZ+5b8PKlv0/fnOFxISAplMhkGDBpVugGRQ+s73kydPMHnyZDg7O8PCwgIeHh78b3o5ou98L1u2DA0bNoSVlRVcXV0xffp0ZGVlGSlaKonjx49jwIABcHFxgUwmw549e155TGRkJFq3bg0LCwvUr18fmzZt0v/CooIJCQkRSqVSbNiwQVy6dEm8++67onLlyiIlJaXA8SdPnhQKhUIsWrRIXL58WXzxxRfC3Nxc/PHHH0aOnIpD3/keOXKkWLlypTh//ry4cuWKGDdunLC3txeJiYlGjpyKQ9/5zhcXFydq1qwpunTpIgYOHGicYKnE9J3v7Oxs0aZNG9GvXz9x4sQJERcXJyIjI0VMTIyRI6fi0He+g4ODhYWFhQgODhZxcXHi4MGDwtnZWUyfPt3IkVNxhIWFic8//1zs2rVLABC7d+9+6fibN28Ka2trERAQIC5fvixWrFghFAqFCA8P1+u6FS7xbdeunZg8ebL2dV5ennBxcRELFiwocLyvr6/o37+/Tl/79u3Fe++9V6pxkmHoO9//lJubK2xtbcXmzZtLK0QyoOLMd25urujYsaNYv369GDt2LBPfckTf+V61apWoW7euUKlUxgqRDEjf+Z48ebLo0aOHTl9AQIDo1KlTqcZJhleUxPfTTz8VTZo00enz8/MTPj4+el2rQpU6qFQqREdHw9vbW9snl8vh7e2NU6dOFXjMqVOndMYDgI+PT6Hjqewoznz/U0ZGBnJyclC1atXSCpMMpLjz/e9//xsODg6YMGGCMcIkAynOfO/btw9eXl6YPHkyHB0d0bRpU3z11VfIy8szVthUTMWZ744dOyI6OlpbDnHz5k2EhYWhX79+RomZjMtQ+ZrkT24zpAcPHiAvL0/71Ld8jo6OuHr1aoHHJCcnFzg+OTm51OIkwyjOfP/TjBkz4OLi8sIPE5U9xZnvEydOICgoCDExMUaIkAypOPN98+ZN/PLLLxg1ahTCwsJw/fp1+Pv7IycnB4GBgcYIm4qpOPM9cuRIPHjwAJ07d4YQArm5uXj//fcxa9YsY4RMRlZYvpaWlobMzExYWVkV6TwVasWXSB8LFy5ESEgIdu/eDUtLS6nDIQN7+vQpRo8ejXXr1qF69epSh0NGoFar4eDggLVr18LT0xN+fn74/PPPsXr1aqlDo1IQGRmJr776Ct999x3OnTuHXbt24cCBA5g3b57UoVEZVqFWfKtXrw6FQoGUlBSd/pSUFDg5ORV4jJOTk17jqewoznzn++abb7Bw4UIcOXIEzZs3L80wyUD0ne8bN27g1q1bGDBggLZPrVYDAMzMzBAbG4t69eqVbtBUbMX5+XZ2doa5uTkUCoW277XXXkNycjJUKhWUSmWpxkzFV5z5/vLLLzF69GhMnDgRANCsWTM8e/YMkyZNwueffw65nGt7FUlh+ZqdnV2RV3uBCrbiq1Qq4enpiYiICG2fWq1GREQEvLy8CjzGy8tLZzwAHD58uNDxVHYUZ74BYNGiRZg3bx7Cw8PRpk0bY4RKBqDvfDdq1Ah//PEHYmJitF9vvvkmunfvjpiYGLi6uhozfNJTcX6+O3XqhOvXr2t/wQGAa9euwdnZmUlvGVec+c7IyHghuc3/pUdzvxRVJAbL1/S7767sCwkJERYWFmLTpk3i8uXLYtKkSaJy5coiOTlZCCHE6NGjxcyZM7XjT548KczMzMQ333wjrly5IgIDA7mdWTmi73wvXLhQKJVK8cMPP4ikpCTt19OnT6X6CKQHfef7n7irQ/mi73zHx8cLW1tbMWXKFBEbGyv2798vHBwcxP/93/9J9RFID/rOd2BgoLC1tRXbt28XN2/eFIcOHRL16tUTvr6+Un0E0sPTp0/F+fPnxfnz5wUAsWTJEnH+/Hlx+/ZtIYQQM2fOFKNHj9aOz9/O7F//+pe4cuWKWLlyJbczy7dixQrh5uYmlEqlaNeunTh9+rT2va5du4qxY8fqjN+5c6fw8PAQSqVSNGnSRBw4cMDIEVNJ6DPftWvXFgBe+AoMDDR+4FQs+v58/x0T3/JH3/n+9ddfRfv27YWFhYWoW7eumD9/vsjNzTVy1FRc+sx3Tk6OmDNnjqhXr56wtLQUrq6uwt/fXzx+/Nj4gZPejh49WuD/j/PneOzYsaJr164vHNOyZUuhVCpF3bp1xcaNG/W+rkwI/j2AiIiIiCq+ClXjS0RERERUGCa+RERERGQSmPgSERERkUlg4ktEREREJoGJLxERERGZBCa+RERERGQSmPgSERERkUlg4ktEREREJoGJLxERgE2bNqFy5cpSh1FsMpkMe/bseemYcePGYdCgQUaJh4ioLGLiS0QVxrhx4yCTyV74un79utShYdOmTdp45HI5atWqhfHjx+PevXsGOX9SUhL69u0LALh16xZkMhliYmJ0xixfvhybNm0yyPUKM2fOHO3nVCgUcHV1xaRJk/Do0SO9zsMknYhKg5nUARARGVKfPn2wceNGnb4aNWpIFI0uOzs7xMbGQq1W48KFCxg/fjzu3r2LgwcPlvjcTk5Orxxjb29f4usURZMmTXDkyBHk5eXhypUreOedd5CamoodO3YY5fpERIXhii8RVSgWFhZwcnLS+VIoFFiyZAmaNWuGSpUqwdXVFf7+/khPTy/0PBcuXED37t1ha2sLOzs7eHp64uzZs9r3T5w4gS5dusDKygqurq6YNm0anj179tLYZDIZnJyc4OLigr59+2LatGk4cuQIMjMzoVar8e9//xu1atWChYUFWrZsifDwcO2xKpUKU6ZMgbOzMywtLVG7dm0sWLBA59z5pQ516tQBALRq1QoymQzdunUDoLuKunbtWri4uECtVuvEOHDgQLzzzjva13v37kXr1q1haWmJunXrYu7cucjNzX3p5zQzM4OTkxNq1qwJb29vDB8+HIcPH9a+n5eXhwkTJqBOnTqwsrJCw4YNsXz5cu37c+bMwebNm7F3717t6nFkZCQAICEhAb6+vqhcuTKqVq2KgQMH4tatWy+Nh4goHxNfIjIJcrkc3377LS5duoTNmzfjl19+waefflro+FGjRqFWrVr4/fffER0djZkzZ8Lc3BwAcOPGDfTp0wdDhw7FxYsXsWPHDpw4cQJTpkzRKyYrKyuo1Wrk5uZi+fLlWLx4Mb755htcvHgRPj4+ePPNN/HXX38BAL799lvs27cPO3fuRGxsLIKDg+Hu7l7gec+cOQMAOHLkCJKSkrBr164XxgwfPhwPHz7E0aNHtX2PHj1CeHg4Ro0aBQCIiorCmDFj8OGHH+Ly5ctYs2YNNm3ahPnz5xf5M966dQsHDx6EUqnU9qnVatSqVQuhoaG4fPkyZs+ejVmzZmHnzp0AgE8++QS+vr7o06cPkpKSkJSUhI4dOyInJwc+Pj6wtbVFVFQUTp48CRsbG/Tp0wcqlarIMRGRCRNERBXE2LFjhUKhEJUqVdJ+DRs2rMCxoaGholq1atrXGzduFPb29trXtra2YtOmTQUeO2HCBDFp0iSdvqioKCGXy0VmZmaBx/zz/NeuXRMeHh6iTZs2QgghXFxcxPz583WOadu2rfD39xdCCDF16lTRo0cPoVarCzw/ALF7924hhBBxcXECgDh//rzOmLFjx4qBAwdqXw8cOFC888472tdr1qwRLi4uIi8vTwghRM+ePcVXX32lc46tW7cKZ2fnAmMQQojAwEAhl8tFpUqVhKWlpQAgAIglS5YUeowQQkyePFkMHTq00Fjzr92wYUOd70F2drawsrISBw8efOn5iYiEEII1vkRUoXTv3h2rVq3Svq5UqRIAzernggULcPXqVaSlpSE3NxdZWVnIyMiAtbX1C+cJCAjAxIkTsXXrVu2f6+vVqwdAUwZx8eJFBAcHa8cLIaBWqxEXF4fXXnutwNhSU1NhY2MDtVqNrKwsdO7cGevXr0daWhru3r2LTp066Yzv1KkTLly4AEBTptCrVy80bNgQffr0wRtvvIHevXuX6Hs1atQovPvuu/juu+9gYWGB4OBgvPXWW5DL5drPefLkSZ0V3ry8vJd+3wCgYcOG2LdvH7KysvD9998jJiYGU6dO1RmzcuVKbNiwAfHx8cjMzIRKpULLli1fGu+FCxdw/fp12Nra6vRnZWXhxo0bxfgOEJGpYeJLRBVKpUqVUL9+fZ2+W7du4Y033sAHH3yA+fPno2rVqjhx4gQmTJgAlUpVYAI3Z84cjBw5EgcOHMDPP/+MwMBAhISEYPDgwUhPT8d7772HadOmvXCcm5tbobHZ2tri3LlzkMvlcHZ2hpWVFQAgLS3tlZ+rdevWiIuLw88//4wjR47A19cX3t7e+OGHH155bGEGDBgAIQQOHDiAtm3bIioqCkuXLtW+n56ejrlz52LIkCEvHGtpaVnoeZVKpXYOFi5ciP79+2Pu3LmYN28eACAkJASffPIJFi9eDC8vL9ja2uI///kPfvvtt5fGm56eDk9PT51fOPKVlRsYiahsY+JLRBVedHQ01Go1Fi9erF3NzK8nfRkPDw94eHhg+vTpGDFiBDZu3IjBgwejdevWuHz58gsJ9qvI5fICj7Gzs4OLiwtOnjyJrl27avtPnjyJdu3a6Yzz8/ODn58fhg0bhj59+uDRo0eoWrWqzvny62nz8vJeGo+lpSWGDBmC4OBgXL9+HQ0bNkTr1q2177du3RqxsbF6f85/+uKLL9CjRw988MEH2s/ZsWNH+Pv7a8f8c8VWqVS+EH/r1q2xY8cOODg4wM7OrkQxEZFp4s1tRFTh1a9fHzk5OVixYgVu3ryJrVu3YvXq1YWOz8zMxJQpUxAZGYnbt2/j5MmT+P3337UlDDNmzMCvv/6KKVOmICYmBn/99Rf27t2r981tf/evf/0LX3/9NXbs2IHY2FjMnDkTMTEx+PDDDwEAS5Yswfbt23H16lVcu3YNoaGhcHJyKvChGw4ODrCyskJ4eDhSUlKQmppa6HVHjRqFAwcOYMOGDdqb2vLNnj0bW7Zswdy5c3Hp0iVcuXIFISEh+OKLL/T6bF5eXmjevDm++uorAECDBg1w9uxZHDx4ENeuXcOXX36J33//XecYd3d3XLx4EbGxsXjw4AFycnIwatQoVK9eHQMHDkRUVBTi4uIQGRmJadOmITExUa+YiMg0MfElogqvRYsWWLJkCb7++ms0bdoUwcHBOluB/ZNCocDDhw8xZswYeHh4wNfXF3379sXcuXMBAM2bN8exY8dw7do1dOnSBa1atcLs2bPh4uJS7BinTZuGgIAAfPzxx2jWrBnCw8Oxb98+NGjQAICmTGLRokVo06YN2rZti1u3biEsLEy7gv13ZmZm+Pbbb7FmzRq4uLhg4MCBhV63R48eqFq1KmJjYzFy5Eid93x8fLB//34cOnQIbdu2RYcOHbB06VLUrl1b7883ffp0rF+/HgkJCXjvvfcwZMgQ+Pn5oX379nj48KHO6i8AvPvuu2jYsCHatGmDGjVq4OTJk7C2tsbx48fh5uaGIUOG4LXXXsOECROQlZXFFWAiKhKZEEJIHQQRERERUWnjii8RERERmQQmvkRERERkEpj4EhEREZFJYOJLRERERCaBiS8RERERmQQmvkRERERkEpj4EhEREZFJYOJLRERERCaBiS8RERERmQQmvkRERERkEpj4EhEREZFJ+H9vYuutk7vdzwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0172, Test Accuracy: 51.96%\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[[497   8]\n",
      " [506  59]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "def test_model(threshold):\n",
    "  # Set the model to evaluation mode\n",
    "  model.eval()\n",
    "  test_loss = 0.0\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  # Get the total number of testing samples\n",
    "  testing_size = len(test_data_loader.dataset)\n",
    "  predicted_class = []\n",
    "  actual_class = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "      training_batch = 0\n",
    "      # Iterate over the test data\n",
    "      for inputs, labels in test_data_loader:\n",
    "          # Move inputs and labels to the specified device\n",
    "          inputs, labels = inputs.to(device), labels.to(device)\n",
    "          inputs = inputs.float()\n",
    "          labels = labels.float()\n",
    "          # Forward pass (with batch size of 1)\n",
    "          outputs = model(inputs.unsqueeze(0))\n",
    "\n",
    "          # Compute the loss\n",
    "          loss = loss_function(outputs.squeeze(), labels)\n",
    "\n",
    "          # Accumulate the test loss\n",
    "          test_loss += loss.item()\n",
    "\n",
    "          # Get the predicted class\n",
    "\n",
    "          predicted = (outputs > threshold).float()\n",
    "\n",
    "          total += labels.size(0)\n",
    "\n",
    "          predicted = predicted.view(-1)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "\n",
    "          # Store actual and predicted classes\n",
    "          actual_class.extend(labels.tolist())\n",
    "          predicted_class.extend(predicted.tolist())\n",
    "\n",
    "          # Count correct predictions\n",
    "          training_batch += len(inputs)\n",
    "          #print(f\"test loss: {loss:>7f}  Batch: [{training_batch:>5d}/{testing_size:>5d}]\")\n",
    "  return test_loss, correct, total, predicted_class, actual_class\n",
    "\n",
    "all_predicted_probs = []\n",
    "all_actual_labels = []\n",
    "thresholds = np.arange(0.3, 0.8, 0.05)\n",
    "\n",
    "for i in thresholds:\n",
    "  print(f\"Threshold: {i}\")\n",
    "  test_loss, correct, total, predicted_class, actual_class = test_model(i)\n",
    "  conf_mat = confusion_matrix(actual_class, predicted_class)\n",
    "\n",
    "fpr, tpr, thresholds_roc = roc_curve(all_actual_labels, all_predicted_probs)\n",
    "\n",
    "# Calculate the area under the ROC curve (AUC)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Calculate final test loss using the total test dataset size\n",
    "final_test_loss = test_loss / len(test_data_loader.dataset)\n",
    "\n",
    "# Calculate the accuracy of the model on the test set\n",
    "test_accuracy = 100 * correct / total\n",
    "\n",
    "# Print out the final test loss and accuracy\n",
    "print(f\"Test Loss: {final_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "print(actual_class)\n",
    "print(predicted_class)\n",
    "\n",
    "# Compute the confusion matrix using the actual and predicted classes\n",
    "conf_mat = confusion_matrix(actual_class, predicted_class)\n",
    "\n",
    "# Print out the confusion matrix\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqdMRyG9Z-1M"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
